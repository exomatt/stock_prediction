{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import yfinance as yf\n",
    "from finta import TA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabulate import tabulate\n",
    "from ta import add_all_ta_features\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "WINDOW = 8  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '2y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "ROWS_TO_PREDICT = 128\n",
    "# one day 16 rows of data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\preprocess\\\\AAPL_16_21_04_2021 00_40_43_full.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "important_columns = ['open', 'high', 'low']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_diffs(diff_number, col_name):\n",
    "#     new_col_name = f'{col_name}_{diff_number}'\n",
    "#     data[new_col_name] = data[col_name].diff(diff_number)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for name in important_columns:\n",
    "#     for i in range(1,11):\n",
    "#         calculate_diffs(i,name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['open'])\n",
    "# del (data['high'])\n",
    "# del (data['volume'])\n",
    "del (data['close_shift'])\n",
    "data = data.dropna()\n",
    "train_set = data.iloc[:-ROWS_TO_PREDICT]\n",
    "train_set = train_set.iloc[:-WINDOW] # optional drop last n rows (avoid of data leak)\n",
    "test_set =data.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1398\n 0    1396\n-1    1388\nName: class_column, dtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0        open        high         low       close   Adj Close  \\\n10            10   55.497501   56.161598   54.549999   55.264999   55.264999   \n11            11   55.247501   55.596424   54.255001   54.432499   54.432499   \n12            12   54.435001   56.700001   54.365250   55.994999   55.994999   \n13            13   56.092500   56.222500   55.269924   56.000000   56.000000   \n14            14   55.912500   56.250000   55.800026   56.200000   56.200000   \n...          ...         ...         ...         ...         ...         ...   \n4042        4042  121.849998  121.949997  119.794998  119.930000  119.930000   \n4043        4043  119.919998  120.489998  119.470001  120.429298  120.429298   \n4044        4044  120.419998  120.500000  119.794998  119.861000  119.861000   \n4045        4045  119.867500  120.410004  119.550003  120.327003  120.327003   \n4046        4046  120.324997  120.480003  119.739998  120.309998  120.309998   \n\n        volume  close_pct  class_column    volume_adi  ...     low_1  \\\n10     7868884  -0.003988             1 -9.811428e+06  ...  0.077499   \n11     6766480  -0.015064             1 -1.478722e+07  ... -0.294998   \n12     8603180   0.028705             1 -1.137967e+07  ...  0.110249   \n13           0   0.000089             1 -1.137967e+07  ...  0.904674   \n14           0   0.003571             1 -1.137967e+07  ...  0.530102   \n...        ...        ...           ...           ...  ...       ...   \n4042  34515975  -0.016806             1  6.971709e+08  ... -0.245002   \n4043  18588318   0.004163             1  7.135468e+08  ... -0.324997   \n4044  11044540  -0.004719             1  7.045703e+08  ...  0.324997   \n4045  13045822   0.003888             1  7.150979e+08  ... -0.244995   \n4046  10712122  -0.000141             1  7.208881e+08  ...  0.189995   \n\n         low_2     low_3     low_4     low_5     low_6     low_7     low_8  \\\n10    1.397499  1.137501 -0.273750 -2.337501 -1.075001 -1.492501 -1.387501   \n11   -0.217499  1.102501  0.842503 -0.568748 -2.632499 -1.369999 -1.787499   \n12   -0.184750 -0.107250  1.212749  0.952751 -0.458500 -2.522250 -1.259750   \n13    1.014923  0.719925  0.797424  2.117424  1.857426  0.446174 -1.617576   \n14    1.434776  1.545025  1.250027  1.327526  2.647526  2.387528  0.976276   \n...        ...       ...       ...       ...       ...       ...       ...   \n4042 -0.205002  0.104998  0.094998 -0.355002 -0.205002 -1.255002  6.968138   \n4043 -0.569999 -0.529999 -0.219999 -0.229999 -0.679999 -0.529999 -1.579999   \n4044  0.000000 -0.245002 -0.205002  0.104998  0.094998 -0.355002 -0.205002   \n4045  0.080002 -0.244995 -0.489997 -0.449997 -0.139997 -0.149997 -0.599997   \n4046 -0.055000  0.269997 -0.055000 -0.300002 -0.260002  0.049998  0.039998   \n\n         low_9    low_10  \n10   -0.820001 -0.950001  \n11   -1.682499 -1.114999  \n12   -1.677250 -1.572250  \n13   -0.355076 -0.772576  \n14   -1.087474  0.175026  \n...        ...       ...  \n4042  6.454428 -1.025002  \n4043  6.643141  6.129431  \n4044 -1.255002  6.968138  \n4045 -0.449997 -1.499997  \n4046 -0.410002 -0.260002  \n\n[4037 rows x 122 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>class_column</th>\n      <th>volume_adi</th>\n      <th>...</th>\n      <th>low_1</th>\n      <th>low_2</th>\n      <th>low_3</th>\n      <th>low_4</th>\n      <th>low_5</th>\n      <th>low_6</th>\n      <th>low_7</th>\n      <th>low_8</th>\n      <th>low_9</th>\n      <th>low_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>55.497501</td>\n      <td>56.161598</td>\n      <td>54.549999</td>\n      <td>55.264999</td>\n      <td>55.264999</td>\n      <td>7868884</td>\n      <td>-0.003988</td>\n      <td>1</td>\n      <td>-9.811428e+06</td>\n      <td>...</td>\n      <td>0.077499</td>\n      <td>1.397499</td>\n      <td>1.137501</td>\n      <td>-0.273750</td>\n      <td>-2.337501</td>\n      <td>-1.075001</td>\n      <td>-1.492501</td>\n      <td>-1.387501</td>\n      <td>-0.820001</td>\n      <td>-0.950001</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>55.247501</td>\n      <td>55.596424</td>\n      <td>54.255001</td>\n      <td>54.432499</td>\n      <td>54.432499</td>\n      <td>6766480</td>\n      <td>-0.015064</td>\n      <td>1</td>\n      <td>-1.478722e+07</td>\n      <td>...</td>\n      <td>-0.294998</td>\n      <td>-0.217499</td>\n      <td>1.102501</td>\n      <td>0.842503</td>\n      <td>-0.568748</td>\n      <td>-2.632499</td>\n      <td>-1.369999</td>\n      <td>-1.787499</td>\n      <td>-1.682499</td>\n      <td>-1.114999</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>54.435001</td>\n      <td>56.700001</td>\n      <td>54.365250</td>\n      <td>55.994999</td>\n      <td>55.994999</td>\n      <td>8603180</td>\n      <td>0.028705</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.110249</td>\n      <td>-0.184750</td>\n      <td>-0.107250</td>\n      <td>1.212749</td>\n      <td>0.952751</td>\n      <td>-0.458500</td>\n      <td>-2.522250</td>\n      <td>-1.259750</td>\n      <td>-1.677250</td>\n      <td>-1.572250</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>56.092500</td>\n      <td>56.222500</td>\n      <td>55.269924</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n      <td>0</td>\n      <td>0.000089</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.904674</td>\n      <td>1.014923</td>\n      <td>0.719925</td>\n      <td>0.797424</td>\n      <td>2.117424</td>\n      <td>1.857426</td>\n      <td>0.446174</td>\n      <td>-1.617576</td>\n      <td>-0.355076</td>\n      <td>-0.772576</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>55.912500</td>\n      <td>56.250000</td>\n      <td>55.800026</td>\n      <td>56.200000</td>\n      <td>56.200000</td>\n      <td>0</td>\n      <td>0.003571</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.530102</td>\n      <td>1.434776</td>\n      <td>1.545025</td>\n      <td>1.250027</td>\n      <td>1.327526</td>\n      <td>2.647526</td>\n      <td>2.387528</td>\n      <td>0.976276</td>\n      <td>-1.087474</td>\n      <td>0.175026</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4042</th>\n      <td>4042</td>\n      <td>121.849998</td>\n      <td>121.949997</td>\n      <td>119.794998</td>\n      <td>119.930000</td>\n      <td>119.930000</td>\n      <td>34515975</td>\n      <td>-0.016806</td>\n      <td>1</td>\n      <td>6.971709e+08</td>\n      <td>...</td>\n      <td>-0.245002</td>\n      <td>-0.205002</td>\n      <td>0.104998</td>\n      <td>0.094998</td>\n      <td>-0.355002</td>\n      <td>-0.205002</td>\n      <td>-1.255002</td>\n      <td>6.968138</td>\n      <td>6.454428</td>\n      <td>-1.025002</td>\n    </tr>\n    <tr>\n      <th>4043</th>\n      <td>4043</td>\n      <td>119.919998</td>\n      <td>120.489998</td>\n      <td>119.470001</td>\n      <td>120.429298</td>\n      <td>120.429298</td>\n      <td>18588318</td>\n      <td>0.004163</td>\n      <td>1</td>\n      <td>7.135468e+08</td>\n      <td>...</td>\n      <td>-0.324997</td>\n      <td>-0.569999</td>\n      <td>-0.529999</td>\n      <td>-0.219999</td>\n      <td>-0.229999</td>\n      <td>-0.679999</td>\n      <td>-0.529999</td>\n      <td>-1.579999</td>\n      <td>6.643141</td>\n      <td>6.129431</td>\n    </tr>\n    <tr>\n      <th>4044</th>\n      <td>4044</td>\n      <td>120.419998</td>\n      <td>120.500000</td>\n      <td>119.794998</td>\n      <td>119.861000</td>\n      <td>119.861000</td>\n      <td>11044540</td>\n      <td>-0.004719</td>\n      <td>1</td>\n      <td>7.045703e+08</td>\n      <td>...</td>\n      <td>0.324997</td>\n      <td>0.000000</td>\n      <td>-0.245002</td>\n      <td>-0.205002</td>\n      <td>0.104998</td>\n      <td>0.094998</td>\n      <td>-0.355002</td>\n      <td>-0.205002</td>\n      <td>-1.255002</td>\n      <td>6.968138</td>\n    </tr>\n    <tr>\n      <th>4045</th>\n      <td>4045</td>\n      <td>119.867500</td>\n      <td>120.410004</td>\n      <td>119.550003</td>\n      <td>120.327003</td>\n      <td>120.327003</td>\n      <td>13045822</td>\n      <td>0.003888</td>\n      <td>1</td>\n      <td>7.150979e+08</td>\n      <td>...</td>\n      <td>-0.244995</td>\n      <td>0.080002</td>\n      <td>-0.244995</td>\n      <td>-0.489997</td>\n      <td>-0.449997</td>\n      <td>-0.139997</td>\n      <td>-0.149997</td>\n      <td>-0.599997</td>\n      <td>-0.449997</td>\n      <td>-1.499997</td>\n    </tr>\n    <tr>\n      <th>4046</th>\n      <td>4046</td>\n      <td>120.324997</td>\n      <td>120.480003</td>\n      <td>119.739998</td>\n      <td>120.309998</td>\n      <td>120.309998</td>\n      <td>10712122</td>\n      <td>-0.000141</td>\n      <td>1</td>\n      <td>7.208881e+08</td>\n      <td>...</td>\n      <td>0.189995</td>\n      <td>-0.055000</td>\n      <td>0.269997</td>\n      <td>-0.055000</td>\n      <td>-0.300002</td>\n      <td>-0.260002</td>\n      <td>0.049998</td>\n      <td>0.039998</td>\n      <td>-0.410002</td>\n      <td>-0.260002</td>\n    </tr>\n  </tbody>\n</table>\n<p>4037 rows × 122 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train= x.iloc[:-ROWS_TO_PREDICT]\n",
    "y_train= y.iloc[:-ROWS_TO_PREDICT]\n",
    "x_test =x.iloc[-ROWS_TO_PREDICT:]\n",
    "y_test=y.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:30:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 2\n",
      "0.9985166872682324\n",
      "0.40625\n",
      "------------\n",
      "[23:30:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 3\n",
      "1.0\n",
      "0.4375\n",
      "------------\n",
      "[23:31:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 4\n",
      "1.0\n",
      "0.484375\n",
      "------------\n",
      "[23:31:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 5\n",
      "1.0\n",
      "0.5234375\n",
      "------------\n",
      "[23:31:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 6\n",
      "1.0\n",
      "0.4921875\n",
      "------------\n",
      "[23:32:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 7\n",
      "1.0\n",
      "0.5078125\n",
      "------------\n",
      "[23:32:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 8\n",
      "1.0\n",
      "0.5078125\n",
      "------------\n",
      "[23:32:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 9\n",
      "1.0\n",
      "0.4765625\n",
      "------------\n",
      "[23:33:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 10\n",
      "1.0\n",
      "0.4296875\n",
      "------------\n",
      "[23:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 11\n",
      "1.0\n",
      "0.5\n",
      "------------\n",
      "[23:33:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 12\n",
      "1.0\n",
      "0.4453125\n",
      "------------\n",
      "[23:34:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 13\n",
      "1.0\n",
      "0.4765625\n",
      "------------\n",
      "[23:34:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 14\n",
      "1.0\n",
      "0.4375\n",
      "------------\n",
      "[23:34:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 15\n",
      "1.0\n",
      "0.4765625\n",
      "------------\n",
      "[23:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 16\n",
      "1.0\n",
      "0.4921875\n",
      "------------\n",
      "[23:35:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 17\n",
      "1.0\n",
      "0.4296875\n",
      "------------\n",
      "[23:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 18\n",
      "1.0\n",
      "0.4453125\n",
      "------------\n",
      "[23:36:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 19\n",
      "1.0\n",
      "0.4609375\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# for i in range(2,50):\n",
    "#     model = xgb.XGBClassifier(nthread =-1,max_depth=i,n_estimators=1000,\n",
    "#                           eta =0.2)\n",
    "#     model.fit(x_train,y_train)\n",
    "#     predicted_train = model.predict(x_train)\n",
    "#     predicted_test = model.predict(x_test)\n",
    "#     print(\"------------\")\n",
    "#     print(f'max_depth: {i}')\n",
    "#     print(accuracy_score(y_train.values, predicted_train))\n",
    "#     print(accuracy_score(y_test.values, predicted_test))\n",
    "#     print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  booster='dart',\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# predicted_test\n",
    "#\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# accuracy_score(y_train.values, predicted_train)\n",
    "#\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# accuracy_score(y_test.values, predicted_test)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:24:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 14\n",
      "1.0\n",
      "0.546875\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(nthread =-1,max_depth=14,n_estimators=1000,\n",
    "                          eta =0.2)\n",
    "model.fit(x_train,y_train)\n",
    "predicted_train = model.predict(x_train)\n",
    "predicted_test = model.predict(x_test)\n",
    "print(\"------------\")\n",
    "print(f'max_depth: {14}')\n",
    "print(accuracy_score(y_train.values, predicted_train))\n",
    "print(accuracy_score(y_test.values, predicted_test))\n",
    "print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}