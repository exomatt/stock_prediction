{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import yfinance as yf\n",
    "from finta import TA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabulate import tabulate\n",
    "from ta import add_all_ta_features\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "WINDOW = 8  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '2y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "ROWS_TO_PREDICT = 128\n",
    "# one day 16 rows of data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\preprocess\\\\AAPL_16_21_04_2021 00_40_43_full.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "important_columns = ['open', 'high', 'low']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_diffs(diff_number, col_name):\n",
    "    new_col_name = f'{col_name}_{diff_number}'\n",
    "    data[new_col_name] = data[col_name].diff(diff_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0       open       high        low      close  Adj Close  \\\n0           0  56.220000  56.220000  55.500000  55.600000  55.600000   \n1           1  55.550000  56.095000  55.370000  55.700000  55.700000   \n2           2  56.032500  56.437500  55.937500  56.247500  56.247500   \n3           3  56.132500  56.750000  56.042500  56.595000  56.595000   \n4           4  56.550000  58.197500  55.625000  57.812500  57.812500   \n5           5  57.837500  59.525000  56.887500  57.020000  57.020000   \n6           6  56.299999  56.750000  54.823750  55.450001  55.450001   \n7           7  55.465000  55.889999  53.412498  53.767502  53.767502   \n8           8  53.762501  55.097500  53.152500  54.803925  54.803925   \n9           9  54.823875  56.775002  54.472500  55.486252  55.486252   \n\n     volume  close_pct  close_shift  class_column  ...     low_1     low_2  \\\n0         0        NaN    56.875000             1  ...       NaN       NaN   \n1         0   0.001799    58.447500             1  ... -0.130000       NaN   \n2         0   0.009829    58.550000             1  ...  0.567500  0.437500   \n3         0   0.006178    59.222500             1  ...  0.105000  0.672500   \n4         0   0.021513    58.307500             0  ... -0.417500 -0.312500   \n5         0  -0.013708    58.175000             1  ...  1.262500  0.845000   \n6  21473989  -0.027534    58.892525             1  ... -2.063750 -0.801250   \n7  11306818  -0.030343    60.434502             1  ... -1.411251 -3.475002   \n8  11355189   0.019276    60.172501             1  ... -0.259998 -1.671249   \n9  10671797   0.012450    60.645000             1  ...  1.320000  1.060001   \n\n      low_3     low_4     low_5     low_6     low_7   low_8   low_9  low_10  \n0       NaN       NaN       NaN       NaN       NaN     NaN     NaN     NaN  \n1       NaN       NaN       NaN       NaN       NaN     NaN     NaN     NaN  \n2       NaN       NaN       NaN       NaN       NaN     NaN     NaN     NaN  \n3  0.542500       NaN       NaN       NaN       NaN     NaN     NaN     NaN  \n4  0.255000  0.125000       NaN       NaN       NaN     NaN     NaN     NaN  \n5  0.950000  1.517500  1.387500       NaN       NaN     NaN     NaN     NaN  \n6 -1.218750 -1.113750 -0.546250 -0.676250       NaN     NaN     NaN     NaN  \n7 -2.212502 -2.630002 -2.525002 -1.957502 -2.087502     NaN     NaN     NaN  \n8 -3.735000 -2.472500 -2.890000 -2.785000 -2.217500 -2.3475     NaN     NaN  \n9 -0.351250 -2.415000 -1.152500 -1.570000 -1.465000 -0.8975 -1.0275     NaN  \n\n[10 rows x 123 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>close_shift</th>\n      <th>class_column</th>\n      <th>...</th>\n      <th>low_1</th>\n      <th>low_2</th>\n      <th>low_3</th>\n      <th>low_4</th>\n      <th>low_5</th>\n      <th>low_6</th>\n      <th>low_7</th>\n      <th>low_8</th>\n      <th>low_9</th>\n      <th>low_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>56.220000</td>\n      <td>56.220000</td>\n      <td>55.500000</td>\n      <td>55.600000</td>\n      <td>55.600000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>56.875000</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>55.550000</td>\n      <td>56.095000</td>\n      <td>55.370000</td>\n      <td>55.700000</td>\n      <td>55.700000</td>\n      <td>0</td>\n      <td>0.001799</td>\n      <td>58.447500</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.130000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>56.032500</td>\n      <td>56.437500</td>\n      <td>55.937500</td>\n      <td>56.247500</td>\n      <td>56.247500</td>\n      <td>0</td>\n      <td>0.009829</td>\n      <td>58.550000</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.567500</td>\n      <td>0.437500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>56.132500</td>\n      <td>56.750000</td>\n      <td>56.042500</td>\n      <td>56.595000</td>\n      <td>56.595000</td>\n      <td>0</td>\n      <td>0.006178</td>\n      <td>59.222500</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.105000</td>\n      <td>0.672500</td>\n      <td>0.542500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>56.550000</td>\n      <td>58.197500</td>\n      <td>55.625000</td>\n      <td>57.812500</td>\n      <td>57.812500</td>\n      <td>0</td>\n      <td>0.021513</td>\n      <td>58.307500</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.417500</td>\n      <td>-0.312500</td>\n      <td>0.255000</td>\n      <td>0.125000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>57.837500</td>\n      <td>59.525000</td>\n      <td>56.887500</td>\n      <td>57.020000</td>\n      <td>57.020000</td>\n      <td>0</td>\n      <td>-0.013708</td>\n      <td>58.175000</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.262500</td>\n      <td>0.845000</td>\n      <td>0.950000</td>\n      <td>1.517500</td>\n      <td>1.387500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>56.299999</td>\n      <td>56.750000</td>\n      <td>54.823750</td>\n      <td>55.450001</td>\n      <td>55.450001</td>\n      <td>21473989</td>\n      <td>-0.027534</td>\n      <td>58.892525</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-2.063750</td>\n      <td>-0.801250</td>\n      <td>-1.218750</td>\n      <td>-1.113750</td>\n      <td>-0.546250</td>\n      <td>-0.676250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>55.465000</td>\n      <td>55.889999</td>\n      <td>53.412498</td>\n      <td>53.767502</td>\n      <td>53.767502</td>\n      <td>11306818</td>\n      <td>-0.030343</td>\n      <td>60.434502</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-1.411251</td>\n      <td>-3.475002</td>\n      <td>-2.212502</td>\n      <td>-2.630002</td>\n      <td>-2.525002</td>\n      <td>-1.957502</td>\n      <td>-2.087502</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>53.762501</td>\n      <td>55.097500</td>\n      <td>53.152500</td>\n      <td>54.803925</td>\n      <td>54.803925</td>\n      <td>11355189</td>\n      <td>0.019276</td>\n      <td>60.172501</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.259998</td>\n      <td>-1.671249</td>\n      <td>-3.735000</td>\n      <td>-2.472500</td>\n      <td>-2.890000</td>\n      <td>-2.785000</td>\n      <td>-2.217500</td>\n      <td>-2.3475</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>54.823875</td>\n      <td>56.775002</td>\n      <td>54.472500</td>\n      <td>55.486252</td>\n      <td>55.486252</td>\n      <td>10671797</td>\n      <td>0.012450</td>\n      <td>60.645000</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.320000</td>\n      <td>1.060001</td>\n      <td>-0.351250</td>\n      <td>-2.415000</td>\n      <td>-1.152500</td>\n      <td>-1.570000</td>\n      <td>-1.465000</td>\n      <td>-0.8975</td>\n      <td>-1.0275</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 123 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in important_columns:\n",
    "    for i in range(1,11):\n",
    "        calculate_diffs(i,name)\n",
    "\n",
    "data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['open'])\n",
    "# del (data['high'])\n",
    "# del (data['volume'])\n",
    "del (data['close_shift'])\n",
    "data = data.dropna()\n",
    "train_set = data.iloc[:-ROWS_TO_PREDICT]\n",
    "train_set = train_set.iloc[:-WINDOW] # optional drop last n rows (avoid of data leak)\n",
    "test_set =data.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1390\n-1    1388\n 0    1379\nName: class_column, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0        open        high         low       close   Adj Close  \\\n10            10   55.497501   56.161598   54.549999   55.264999   55.264999   \n11            11   55.247501   55.596424   54.255001   54.432499   54.432499   \n12            12   54.435001   56.700001   54.365250   55.994999   55.994999   \n13            13   56.092500   56.222500   55.269924   56.000000   56.000000   \n14            14   55.912500   56.250000   55.800026   56.200000   56.200000   \n...          ...         ...         ...         ...         ...         ...   \n4026        4026  119.000000  121.559898  118.790001  121.317001  121.317001   \n4027        4027  121.315002  121.370003  120.050003  120.427101  120.427101   \n4028        4028  120.430000  121.235001  120.199997  121.103401  121.103401   \n4029        4029  121.099998  121.690002  120.750000  121.565002  121.565002   \n4030        4030  121.570000  121.949997  121.099998  121.154999  121.154999   \n\n        volume  close_pct  class_column    volume_adi  ...     low_1  \\\n10     7868884  -0.003988             1 -9.811428e+06  ...  0.077499   \n11     6766480  -0.015064             1 -1.478722e+07  ... -0.294998   \n12     8603180   0.028705             1 -1.137967e+07  ...  0.110249   \n13           0   0.000089             1 -1.137967e+07  ...  0.904674   \n14           0   0.003571             1 -1.137967e+07  ...  0.530102   \n...        ...        ...           ...           ...  ...       ...   \n4026  39941189   0.018700            -1  7.316685e+08  ...  0.260001   \n4027  16707654  -0.007335             0  7.245069e+08  ...  1.260002   \n4028  11116493   0.005616            -1  7.327965e+08  ...  0.149994   \n4029  11146638   0.003812            -1  7.409786e+08  ...  0.550003   \n4030  10818832  -0.003373            -1  7.315599e+08  ...  0.349998   \n\n         low_2     low_3     low_4     low_5     low_6     low_7     low_8  \\\n10    1.397499  1.137501 -0.273750 -2.337501 -1.075001 -1.492501 -1.387501   \n11   -0.217499  1.102501  0.842503 -0.568748 -2.632499 -1.369999 -1.787499   \n12   -0.184750 -0.107250  1.212749  0.952751 -0.458500 -2.522250 -1.259750   \n13    1.014923  0.719925  0.797424  2.117424  1.857426  0.446174 -1.617576   \n14    1.434776  1.545025  1.250027  1.327526  2.647526  2.387528  0.976276   \n...        ...       ...       ...       ...       ...       ...       ...   \n4026  2.430001  0.110001 -0.029999  0.310001  0.680001  2.290001  9.512146   \n4027  1.520003  3.690003  1.370003  1.230003  1.570003  1.940003  3.550003   \n4028  1.409996  1.669997  3.839997  1.519997  1.379997  1.719997  2.089997   \n4029  0.699997  1.959999  2.220000  4.390000  2.070000  1.930000  2.270000   \n4030  0.900002  1.049995  2.309998  2.569998  4.739998  2.419998  2.279998   \n\n          low_9     low_10  \n10    -0.820001  -0.950001  \n11    -1.682499  -1.114999  \n12    -1.677250  -1.572250  \n13    -0.355076  -0.772576  \n14    -1.087474   0.175026  \n...         ...        ...  \n4026   2.680001   2.580002  \n4027  10.772148   3.940003  \n4028   3.699997  10.922142  \n4029   2.640000   4.250000  \n4030   2.619998   2.989998  \n\n[4021 rows x 122 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>class_column</th>\n      <th>volume_adi</th>\n      <th>...</th>\n      <th>low_1</th>\n      <th>low_2</th>\n      <th>low_3</th>\n      <th>low_4</th>\n      <th>low_5</th>\n      <th>low_6</th>\n      <th>low_7</th>\n      <th>low_8</th>\n      <th>low_9</th>\n      <th>low_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>55.497501</td>\n      <td>56.161598</td>\n      <td>54.549999</td>\n      <td>55.264999</td>\n      <td>55.264999</td>\n      <td>7868884</td>\n      <td>-0.003988</td>\n      <td>1</td>\n      <td>-9.811428e+06</td>\n      <td>...</td>\n      <td>0.077499</td>\n      <td>1.397499</td>\n      <td>1.137501</td>\n      <td>-0.273750</td>\n      <td>-2.337501</td>\n      <td>-1.075001</td>\n      <td>-1.492501</td>\n      <td>-1.387501</td>\n      <td>-0.820001</td>\n      <td>-0.950001</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>55.247501</td>\n      <td>55.596424</td>\n      <td>54.255001</td>\n      <td>54.432499</td>\n      <td>54.432499</td>\n      <td>6766480</td>\n      <td>-0.015064</td>\n      <td>1</td>\n      <td>-1.478722e+07</td>\n      <td>...</td>\n      <td>-0.294998</td>\n      <td>-0.217499</td>\n      <td>1.102501</td>\n      <td>0.842503</td>\n      <td>-0.568748</td>\n      <td>-2.632499</td>\n      <td>-1.369999</td>\n      <td>-1.787499</td>\n      <td>-1.682499</td>\n      <td>-1.114999</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>54.435001</td>\n      <td>56.700001</td>\n      <td>54.365250</td>\n      <td>55.994999</td>\n      <td>55.994999</td>\n      <td>8603180</td>\n      <td>0.028705</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.110249</td>\n      <td>-0.184750</td>\n      <td>-0.107250</td>\n      <td>1.212749</td>\n      <td>0.952751</td>\n      <td>-0.458500</td>\n      <td>-2.522250</td>\n      <td>-1.259750</td>\n      <td>-1.677250</td>\n      <td>-1.572250</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>56.092500</td>\n      <td>56.222500</td>\n      <td>55.269924</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n      <td>0</td>\n      <td>0.000089</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.904674</td>\n      <td>1.014923</td>\n      <td>0.719925</td>\n      <td>0.797424</td>\n      <td>2.117424</td>\n      <td>1.857426</td>\n      <td>0.446174</td>\n      <td>-1.617576</td>\n      <td>-0.355076</td>\n      <td>-0.772576</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>55.912500</td>\n      <td>56.250000</td>\n      <td>55.800026</td>\n      <td>56.200000</td>\n      <td>56.200000</td>\n      <td>0</td>\n      <td>0.003571</td>\n      <td>1</td>\n      <td>-1.137967e+07</td>\n      <td>...</td>\n      <td>0.530102</td>\n      <td>1.434776</td>\n      <td>1.545025</td>\n      <td>1.250027</td>\n      <td>1.327526</td>\n      <td>2.647526</td>\n      <td>2.387528</td>\n      <td>0.976276</td>\n      <td>-1.087474</td>\n      <td>0.175026</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4026</th>\n      <td>4026</td>\n      <td>119.000000</td>\n      <td>121.559898</td>\n      <td>118.790001</td>\n      <td>121.317001</td>\n      <td>121.317001</td>\n      <td>39941189</td>\n      <td>0.018700</td>\n      <td>-1</td>\n      <td>7.316685e+08</td>\n      <td>...</td>\n      <td>0.260001</td>\n      <td>2.430001</td>\n      <td>0.110001</td>\n      <td>-0.029999</td>\n      <td>0.310001</td>\n      <td>0.680001</td>\n      <td>2.290001</td>\n      <td>9.512146</td>\n      <td>2.680001</td>\n      <td>2.580002</td>\n    </tr>\n    <tr>\n      <th>4027</th>\n      <td>4027</td>\n      <td>121.315002</td>\n      <td>121.370003</td>\n      <td>120.050003</td>\n      <td>120.427101</td>\n      <td>120.427101</td>\n      <td>16707654</td>\n      <td>-0.007335</td>\n      <td>0</td>\n      <td>7.245069e+08</td>\n      <td>...</td>\n      <td>1.260002</td>\n      <td>1.520003</td>\n      <td>3.690003</td>\n      <td>1.370003</td>\n      <td>1.230003</td>\n      <td>1.570003</td>\n      <td>1.940003</td>\n      <td>3.550003</td>\n      <td>10.772148</td>\n      <td>3.940003</td>\n    </tr>\n    <tr>\n      <th>4028</th>\n      <td>4028</td>\n      <td>120.430000</td>\n      <td>121.235001</td>\n      <td>120.199997</td>\n      <td>121.103401</td>\n      <td>121.103401</td>\n      <td>11116493</td>\n      <td>0.005616</td>\n      <td>-1</td>\n      <td>7.327965e+08</td>\n      <td>...</td>\n      <td>0.149994</td>\n      <td>1.409996</td>\n      <td>1.669997</td>\n      <td>3.839997</td>\n      <td>1.519997</td>\n      <td>1.379997</td>\n      <td>1.719997</td>\n      <td>2.089997</td>\n      <td>3.699997</td>\n      <td>10.922142</td>\n    </tr>\n    <tr>\n      <th>4029</th>\n      <td>4029</td>\n      <td>121.099998</td>\n      <td>121.690002</td>\n      <td>120.750000</td>\n      <td>121.565002</td>\n      <td>121.565002</td>\n      <td>11146638</td>\n      <td>0.003812</td>\n      <td>-1</td>\n      <td>7.409786e+08</td>\n      <td>...</td>\n      <td>0.550003</td>\n      <td>0.699997</td>\n      <td>1.959999</td>\n      <td>2.220000</td>\n      <td>4.390000</td>\n      <td>2.070000</td>\n      <td>1.930000</td>\n      <td>2.270000</td>\n      <td>2.640000</td>\n      <td>4.250000</td>\n    </tr>\n    <tr>\n      <th>4030</th>\n      <td>4030</td>\n      <td>121.570000</td>\n      <td>121.949997</td>\n      <td>121.099998</td>\n      <td>121.154999</td>\n      <td>121.154999</td>\n      <td>10818832</td>\n      <td>-0.003373</td>\n      <td>-1</td>\n      <td>7.315599e+08</td>\n      <td>...</td>\n      <td>0.349998</td>\n      <td>0.900002</td>\n      <td>1.049995</td>\n      <td>2.309998</td>\n      <td>2.569998</td>\n      <td>4.739998</td>\n      <td>2.419998</td>\n      <td>2.279998</td>\n      <td>2.619998</td>\n      <td>2.989998</td>\n    </tr>\n  </tbody>\n</table>\n<p>4021 rows × 122 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "# x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train= x.iloc[:-ROWS_TO_PREDICT]\n",
    "y_train= y.iloc[:-ROWS_TO_PREDICT]\n",
    "x_test =x.iloc[-ROWS_TO_PREDICT:]\n",
    "y_test=y.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:41:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 2\n",
      "0.513030528667163\n",
      "0.4453125\n",
      "------------\n",
      "[23:41:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 3\n",
      "0.5656490444278978\n",
      "0.4453125\n",
      "------------\n",
      "[23:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 4\n",
      "0.653512037726483\n",
      "0.4453125\n",
      "------------\n",
      "[23:41:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 5\n",
      "0.7614792752544055\n",
      "0.421875\n",
      "------------\n",
      "[23:41:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 6\n",
      "0.8255150161330355\n",
      "0.46875\n",
      "------------\n",
      "[23:42:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 7\n",
      "0.876644328617523\n",
      "0.4921875\n",
      "------------\n",
      "[23:42:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 8\n",
      "0.919831223628692\n",
      "0.5078125\n",
      "------------\n",
      "[23:42:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 9\n",
      "0.9431620749565649\n",
      "0.5078125\n",
      "------------\n",
      "[23:42:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 10\n",
      "0.9610325142715314\n",
      "0.5234375\n",
      "------------\n",
      "[23:42:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 11\n",
      "0.9751799453958798\n",
      "0.515625\n",
      "------------\n",
      "[23:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 12\n",
      "0.9843633655994043\n",
      "0.5234375\n",
      "------------\n",
      "[23:42:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 13\n",
      "0.9895755770662695\n",
      "0.5078125\n",
      "------------\n",
      "[23:42:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 14\n",
      "0.9930503847108464\n",
      "0.515625\n",
      "------------\n",
      "[23:42:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 15\n",
      "0.9940431868950111\n",
      "0.5078125\n",
      "------------\n",
      "[23:42:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 16\n",
      "0.9967733929014644\n",
      "0.5\n",
      "------------\n",
      "[23:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 17\n",
      "0.997517994539588\n",
      "0.515625\n",
      "------------\n",
      "[23:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 18\n",
      "0.9982625961777116\n",
      "0.515625\n",
      "------------\n",
      "[23:42:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "max_depth: 19\n",
      "0.9985107967237528\n",
      "0.515625\n",
      "------------\n",
      "[23:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "eta: \n",
      "0.9610325142715314\n",
      "0.5234375\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,20):\n",
    "    model = xgb.XGBRFClassifier(nthread =-1,max_depth=i,num_parallel_tree=100,eta =0.4)\n",
    "    model.fit(x_train,y_train)\n",
    "    predicted_train = model.predict(x_train)\n",
    "    predicted_test = model.predict(x_test)\n",
    "    print(\"------------\")\n",
    "    print(f'max_depth: {i}')\n",
    "    print(accuracy_score(y_train.values, predicted_train))\n",
    "    print(accuracy_score(y_test.values, predicted_test))\n",
    "    print(\"------------\")\n",
    "\n",
    "# for i in range(0,100):\n",
    "\n",
    "model = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=12,n_estimators =100,eta=0.4)\n",
    "model.fit(x_train,y_train)\n",
    "predicted_train = model.predict(x_train)\n",
    "predicted_test = model.predict(x_test)\n",
    "print(\"------------\")\n",
    "print(f'eta: ')\n",
    "print(accuracy_score(y_train.values, predicted_train))\n",
    "print(accuracy_score(y_test.values, predicted_test))\n",
    "print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  booster='dart',\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  1,  1, ..., -1, -1, -1], dtype=int64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9965466206216083"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train.values, predicted_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4921875"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test.values, predicted_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.00570821, 0.00381237, 0.00671404, 0.00632185, 0.00920099,\n       0.00962175, 0.00414626, 0.00327022, 0.01308581, 0.01473213,\n       0.01021574, 0.0073754 , 0.00863127, 0.01194806, 0.01144959,\n       0.00496418, 0.01368823, 0.01187784, 0.00808209, 0.01010779,\n       0.01332559, 0.01146628, 0.00927289, 0.00666881, 0.00474349,\n       0.0058435 , 0.01131136, 0.01040499, 0.01207635, 0.0089634 ,\n       0.0068779 , 0.00707698, 0.0058231 , 0.01800122, 0.02046858,\n       0.01484249, 0.01270077, 0.00760605, 0.01038803, 0.01082772,\n       0.01151109, 0.01021596, 0.01260001, 0.01380788, 0.01478382,\n       0.01742299, 0.01117344, 0.00926107, 0.01008793, 0.00659338,\n       0.00702907, 0.00750393, 0.01071193, 0.01008036, 0.00740577,\n       0.00641479, 0.0117988 , 0.01252222, 0.00990448, 0.01581678,\n       0.01933273, 0.02334621, 0.01971269, 0.01980872, 0.02068326,\n       0.00739919, 0.00838184, 0.01333402, 0.01592654, 0.02283322,\n       0.00250278, 0.0035254 , 0.01184978, 0.01096373, 0.00779554,\n       0.00808505, 0.01000679, 0.01333628, 0.00957774, 0.00907977,\n       0.00966856, 0.00921853, 0.0137265 , 0.05189564, 0.01037148,\n       0.00779231, 0.00748134, 0.00845038, 0.00480435, 0.00464161,\n       0.01417904], dtype=float32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "max_depth: 14\n",
      "0.9965466206216083\n",
      "0.4921875\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# model = xgb.XGBRFClassifier(nthread =-1,max_depth=14,n_estimators=1000,\n",
    "#                           eta =0.2)\n",
    "# model.fit(x_train,y_train)\n",
    "predicted_train = model.predict(x_train)\n",
    "predicted_test = model.predict(x_test)\n",
    "print(\"------------\")\n",
    "print(f'max_depth: {14}')\n",
    "print(accuracy_score(y_train.values, predicted_train))\n",
    "print(accuracy_score(y_test.values, predicted_test))\n",
    "print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}