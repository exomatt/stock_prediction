{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import yfinance as yf\n",
    "from finta import TA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabulate import tabulate\n",
    "from ta import add_all_ta_features\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "WINDOW = 8  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '2y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "ROWS_TO_PREDICT = 128\n",
    "# one day 16 rows of data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\preprocess\\\\AAPL_16_21_04_2021 00_40_43_full.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['open'])\n",
    "# del (data['high'])\n",
    "# del (data['volume'])\n",
    "del (data['close_shift'])\n",
    "data = data.dropna()\n",
    "train_set = data.iloc[:-ROWS_TO_PREDICT]\n",
    "train_set = train_set.iloc[:-WINDOW] # optional drop last n rows (avoid of data leak)\n",
    "test_set =data.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1398\n 0    1396\n-1    1388\nName: class_column, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0        open        high         low       close   Adj Close  \\\n1              1   55.550000   56.095000   55.370000   55.700000   55.700000   \n2              2   56.032500   56.437500   55.937500   56.247500   56.247500   \n3              3   56.132500   56.750000   56.042500   56.595000   56.595000   \n4              4   56.550000   58.197500   55.625000   57.812500   57.812500   \n5              5   57.837500   59.525000   56.887500   57.020000   57.020000   \n...          ...         ...         ...         ...         ...         ...   \n4042        4042  121.849998  121.949997  119.794998  119.930000  119.930000   \n4043        4043  119.919998  120.489998  119.470001  120.429298  120.429298   \n4044        4044  120.419998  120.500000  119.794998  119.861000  119.861000   \n4045        4045  119.867500  120.410004  119.550003  120.327003  120.327003   \n4046        4046  120.324997  120.480003  119.739998  120.309998  120.309998   \n\n        volume  close_pct  class_column    volume_adi  ...  momentum_wr  \\\n1            0   0.001799             1 -0.000000e+00  ...   -61.176471   \n2            0   0.009829             1  0.000000e+00  ...   -17.798595   \n3            0   0.006178             1  0.000000e+00  ...   -11.231884   \n4            0   0.021513             0  0.000000e+00  ...   -13.616269   \n5            0  -0.013708             1  0.000000e+00  ...   -60.288809   \n...        ...        ...           ...           ...  ...          ...   \n4042  34515975  -0.016806             1  6.971709e+08  ...   -54.542259   \n4043  18588318   0.004163             1  7.135468e+08  ...   -51.346916   \n4044  11044540  -0.004719             1  7.045703e+08  ...   -54.983837   \n4045  13045822   0.003888             1  7.150979e+08  ...   -52.001570   \n4046  10712122  -0.000141             1  7.208881e+08  ...   -52.110402   \n\n      momentum_ao  momentum_kama  momentum_roc  momentum_ppo  \\\n1        0.000000      55.641486      0.000000      0.000000   \n2        0.000000      55.895109      0.000000      0.000000   \n3        0.000000      56.185689      0.000000      0.000000   \n4        0.000000      56.833695      0.000000      0.000000   \n5        0.137792      56.907320      0.000000      0.000000   \n...           ...            ...           ...           ...   \n4042     1.144553     120.287005     -1.011100      4.174922   \n4043     1.091641     120.288723     -1.064449     14.697685   \n4044     1.043391     120.278308     -1.006774     15.112980   \n4045     0.866795     120.279079     -0.597271     16.801027   \n4046     0.605722     120.279268     -0.717942     15.950082   \n\n      momentum_ppo_signal  momentum_ppo_hist  others_dr  others_dlr  \\\n1                0.000000           0.000000   0.179856    0.179695   \n2                0.000000           0.000000   0.982944    0.978145   \n3                0.000000           0.000000   0.617805    0.615905   \n4                0.000000           0.000000   2.151250    2.128437   \n5                0.000000           0.000000  -1.370811   -1.380293   \n...                   ...                ...        ...         ...   \n4042           -23.072724          27.247646  -1.680603   -1.694886   \n4043           -15.518642          30.216326   0.416325    0.415460   \n4044            -9.392318          24.505297  -0.471894   -0.473011   \n4045            -4.153649          20.954675   0.388787    0.388033   \n4046            -0.132903          16.082984  -0.014133   -0.014134   \n\n       others_cr  \n1       0.179856  \n2       1.164568  \n3       1.789568  \n4       3.979317  \n5       2.553957  \n...          ...  \n4042  115.701439  \n4043  116.599458  \n4044  115.577338  \n4045  116.415474  \n4046  116.384888  \n\n[4046 rows x 92 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>class_column</th>\n      <th>volume_adi</th>\n      <th>...</th>\n      <th>momentum_wr</th>\n      <th>momentum_ao</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>55.550000</td>\n      <td>56.095000</td>\n      <td>55.370000</td>\n      <td>55.700000</td>\n      <td>55.700000</td>\n      <td>0</td>\n      <td>0.001799</td>\n      <td>1</td>\n      <td>-0.000000e+00</td>\n      <td>...</td>\n      <td>-61.176471</td>\n      <td>0.000000</td>\n      <td>55.641486</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.179856</td>\n      <td>0.179695</td>\n      <td>0.179856</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>56.032500</td>\n      <td>56.437500</td>\n      <td>55.937500</td>\n      <td>56.247500</td>\n      <td>56.247500</td>\n      <td>0</td>\n      <td>0.009829</td>\n      <td>1</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>-17.798595</td>\n      <td>0.000000</td>\n      <td>55.895109</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.982944</td>\n      <td>0.978145</td>\n      <td>1.164568</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>56.132500</td>\n      <td>56.750000</td>\n      <td>56.042500</td>\n      <td>56.595000</td>\n      <td>56.595000</td>\n      <td>0</td>\n      <td>0.006178</td>\n      <td>1</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>-11.231884</td>\n      <td>0.000000</td>\n      <td>56.185689</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.617805</td>\n      <td>0.615905</td>\n      <td>1.789568</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>56.550000</td>\n      <td>58.197500</td>\n      <td>55.625000</td>\n      <td>57.812500</td>\n      <td>57.812500</td>\n      <td>0</td>\n      <td>0.021513</td>\n      <td>0</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>-13.616269</td>\n      <td>0.000000</td>\n      <td>56.833695</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.151250</td>\n      <td>2.128437</td>\n      <td>3.979317</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>57.837500</td>\n      <td>59.525000</td>\n      <td>56.887500</td>\n      <td>57.020000</td>\n      <td>57.020000</td>\n      <td>0</td>\n      <td>-0.013708</td>\n      <td>1</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>-60.288809</td>\n      <td>0.137792</td>\n      <td>56.907320</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.370811</td>\n      <td>-1.380293</td>\n      <td>2.553957</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4042</th>\n      <td>4042</td>\n      <td>121.849998</td>\n      <td>121.949997</td>\n      <td>119.794998</td>\n      <td>119.930000</td>\n      <td>119.930000</td>\n      <td>34515975</td>\n      <td>-0.016806</td>\n      <td>1</td>\n      <td>6.971709e+08</td>\n      <td>...</td>\n      <td>-54.542259</td>\n      <td>1.144553</td>\n      <td>120.287005</td>\n      <td>-1.011100</td>\n      <td>4.174922</td>\n      <td>-23.072724</td>\n      <td>27.247646</td>\n      <td>-1.680603</td>\n      <td>-1.694886</td>\n      <td>115.701439</td>\n    </tr>\n    <tr>\n      <th>4043</th>\n      <td>4043</td>\n      <td>119.919998</td>\n      <td>120.489998</td>\n      <td>119.470001</td>\n      <td>120.429298</td>\n      <td>120.429298</td>\n      <td>18588318</td>\n      <td>0.004163</td>\n      <td>1</td>\n      <td>7.135468e+08</td>\n      <td>...</td>\n      <td>-51.346916</td>\n      <td>1.091641</td>\n      <td>120.288723</td>\n      <td>-1.064449</td>\n      <td>14.697685</td>\n      <td>-15.518642</td>\n      <td>30.216326</td>\n      <td>0.416325</td>\n      <td>0.415460</td>\n      <td>116.599458</td>\n    </tr>\n    <tr>\n      <th>4044</th>\n      <td>4044</td>\n      <td>120.419998</td>\n      <td>120.500000</td>\n      <td>119.794998</td>\n      <td>119.861000</td>\n      <td>119.861000</td>\n      <td>11044540</td>\n      <td>-0.004719</td>\n      <td>1</td>\n      <td>7.045703e+08</td>\n      <td>...</td>\n      <td>-54.983837</td>\n      <td>1.043391</td>\n      <td>120.278308</td>\n      <td>-1.006774</td>\n      <td>15.112980</td>\n      <td>-9.392318</td>\n      <td>24.505297</td>\n      <td>-0.471894</td>\n      <td>-0.473011</td>\n      <td>115.577338</td>\n    </tr>\n    <tr>\n      <th>4045</th>\n      <td>4045</td>\n      <td>119.867500</td>\n      <td>120.410004</td>\n      <td>119.550003</td>\n      <td>120.327003</td>\n      <td>120.327003</td>\n      <td>13045822</td>\n      <td>0.003888</td>\n      <td>1</td>\n      <td>7.150979e+08</td>\n      <td>...</td>\n      <td>-52.001570</td>\n      <td>0.866795</td>\n      <td>120.279079</td>\n      <td>-0.597271</td>\n      <td>16.801027</td>\n      <td>-4.153649</td>\n      <td>20.954675</td>\n      <td>0.388787</td>\n      <td>0.388033</td>\n      <td>116.415474</td>\n    </tr>\n    <tr>\n      <th>4046</th>\n      <td>4046</td>\n      <td>120.324997</td>\n      <td>120.480003</td>\n      <td>119.739998</td>\n      <td>120.309998</td>\n      <td>120.309998</td>\n      <td>10712122</td>\n      <td>-0.000141</td>\n      <td>1</td>\n      <td>7.208881e+08</td>\n      <td>...</td>\n      <td>-52.110402</td>\n      <td>0.605722</td>\n      <td>120.279268</td>\n      <td>-0.717942</td>\n      <td>15.950082</td>\n      <td>-0.132903</td>\n      <td>16.082984</td>\n      <td>-0.014133</td>\n      <td>-0.014134</td>\n      <td>116.384888</td>\n    </tr>\n  </tbody>\n</table>\n<p>4046 rows Ã— 92 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train= x.iloc[:-ROWS_TO_PREDICT]\n",
    "y_train= y.iloc[:-ROWS_TO_PREDICT]\n",
    "x_test =x.iloc[-ROWS_TO_PREDICT:]\n",
    "y_test=y.iloc[-ROWS_TO_PREDICT:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:07:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "------------\n",
      "eta: \n",
      "0.9583127775037\n",
      "0.484375\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# for i in range(2,20):\n",
    "#     model = xgb.XGBRFClassifier(nthread =-1,max_depth=i,num_parallel_tree=100,eta =0.1)\n",
    "#     model.fit(x_train,y_train)\n",
    "#     predicted_train = model.predict(x_train)\n",
    "#     predicted_test = model.predict(x_test)\n",
    "#     print(\"------------\")\n",
    "#     print(f'max_depth: {i}')\n",
    "#     print(accuracy_score(y_train.values, predicted_train))\n",
    "#     print(accuracy_score(y_test.values, predicted_test))\n",
    "#     print(\"------------\")\n",
    "#\n",
    "# for i in range(0,100):\n",
    "\n",
    "model = xgb.sklearn.XGBRFClassifier(n_jobs  =-1,max_depth=10,n_estimators =100,eta=2)\n",
    "model.fit(x_train,y_train)\n",
    "predicted_train = model.predict(x_train)\n",
    "predicted_test = model.predict(x_test)\n",
    "print(\"------------\")\n",
    "print(f'eta: ')\n",
    "print(accuracy_score(y_train.values, predicted_train))\n",
    "print(accuracy_score(y_test.values, predicted_test))\n",
    "print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  booster='dart',\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  1,  1, ..., -1, -1, -1], dtype=int64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9965466206216083"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train.values, predicted_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4921875"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test.values, predicted_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.00570821, 0.00381237, 0.00671404, 0.00632185, 0.00920099,\n       0.00962175, 0.00414626, 0.00327022, 0.01308581, 0.01473213,\n       0.01021574, 0.0073754 , 0.00863127, 0.01194806, 0.01144959,\n       0.00496418, 0.01368823, 0.01187784, 0.00808209, 0.01010779,\n       0.01332559, 0.01146628, 0.00927289, 0.00666881, 0.00474349,\n       0.0058435 , 0.01131136, 0.01040499, 0.01207635, 0.0089634 ,\n       0.0068779 , 0.00707698, 0.0058231 , 0.01800122, 0.02046858,\n       0.01484249, 0.01270077, 0.00760605, 0.01038803, 0.01082772,\n       0.01151109, 0.01021596, 0.01260001, 0.01380788, 0.01478382,\n       0.01742299, 0.01117344, 0.00926107, 0.01008793, 0.00659338,\n       0.00702907, 0.00750393, 0.01071193, 0.01008036, 0.00740577,\n       0.00641479, 0.0117988 , 0.01252222, 0.00990448, 0.01581678,\n       0.01933273, 0.02334621, 0.01971269, 0.01980872, 0.02068326,\n       0.00739919, 0.00838184, 0.01333402, 0.01592654, 0.02283322,\n       0.00250278, 0.0035254 , 0.01184978, 0.01096373, 0.00779554,\n       0.00808505, 0.01000679, 0.01333628, 0.00957774, 0.00907977,\n       0.00966856, 0.00921853, 0.0137265 , 0.05189564, 0.01037148,\n       0.00779231, 0.00748134, 0.00845038, 0.00480435, 0.00464161,\n       0.01417904], dtype=float32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "max_depth: 14\n",
      "0.9965466206216083\n",
      "0.4921875\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# model = xgb.XGBRFClassifier(nthread =-1,max_depth=14,n_estimators=1000,\n",
    "#                           eta =0.2)\n",
    "# model.fit(x_train,y_train)\n",
    "predicted_train = model.predict(x_train)\n",
    "predicted_test = model.predict(x_test)\n",
    "print(\"------------\")\n",
    "print(f'max_depth: {14}')\n",
    "print(accuracy_score(y_train.values, predicted_train))\n",
    "print(accuracy_score(y_test.values, predicted_test))\n",
    "print(\"------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}