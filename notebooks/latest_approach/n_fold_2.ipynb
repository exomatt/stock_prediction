{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from finta import TA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "from xgboost import XGBClassifier\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "WINDOW = 8  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '1y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "\n",
    "# one day 16 rows of data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 Open        High         Low       Close  \\\nDatetime                                                                    \n2020-07-27 15:30:00-04:00   94.675003   94.904999   94.629997   94.803749   \n2020-07-27 16:00:00-04:00   94.805000   94.975000   94.795000   94.962500   \n2020-07-27 17:00:00-04:00   94.810000   94.992500   94.810000   94.985000   \n2020-07-27 18:00:00-04:00   94.985000   95.125000   94.957500   95.085000   \n2020-07-27 19:00:00-04:00   95.087500   95.247500   95.045000   95.047500   \n...                               ...         ...         ...         ...   \n2021-07-27 12:30:00-04:00  146.380005  146.470001  145.718903  145.945007   \n2021-07-27 13:30:00-04:00  145.925003  146.679993  145.860001  146.664993   \n2021-07-27 14:30:00-04:00  146.664993  147.320007  146.440002  146.615005   \n2021-07-27 15:30:00-04:00  146.610001  146.639999  146.279999  146.389999   \n2021-07-27 15:50:47-04:00  146.501099  146.501099  146.501099  146.501099   \n\n                            Adj Close    Volume  \nDatetime                                         \n2020-07-27 15:30:00-04:00   94.803749         0  \n2020-07-27 16:00:00-04:00   94.962500         0  \n2020-07-27 17:00:00-04:00   94.985000         0  \n2020-07-27 18:00:00-04:00   95.085000         0  \n2020-07-27 19:00:00-04:00   95.047500         0  \n...                               ...       ...  \n2021-07-27 12:30:00-04:00  145.945007   8389728  \n2021-07-27 13:30:00-04:00  146.664993   7438855  \n2021-07-27 14:30:00-04:00  146.615005  11257092  \n2021-07-27 15:30:00-04:00  146.389999   4227033  \n2021-07-27 15:50:47-04:00  146.501099         0  \n\n[4191 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-27 15:30:00-04:00</th>\n      <td>94.675003</td>\n      <td>94.904999</td>\n      <td>94.629997</td>\n      <td>94.803749</td>\n      <td>94.803749</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 16:00:00-04:00</th>\n      <td>94.805000</td>\n      <td>94.975000</td>\n      <td>94.795000</td>\n      <td>94.962500</td>\n      <td>94.962500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 17:00:00-04:00</th>\n      <td>94.810000</td>\n      <td>94.992500</td>\n      <td>94.810000</td>\n      <td>94.985000</td>\n      <td>94.985000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 18:00:00-04:00</th>\n      <td>94.985000</td>\n      <td>95.125000</td>\n      <td>94.957500</td>\n      <td>95.085000</td>\n      <td>95.085000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 19:00:00-04:00</th>\n      <td>95.087500</td>\n      <td>95.247500</td>\n      <td>95.045000</td>\n      <td>95.047500</td>\n      <td>95.047500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 12:30:00-04:00</th>\n      <td>146.380005</td>\n      <td>146.470001</td>\n      <td>145.718903</td>\n      <td>145.945007</td>\n      <td>145.945007</td>\n      <td>8389728</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 13:30:00-04:00</th>\n      <td>145.925003</td>\n      <td>146.679993</td>\n      <td>145.860001</td>\n      <td>146.664993</td>\n      <td>146.664993</td>\n      <td>7438855</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 14:30:00-04:00</th>\n      <td>146.664993</td>\n      <td>147.320007</td>\n      <td>146.440002</td>\n      <td>146.615005</td>\n      <td>146.615005</td>\n      <td>11257092</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:30:00-04:00</th>\n      <td>146.610001</td>\n      <td>146.639999</td>\n      <td>146.279999</td>\n      <td>146.389999</td>\n      <td>146.389999</td>\n      <td>4227033</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:50:47-04:00</th>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4191 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download(  # or pdr.get_data_yahoo(...\n",
    "    tickers=symbol,\n",
    "\n",
    "    period=INTERVAL,\n",
    "\n",
    "    interval=FETCH_INTERVAL,\n",
    "\n",
    "    # group by ticker (to access via data['SPY'])\n",
    "    # (optional, default is 'column')\n",
    "    group_by='ticker',\n",
    "\n",
    "    # adjust all OHLC automatically\n",
    "    # (optional, default is False)\n",
    "    # auto_adjust = True,\n",
    "\n",
    "    # download pre/post regular market hours data\n",
    "    # (optional, default is False)\n",
    "    prepost=True,\n",
    "\n",
    "    # use threads for mass downloading? (True/False/Integer)\n",
    "    # (optional, default is True)\n",
    "    threads=False,\n",
    "\n",
    "    # proxy URL scheme use use when downloading?\n",
    "    # (optional, default is None)\n",
    "    proxy=None\n",
    ")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n",
    "data.head(10)\n",
    "important_columns = ['open', 'high', 'low', 'close', 'volume']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_diffs(diff_number, col_name):\n",
    "    new_col_name = f'{col_name}_{diff_number}'\n",
    "    data[new_col_name] = data[col_name].diff(diff_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for name in important_columns:\n",
    "#     for i in range(1, 11):\n",
    "#         calculate_diffs(i, name)\n",
    "#\n",
    "# data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Datetime'>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['close'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                open       high        low      close  \\\nDatetime                                                                \n2020-07-27 15:30:00-04:00  94.675003  94.904999  94.629997  94.803749   \n2020-07-27 16:00:00-04:00  94.805000  94.975000  94.795000  94.962500   \n2020-07-27 17:00:00-04:00  94.810000  94.992500  94.810000  94.985000   \n2020-07-27 18:00:00-04:00  94.985000  95.125000  94.957500  95.085000   \n2020-07-27 19:00:00-04:00  95.087500  95.247500  95.045000  95.047500   \n\n                           Adj Close  volume  close_pct  \nDatetime                                                 \n2020-07-27 15:30:00-04:00  94.803749       0        NaN  \n2020-07-27 16:00:00-04:00  94.962500       0   0.001675  \n2020-07-27 17:00:00-04:00  94.985000       0   0.000237  \n2020-07-27 18:00:00-04:00  95.085000       0   0.001053  \n2020-07-27 19:00:00-04:00  95.047500       0  -0.000394  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-27 15:30:00-04:00</th>\n      <td>94.675003</td>\n      <td>94.904999</td>\n      <td>94.629997</td>\n      <td>94.803749</td>\n      <td>94.803749</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 16:00:00-04:00</th>\n      <td>94.805000</td>\n      <td>94.975000</td>\n      <td>94.795000</td>\n      <td>94.962500</td>\n      <td>94.962500</td>\n      <td>0</td>\n      <td>0.001675</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 17:00:00-04:00</th>\n      <td>94.810000</td>\n      <td>94.992500</td>\n      <td>94.810000</td>\n      <td>94.985000</td>\n      <td>94.985000</td>\n      <td>0</td>\n      <td>0.000237</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 18:00:00-04:00</th>\n      <td>94.985000</td>\n      <td>95.125000</td>\n      <td>94.957500</td>\n      <td>95.085000</td>\n      <td>95.085000</td>\n      <td>0</td>\n      <td>0.001053</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 19:00:00-04:00</th>\n      <td>95.087500</td>\n      <td>95.247500</td>\n      <td>95.045000</td>\n      <td>95.047500</td>\n      <td>95.047500</td>\n      <td>0</td>\n      <td>-0.000394</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close_pct'] = data['close'].pct_change()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "              open         high          low        close    Adj Close  \\\ncount  4191.000000  4191.000000  4191.000000  4191.000000  4191.000000   \nmean    125.244864   125.825161   124.527782   125.244412   125.244412   \nstd      10.104013    11.241192    10.306301    10.105416    10.105416   \nmin      93.252500    93.477500    58.360000    93.269997    93.269997   \n25%     118.043499   118.565000   117.107499   118.035198   118.035198   \n50%     125.351100   125.840000   124.670000   125.300003   125.300003   \n75%     132.145000   132.660000   131.720001   132.156700   132.156700   \nmax     149.900000   438.440000   149.510000   149.900000   149.900000   \n\n             volume    close_pct  \ncount  4.191000e+03  4190.000000  \nmean   5.564536e+06     0.000117  \nstd    8.851621e+06     0.005194  \nmin    0.000000e+00    -0.051319  \n25%    0.000000e+00    -0.001697  \n50%    0.000000e+00     0.000085  \n75%    9.182685e+06     0.002009  \nmax    9.845401e+07     0.051457  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4191.000000</td>\n      <td>4191.000000</td>\n      <td>4191.000000</td>\n      <td>4191.000000</td>\n      <td>4191.000000</td>\n      <td>4.191000e+03</td>\n      <td>4190.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>125.244864</td>\n      <td>125.825161</td>\n      <td>124.527782</td>\n      <td>125.244412</td>\n      <td>125.244412</td>\n      <td>5.564536e+06</td>\n      <td>0.000117</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>10.104013</td>\n      <td>11.241192</td>\n      <td>10.306301</td>\n      <td>10.105416</td>\n      <td>10.105416</td>\n      <td>8.851621e+06</td>\n      <td>0.005194</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>93.252500</td>\n      <td>93.477500</td>\n      <td>58.360000</td>\n      <td>93.269997</td>\n      <td>93.269997</td>\n      <td>0.000000e+00</td>\n      <td>-0.051319</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>118.043499</td>\n      <td>118.565000</td>\n      <td>117.107499</td>\n      <td>118.035198</td>\n      <td>118.035198</td>\n      <td>0.000000e+00</td>\n      <td>-0.001697</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>125.351100</td>\n      <td>125.840000</td>\n      <td>124.670000</td>\n      <td>125.300003</td>\n      <td>125.300003</td>\n      <td>0.000000e+00</td>\n      <td>0.000085</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>132.145000</td>\n      <td>132.660000</td>\n      <td>131.720001</td>\n      <td>132.156700</td>\n      <td>132.156700</td>\n      <td>9.182685e+06</td>\n      <td>0.002009</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>149.900000</td>\n      <td>438.440000</td>\n      <td>149.510000</td>\n      <td>149.900000</td>\n      <td>149.900000</td>\n      <td>9.845401e+07</td>\n      <td>0.051457</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def _get_indicator_data(data):\n",
    "    data = add_all_ta_features(\n",
    "        data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-07-27 15:30:00-04:00   94.675003   94.904999   94.629997   94.803749   \n2020-07-27 16:00:00-04:00   94.805000   94.975000   94.795000   94.962500   \n2020-07-27 17:00:00-04:00   94.810000   94.992500   94.810000   94.985000   \n2020-07-27 18:00:00-04:00   94.985000   95.125000   94.957500   95.085000   \n2020-07-27 19:00:00-04:00   95.087500   95.247500   95.045000   95.047500   \n...                               ...         ...         ...         ...   \n2021-07-27 12:30:00-04:00  146.380005  146.470001  145.718903  145.945007   \n2021-07-27 13:30:00-04:00  145.925003  146.679993  145.860001  146.664993   \n2021-07-27 14:30:00-04:00  146.664993  147.320007  146.440002  146.615005   \n2021-07-27 15:30:00-04:00  146.610001  146.639999  146.279999  146.389999   \n2021-07-27 15:50:47-04:00  146.501099  146.501099  146.501099  146.501099   \n\n                            Adj Close    volume  close_pct  \nDatetime                                                    \n2020-07-27 15:30:00-04:00   94.803749         0        NaN  \n2020-07-27 16:00:00-04:00   94.962500         0   0.001675  \n2020-07-27 17:00:00-04:00   94.985000         0   0.000237  \n2020-07-27 18:00:00-04:00   95.085000         0   0.001053  \n2020-07-27 19:00:00-04:00   95.047500         0  -0.000394  \n...                               ...       ...        ...  \n2021-07-27 12:30:00-04:00  145.945007   8389728  -0.003040  \n2021-07-27 13:30:00-04:00  146.664993   7438855   0.004933  \n2021-07-27 14:30:00-04:00  146.615005  11257092  -0.000341  \n2021-07-27 15:30:00-04:00  146.389999   4227033  -0.001535  \n2021-07-27 15:50:47-04:00  146.501099         0   0.000759  \n\n[4191 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-27 15:30:00-04:00</th>\n      <td>94.675003</td>\n      <td>94.904999</td>\n      <td>94.629997</td>\n      <td>94.803749</td>\n      <td>94.803749</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 16:00:00-04:00</th>\n      <td>94.805000</td>\n      <td>94.975000</td>\n      <td>94.795000</td>\n      <td>94.962500</td>\n      <td>94.962500</td>\n      <td>0</td>\n      <td>0.001675</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 17:00:00-04:00</th>\n      <td>94.810000</td>\n      <td>94.992500</td>\n      <td>94.810000</td>\n      <td>94.985000</td>\n      <td>94.985000</td>\n      <td>0</td>\n      <td>0.000237</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 18:00:00-04:00</th>\n      <td>94.985000</td>\n      <td>95.125000</td>\n      <td>94.957500</td>\n      <td>95.085000</td>\n      <td>95.085000</td>\n      <td>0</td>\n      <td>0.001053</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 19:00:00-04:00</th>\n      <td>95.087500</td>\n      <td>95.247500</td>\n      <td>95.045000</td>\n      <td>95.047500</td>\n      <td>95.047500</td>\n      <td>0</td>\n      <td>-0.000394</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 12:30:00-04:00</th>\n      <td>146.380005</td>\n      <td>146.470001</td>\n      <td>145.718903</td>\n      <td>145.945007</td>\n      <td>145.945007</td>\n      <td>8389728</td>\n      <td>-0.003040</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 13:30:00-04:00</th>\n      <td>145.925003</td>\n      <td>146.679993</td>\n      <td>145.860001</td>\n      <td>146.664993</td>\n      <td>146.664993</td>\n      <td>7438855</td>\n      <td>0.004933</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 14:30:00-04:00</th>\n      <td>146.664993</td>\n      <td>147.320007</td>\n      <td>146.440002</td>\n      <td>146.615005</td>\n      <td>146.615005</td>\n      <td>11257092</td>\n      <td>-0.000341</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:30:00-04:00</th>\n      <td>146.610001</td>\n      <td>146.639999</td>\n      <td>146.279999</td>\n      <td>146.389999</td>\n      <td>146.389999</td>\n      <td>4227033</td>\n      <td>-0.001535</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:50:47-04:00</th>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>0</td>\n      <td>0.000759</td>\n    </tr>\n  </tbody>\n</table>\n<p>4191 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['open', 'high', 'low', 'close', 'Adj Close', 'volume', 'close_pct'], dtype='object')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def create_class_column(row, lowest_threshold, higher_threshold):\n",
    "    if row['close_shift'] - row['close'] > higher_threshold:\n",
    "        return 1\n",
    "    if row['close_shift'] - row['close'] < lowest_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-07-27 15:30:00-04:00   94.675003   94.904999   94.629997   94.803749   \n2020-07-27 16:00:00-04:00   94.805000   94.975000   94.795000   94.962500   \n2020-07-27 17:00:00-04:00   94.810000   94.992500   94.810000   94.985000   \n2020-07-27 18:00:00-04:00   94.985000   95.125000   94.957500   95.085000   \n2020-07-27 19:00:00-04:00   95.087500   95.247500   95.045000   95.047500   \n...                               ...         ...         ...         ...   \n2021-07-27 12:30:00-04:00  146.380005  146.470001  145.718903  145.945007   \n2021-07-27 13:30:00-04:00  145.925003  146.679993  145.860001  146.664993   \n2021-07-27 14:30:00-04:00  146.664993  147.320007  146.440002  146.615005   \n2021-07-27 15:30:00-04:00  146.610001  146.639999  146.279999  146.389999   \n2021-07-27 15:50:47-04:00  146.501099  146.501099  146.501099  146.501099   \n\n                            Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                   \n2020-07-27 15:30:00-04:00   94.803749         0        NaN  0.000000e+00   \n2020-07-27 16:00:00-04:00   94.962500         0   0.001675  0.000000e+00   \n2020-07-27 17:00:00-04:00   94.985000         0   0.000237  0.000000e+00   \n2020-07-27 18:00:00-04:00   95.085000         0   0.001053  0.000000e+00   \n2020-07-27 19:00:00-04:00   95.047500         0  -0.000394  0.000000e+00   \n...                               ...       ...        ...           ...   \n2021-07-27 12:30:00-04:00  145.945007   8389728  -0.003040  9.246432e+08   \n2021-07-27 13:30:00-04:00  146.664993   7438855   0.004933  9.318099e+08   \n2021-07-27 14:30:00-04:00  146.615005  11257092  -0.000341  9.250301e+08   \n2021-07-27 15:30:00-04:00  146.389999   4227033  -0.001535  9.233863e+08   \n2021-07-27 15:50:47-04:00  146.501099         0   0.000759  9.233863e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_ao  \\\nDatetime                                           ...                \n2020-07-27 15:30:00-04:00           0    0.000000  ...     0.000000   \n2020-07-27 16:00:00-04:00           0    0.000000  ...     0.000000   \n2020-07-27 17:00:00-04:00           0    0.000000  ...     0.000000   \n2020-07-27 18:00:00-04:00           0    0.000000  ...     0.000000   \n2020-07-27 19:00:00-04:00           0    0.000000  ...     0.000000   \n...                               ...         ...  ...          ...   \n2021-07-27 12:30:00-04:00  -275542293   -0.157192  ...    -1.130783   \n2021-07-27 13:30:00-04:00  -268103438   -0.135439  ...    -1.662215   \n2021-07-27 14:30:00-04:00  -279360530   -0.142938  ...    -1.848552   \n2021-07-27 15:30:00-04:00  -283587563   -0.192065  ...    -1.919113   \n2021-07-27 15:50:47-04:00  -283587563   -0.202338  ...    -1.806513   \n\n                           momentum_kama  momentum_roc  momentum_ppo  \\\nDatetime                                                               \n2020-07-27 15:30:00-04:00      94.803749      0.000000      0.000000   \n2020-07-27 16:00:00-04:00      94.880152      0.000000      0.000000   \n2020-07-27 17:00:00-04:00      94.927593      0.000000      0.000000   \n2020-07-27 18:00:00-04:00      94.997069      0.000000      0.000000   \n2020-07-27 19:00:00-04:00      95.018962      0.000000      0.000000   \n...                                  ...           ...           ...   \n2021-07-27 12:30:00-04:00     147.388930     -2.247148     32.495617   \n2021-07-27 13:30:00-04:00     147.313767     -1.764907     29.854762   \n2021-07-27 14:30:00-04:00     147.221537     -1.772072     31.313111   \n2021-07-27 15:30:00-04:00     147.089313     -1.553464     24.776442   \n2021-07-27 15:50:47-04:00     146.980923     -1.611082     14.026471   \n\n                           momentum_ppo_signal  momentum_ppo_hist  others_dr  \\\nDatetime                                                                       \n2020-07-27 15:30:00-04:00             0.000000           0.000000 -24.305007   \n2020-07-27 16:00:00-04:00             0.000000           0.000000   0.167452   \n2020-07-27 17:00:00-04:00             0.000000           0.000000   0.023694   \n2020-07-27 18:00:00-04:00             0.000000           0.000000   0.105280   \n2020-07-27 19:00:00-04:00             0.000000           0.000000  -0.039438   \n...                                        ...                ...        ...   \n2021-07-27 12:30:00-04:00             3.798693          28.696924  -0.303977   \n2021-07-27 13:30:00-04:00             9.009906          20.844855   0.493327   \n2021-07-27 14:30:00-04:00            13.470547          17.842563  -0.034083   \n2021-07-27 15:30:00-04:00            15.731726           9.044716  -0.153467   \n2021-07-27 15:50:47-04:00            15.390675          -1.364204   0.075893   \n\n                           others_dlr  others_cr  close_shift  \nDatetime                                                       \n2020-07-27 15:30:00-04:00    0.000000   0.000000    94.625000  \n2020-07-27 16:00:00-04:00    0.167312   0.167452    94.572500  \n2020-07-27 17:00:00-04:00    0.023691   0.191185    94.375000  \n2020-07-27 18:00:00-04:00    0.105224   0.296666    93.680000  \n2020-07-27 19:00:00-04:00   -0.039446   0.257111    93.949997  \n...                               ...        ...          ...  \n2021-07-27 12:30:00-04:00   -0.304440  53.944342          NaN  \n2021-07-27 13:30:00-04:00    0.492114  54.703790          NaN  \n2021-07-27 14:30:00-04:00   -0.034089  54.651063          NaN  \n2021-07-27 15:30:00-04:00   -0.153585  54.413724          NaN  \n2021-07-27 15:50:47-04:00    0.075864  54.530913          NaN  \n\n[4191 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_ao</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-27 15:30:00-04:00</th>\n      <td>94.675003</td>\n      <td>94.904999</td>\n      <td>94.629997</td>\n      <td>94.803749</td>\n      <td>94.803749</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>94.803749</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-24.305007</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>94.625000</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 16:00:00-04:00</th>\n      <td>94.805000</td>\n      <td>94.975000</td>\n      <td>94.795000</td>\n      <td>94.962500</td>\n      <td>94.962500</td>\n      <td>0</td>\n      <td>0.001675</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>94.880152</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.167452</td>\n      <td>0.167312</td>\n      <td>0.167452</td>\n      <td>94.572500</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 17:00:00-04:00</th>\n      <td>94.810000</td>\n      <td>94.992500</td>\n      <td>94.810000</td>\n      <td>94.985000</td>\n      <td>94.985000</td>\n      <td>0</td>\n      <td>0.000237</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>94.927593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.023694</td>\n      <td>0.023691</td>\n      <td>0.191185</td>\n      <td>94.375000</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 18:00:00-04:00</th>\n      <td>94.985000</td>\n      <td>95.125000</td>\n      <td>94.957500</td>\n      <td>95.085000</td>\n      <td>95.085000</td>\n      <td>0</td>\n      <td>0.001053</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>94.997069</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.105280</td>\n      <td>0.105224</td>\n      <td>0.296666</td>\n      <td>93.680000</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 19:00:00-04:00</th>\n      <td>95.087500</td>\n      <td>95.247500</td>\n      <td>95.045000</td>\n      <td>95.047500</td>\n      <td>95.047500</td>\n      <td>0</td>\n      <td>-0.000394</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>95.018962</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.039438</td>\n      <td>-0.039446</td>\n      <td>0.257111</td>\n      <td>93.949997</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 12:30:00-04:00</th>\n      <td>146.380005</td>\n      <td>146.470001</td>\n      <td>145.718903</td>\n      <td>145.945007</td>\n      <td>145.945007</td>\n      <td>8389728</td>\n      <td>-0.003040</td>\n      <td>9.246432e+08</td>\n      <td>-275542293</td>\n      <td>-0.157192</td>\n      <td>...</td>\n      <td>-1.130783</td>\n      <td>147.388930</td>\n      <td>-2.247148</td>\n      <td>32.495617</td>\n      <td>3.798693</td>\n      <td>28.696924</td>\n      <td>-0.303977</td>\n      <td>-0.304440</td>\n      <td>53.944342</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 13:30:00-04:00</th>\n      <td>145.925003</td>\n      <td>146.679993</td>\n      <td>145.860001</td>\n      <td>146.664993</td>\n      <td>146.664993</td>\n      <td>7438855</td>\n      <td>0.004933</td>\n      <td>9.318099e+08</td>\n      <td>-268103438</td>\n      <td>-0.135439</td>\n      <td>...</td>\n      <td>-1.662215</td>\n      <td>147.313767</td>\n      <td>-1.764907</td>\n      <td>29.854762</td>\n      <td>9.009906</td>\n      <td>20.844855</td>\n      <td>0.493327</td>\n      <td>0.492114</td>\n      <td>54.703790</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 14:30:00-04:00</th>\n      <td>146.664993</td>\n      <td>147.320007</td>\n      <td>146.440002</td>\n      <td>146.615005</td>\n      <td>146.615005</td>\n      <td>11257092</td>\n      <td>-0.000341</td>\n      <td>9.250301e+08</td>\n      <td>-279360530</td>\n      <td>-0.142938</td>\n      <td>...</td>\n      <td>-1.848552</td>\n      <td>147.221537</td>\n      <td>-1.772072</td>\n      <td>31.313111</td>\n      <td>13.470547</td>\n      <td>17.842563</td>\n      <td>-0.034083</td>\n      <td>-0.034089</td>\n      <td>54.651063</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:30:00-04:00</th>\n      <td>146.610001</td>\n      <td>146.639999</td>\n      <td>146.279999</td>\n      <td>146.389999</td>\n      <td>146.389999</td>\n      <td>4227033</td>\n      <td>-0.001535</td>\n      <td>9.233863e+08</td>\n      <td>-283587563</td>\n      <td>-0.192065</td>\n      <td>...</td>\n      <td>-1.919113</td>\n      <td>147.089313</td>\n      <td>-1.553464</td>\n      <td>24.776442</td>\n      <td>15.731726</td>\n      <td>9.044716</td>\n      <td>-0.153467</td>\n      <td>-0.153585</td>\n      <td>54.413724</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:50:47-04:00</th>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>0</td>\n      <td>0.000759</td>\n      <td>9.233863e+08</td>\n      <td>-283587563</td>\n      <td>-0.202338</td>\n      <td>...</td>\n      <td>-1.806513</td>\n      <td>146.980923</td>\n      <td>-1.611082</td>\n      <td>14.026471</td>\n      <td>15.390675</td>\n      <td>-1.364204</td>\n      <td>0.075893</td>\n      <td>0.075864</td>\n      <td>54.530913</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4191 rows Ã— 91 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _get_indicator_data(data)\n",
    "data['close_shift'] = data.shift(-WINDOW)['close']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    1412\n",
      "-1    1400\n",
      " 0    1379\n",
      "Name: class_column, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-07-27 15:30:00-04:00   94.675003   94.904999   94.629997   94.803749   \n2020-07-27 16:00:00-04:00   94.805000   94.975000   94.795000   94.962500   \n2020-07-27 17:00:00-04:00   94.810000   94.992500   94.810000   94.985000   \n2020-07-27 18:00:00-04:00   94.985000   95.125000   94.957500   95.085000   \n2020-07-27 19:00:00-04:00   95.087500   95.247500   95.045000   95.047500   \n...                               ...         ...         ...         ...   \n2021-07-27 12:30:00-04:00  146.380005  146.470001  145.718903  145.945007   \n2021-07-27 13:30:00-04:00  145.925003  146.679993  145.860001  146.664993   \n2021-07-27 14:30:00-04:00  146.664993  147.320007  146.440002  146.615005   \n2021-07-27 15:30:00-04:00  146.610001  146.639999  146.279999  146.389999   \n2021-07-27 15:50:47-04:00  146.501099  146.501099  146.501099  146.501099   \n\n                            Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                   \n2020-07-27 15:30:00-04:00   94.803749         0        NaN  0.000000e+00   \n2020-07-27 16:00:00-04:00   94.962500         0   0.001675  0.000000e+00   \n2020-07-27 17:00:00-04:00   94.985000         0   0.000237  0.000000e+00   \n2020-07-27 18:00:00-04:00   95.085000         0   0.001053  0.000000e+00   \n2020-07-27 19:00:00-04:00   95.047500         0  -0.000394  0.000000e+00   \n...                               ...       ...        ...           ...   \n2021-07-27 12:30:00-04:00  145.945007   8389728  -0.003040  9.246432e+08   \n2021-07-27 13:30:00-04:00  146.664993   7438855   0.004933  9.318099e+08   \n2021-07-27 14:30:00-04:00  146.615005  11257092  -0.000341  9.250301e+08   \n2021-07-27 15:30:00-04:00  146.389999   4227033  -0.001535  9.233863e+08   \n2021-07-27 15:50:47-04:00  146.501099         0   0.000759  9.233863e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_kama  \\\nDatetime                                           ...                  \n2020-07-27 15:30:00-04:00           0    0.000000  ...      94.803749   \n2020-07-27 16:00:00-04:00           0    0.000000  ...      94.880152   \n2020-07-27 17:00:00-04:00           0    0.000000  ...      94.927593   \n2020-07-27 18:00:00-04:00           0    0.000000  ...      94.997069   \n2020-07-27 19:00:00-04:00           0    0.000000  ...      95.018962   \n...                               ...         ...  ...            ...   \n2021-07-27 12:30:00-04:00  -275542293   -0.157192  ...     147.388930   \n2021-07-27 13:30:00-04:00  -268103438   -0.135439  ...     147.313767   \n2021-07-27 14:30:00-04:00  -279360530   -0.142938  ...     147.221537   \n2021-07-27 15:30:00-04:00  -283587563   -0.192065  ...     147.089313   \n2021-07-27 15:50:47-04:00  -283587563   -0.202338  ...     146.980923   \n\n                           momentum_roc  momentum_ppo  momentum_ppo_signal  \\\nDatetime                                                                     \n2020-07-27 15:30:00-04:00      0.000000      0.000000             0.000000   \n2020-07-27 16:00:00-04:00      0.000000      0.000000             0.000000   \n2020-07-27 17:00:00-04:00      0.000000      0.000000             0.000000   \n2020-07-27 18:00:00-04:00      0.000000      0.000000             0.000000   \n2020-07-27 19:00:00-04:00      0.000000      0.000000             0.000000   \n...                                 ...           ...                  ...   \n2021-07-27 12:30:00-04:00     -2.247148     32.495617             3.798693   \n2021-07-27 13:30:00-04:00     -1.764907     29.854762             9.009906   \n2021-07-27 14:30:00-04:00     -1.772072     31.313111            13.470547   \n2021-07-27 15:30:00-04:00     -1.553464     24.776442            15.731726   \n2021-07-27 15:50:47-04:00     -1.611082     14.026471            15.390675   \n\n                           momentum_ppo_hist  others_dr  others_dlr  \\\nDatetime                                                              \n2020-07-27 15:30:00-04:00           0.000000 -24.305007    0.000000   \n2020-07-27 16:00:00-04:00           0.000000   0.167452    0.167312   \n2020-07-27 17:00:00-04:00           0.000000   0.023694    0.023691   \n2020-07-27 18:00:00-04:00           0.000000   0.105280    0.105224   \n2020-07-27 19:00:00-04:00           0.000000  -0.039438   -0.039446   \n...                                      ...        ...         ...   \n2021-07-27 12:30:00-04:00          28.696924  -0.303977   -0.304440   \n2021-07-27 13:30:00-04:00          20.844855   0.493327    0.492114   \n2021-07-27 14:30:00-04:00          17.842563  -0.034083   -0.034089   \n2021-07-27 15:30:00-04:00           9.044716  -0.153467   -0.153585   \n2021-07-27 15:50:47-04:00          -1.364204   0.075893    0.075864   \n\n                           others_cr  close_shift  class_column  \nDatetime                                                         \n2020-07-27 15:30:00-04:00   0.000000    94.625000             0  \n2020-07-27 16:00:00-04:00   0.167452    94.572500             0  \n2020-07-27 17:00:00-04:00   0.191185    94.375000            -1  \n2020-07-27 18:00:00-04:00   0.296666    93.680000            -1  \n2020-07-27 19:00:00-04:00   0.257111    93.949997            -1  \n...                              ...          ...           ...  \n2021-07-27 12:30:00-04:00  53.944342          NaN             0  \n2021-07-27 13:30:00-04:00  54.703790          NaN             0  \n2021-07-27 14:30:00-04:00  54.651063          NaN             0  \n2021-07-27 15:30:00-04:00  54.413724          NaN             0  \n2021-07-27 15:50:47-04:00  54.530913          NaN             0  \n\n[4191 rows x 92 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n      <th>class_column</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-27 15:30:00-04:00</th>\n      <td>94.675003</td>\n      <td>94.904999</td>\n      <td>94.629997</td>\n      <td>94.803749</td>\n      <td>94.803749</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>94.803749</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-24.305007</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>94.625000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 16:00:00-04:00</th>\n      <td>94.805000</td>\n      <td>94.975000</td>\n      <td>94.795000</td>\n      <td>94.962500</td>\n      <td>94.962500</td>\n      <td>0</td>\n      <td>0.001675</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>94.880152</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.167452</td>\n      <td>0.167312</td>\n      <td>0.167452</td>\n      <td>94.572500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 17:00:00-04:00</th>\n      <td>94.810000</td>\n      <td>94.992500</td>\n      <td>94.810000</td>\n      <td>94.985000</td>\n      <td>94.985000</td>\n      <td>0</td>\n      <td>0.000237</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>94.927593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.023694</td>\n      <td>0.023691</td>\n      <td>0.191185</td>\n      <td>94.375000</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 18:00:00-04:00</th>\n      <td>94.985000</td>\n      <td>95.125000</td>\n      <td>94.957500</td>\n      <td>95.085000</td>\n      <td>95.085000</td>\n      <td>0</td>\n      <td>0.001053</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>94.997069</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.105280</td>\n      <td>0.105224</td>\n      <td>0.296666</td>\n      <td>93.680000</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-07-27 19:00:00-04:00</th>\n      <td>95.087500</td>\n      <td>95.247500</td>\n      <td>95.045000</td>\n      <td>95.047500</td>\n      <td>95.047500</td>\n      <td>0</td>\n      <td>-0.000394</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>95.018962</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.039438</td>\n      <td>-0.039446</td>\n      <td>0.257111</td>\n      <td>93.949997</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 12:30:00-04:00</th>\n      <td>146.380005</td>\n      <td>146.470001</td>\n      <td>145.718903</td>\n      <td>145.945007</td>\n      <td>145.945007</td>\n      <td>8389728</td>\n      <td>-0.003040</td>\n      <td>9.246432e+08</td>\n      <td>-275542293</td>\n      <td>-0.157192</td>\n      <td>...</td>\n      <td>147.388930</td>\n      <td>-2.247148</td>\n      <td>32.495617</td>\n      <td>3.798693</td>\n      <td>28.696924</td>\n      <td>-0.303977</td>\n      <td>-0.304440</td>\n      <td>53.944342</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 13:30:00-04:00</th>\n      <td>145.925003</td>\n      <td>146.679993</td>\n      <td>145.860001</td>\n      <td>146.664993</td>\n      <td>146.664993</td>\n      <td>7438855</td>\n      <td>0.004933</td>\n      <td>9.318099e+08</td>\n      <td>-268103438</td>\n      <td>-0.135439</td>\n      <td>...</td>\n      <td>147.313767</td>\n      <td>-1.764907</td>\n      <td>29.854762</td>\n      <td>9.009906</td>\n      <td>20.844855</td>\n      <td>0.493327</td>\n      <td>0.492114</td>\n      <td>54.703790</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 14:30:00-04:00</th>\n      <td>146.664993</td>\n      <td>147.320007</td>\n      <td>146.440002</td>\n      <td>146.615005</td>\n      <td>146.615005</td>\n      <td>11257092</td>\n      <td>-0.000341</td>\n      <td>9.250301e+08</td>\n      <td>-279360530</td>\n      <td>-0.142938</td>\n      <td>...</td>\n      <td>147.221537</td>\n      <td>-1.772072</td>\n      <td>31.313111</td>\n      <td>13.470547</td>\n      <td>17.842563</td>\n      <td>-0.034083</td>\n      <td>-0.034089</td>\n      <td>54.651063</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:30:00-04:00</th>\n      <td>146.610001</td>\n      <td>146.639999</td>\n      <td>146.279999</td>\n      <td>146.389999</td>\n      <td>146.389999</td>\n      <td>4227033</td>\n      <td>-0.001535</td>\n      <td>9.233863e+08</td>\n      <td>-283587563</td>\n      <td>-0.192065</td>\n      <td>...</td>\n      <td>147.089313</td>\n      <td>-1.553464</td>\n      <td>24.776442</td>\n      <td>15.731726</td>\n      <td>9.044716</td>\n      <td>-0.153467</td>\n      <td>-0.153585</td>\n      <td>54.413724</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-07-27 15:50:47-04:00</th>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>146.501099</td>\n      <td>0</td>\n      <td>0.000759</td>\n      <td>9.233863e+08</td>\n      <td>-283587563</td>\n      <td>-0.202338</td>\n      <td>...</td>\n      <td>146.980923</td>\n      <td>-1.611082</td>\n      <td>14.026471</td>\n      <td>15.390675</td>\n      <td>-1.364204</td>\n      <td>0.075893</td>\n      <td>0.075864</td>\n      <td>54.530913</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4191 rows Ã— 92 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_class(df):\n",
    "    higher_threshold = 1.5\n",
    "    lowest_threshold = -1.5\n",
    "    last_values_higher = []\n",
    "    last_values_lower = []\n",
    "    df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)), axis=1)\n",
    "    while True:\n",
    "        class_counts = df['class_column'].value_counts()\n",
    "        if abs(class_counts[0] - class_counts[1]) < 15 and abs(class_counts[0] - class_counts[-1]) < 15:\n",
    "            break\n",
    "\n",
    "        if len(last_values_higher) == 3:\n",
    "            last_values_higher.pop(0)\n",
    "        if len(last_values_lower) == 3:\n",
    "            last_values_lower.pop(0)\n",
    "\n",
    "        last_values_higher.append(higher_threshold)\n",
    "        last_values_lower.append(lowest_threshold)\n",
    "        if class_counts[0] > class_counts[1]:\n",
    "            higher_threshold -= 0.01\n",
    "        if class_counts[0] > class_counts[-1]:\n",
    "            lowest_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[1]:\n",
    "            higher_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[-1]:\n",
    "            lowest_threshold -= 0.01\n",
    "\n",
    "        if higher_threshold in last_values_higher and lowest_threshold in last_values_lower:\n",
    "            break\n",
    "        df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)),\n",
    "                                      axis=1)\n",
    "    print(df['class_column'].value_counts())\n",
    "    return df\n",
    "\n",
    "\n",
    "data = create_class(data)\n",
    "\n",
    "data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\Desktop\\repo\\magisterka_analiza\\data\\results\\train_test\\AAPL_1y_8_diff_27_07_2021 21_50_56_full.csv\n"
     ]
    }
   ],
   "source": [
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\{symbol}_{INTERVAL}_{WINDOW}_diff_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}_full.csv'\n",
    "data.to_csv(filename_to_export, index=True)\n",
    "print(filename_to_export)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1412\n-1    1400\n 0    1379\nName: class_column, dtype: int64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Class divide\n",
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['close_shift'])\n",
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1412\n-1    1400\n 0    1370\nName: class_column, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size=17):\n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i * chunk_size:(i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def train_model(model, train_x, train_y):\n",
    "    model.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "247"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataframe = split_dataframe(data, 17)\n",
    "len(splited_dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column', 'close_shift']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "# x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "classifiers = dict()\n",
    "\n",
    "classifiers['DecisionTreeClassifier 1'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 2'] = DecisionTreeClassifier(max_depth=20, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 3'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='gini',\n",
    "                                                                 splitter='random')\n",
    "classifiers['DecisionTreeClassifier 4'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='entropy',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 5'] = DecisionTreeClassifier(max_depth=15, random_state=0, criterion='entropy',\n",
    "                                                                 splitter='best')\n",
    "classifiers['RandomForestClassifier 4'] = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0,\n",
    "                                                                 criterion='gini', n_jobs=-1)\n",
    "classifiers['RandomForestClassifier 5'] = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0,\n",
    "                                                                 criterion='entropy', n_jobs=-1)\n",
    "classifiers['GradientBoostingClassifier 1'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.1)\n",
    "classifiers['GradientBoostingClassifier 2'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=10,\n",
    "                                                                         learning_rate=0.3)\n",
    "classifiers['GradientBoostingClassifier 3'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.1)\n",
    "classifiers['XGBClassifier 1'] = xgb.XGBClassifier(nthread=-1, max_depth=10, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBClassifier 2'] = xgb.XGBClassifier(nthread=-1, max_depth=14, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBClassifier 3'] = xgb.XGBClassifier(nthread=-1, max_depth=14, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBClassifier 4'] = xgb.XGBClassifier(nthread=-1, max_depth=10, n_estimators=1000, eta=0.5)\n",
    "classifiers['XGBClassifier 5'] = xgb.XGBClassifier(nthread=-1, max_depth=6, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBClassifier 6'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBRFClassifier 1'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=12, n_estimators=100, eta=0.4)\n",
    "classifiers['XGBRFClassifier 2'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=14, n_estimators=100, eta=0.4)\n",
    "classifiers['XGBRFClassifier 3'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=3, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 4'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=6, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 5'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=10, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 6'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=10, n_estimators=100, eta=0.4)\n",
    "classifiers_boosted = dict()\n",
    "classifiers_boosted['GradientBoostingClassifier 1S'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=3,\n",
    "                                                                                  learning_rate=0.1)\n",
    "classifiers_boosted['GradientBoostingClassifier 2S'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=3,\n",
    "                                                                                  learning_rate=0.3)\n",
    "classifiers_boosted['GradientBoostingClassifier 3S'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=2,\n",
    "                                                                                  learning_rate=0.5)\n",
    "# classifiers_boosted['GradientBoostingClassifier 4S'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=2, learning_rate=0.8)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def count_correct(pred_list, original_list):\n",
    "    correct_inc = 0\n",
    "    correct_dec = 0\n",
    "    correct_stag = 0\n",
    "    all = len(pred_list)\n",
    "    all_inc = 0\n",
    "    all_dec = 0\n",
    "    all_stag = 0\n",
    "    for idx, el in enumerate(pred_list):\n",
    "        if original_list[idx] == 1:\n",
    "            all_inc += 1\n",
    "            if el == 1:\n",
    "                correct_inc += 1\n",
    "        if original_list[idx] == -1:\n",
    "            all_dec += 1\n",
    "            if el == -1:\n",
    "                correct_dec += 1\n",
    "        if original_list[idx] == 0:\n",
    "            all_stag += 1\n",
    "            if el == 0:\n",
    "                correct_stag += 1\n",
    "    return correct_inc, correct_dec, correct_stag, all_inc, all_dec, all_stag, all\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    }
   ],
   "source": [
    "print(len(splited_dataframe))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "start\n",
      " 1    582\n",
      " 0    573\n",
      "-1    562\n",
      "Name: class_column, dtype: int64\n",
      " 1    582\n",
      " 0    573\n",
      "-1    562\n",
      "Name: class_column, dtype: int64\n",
      "1717\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[21:52:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 2\n",
      "[21:52:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 3\n",
      "[21:53:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 4\n",
      "[21:53:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[21:53:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[21:53:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[21:53:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[21:53:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[21:53:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[21:53:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[21:53:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[21:53:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kch', 'volatility_kcw', 'volatility_dcw', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_psar_down', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "1.0\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "1.0\n",
      " 1    588\n",
      " 0    576\n",
      "-1    570\n",
      "Name: class_column, dtype: int64\n",
      " 1    588\n",
      " 0    576\n",
      "-1    570\n",
      "Name: class_column, dtype: int64\n",
      "1734\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 1\n",
      "[21:56:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 2\n",
      "[21:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 3\n",
      "[21:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 4\n",
      "[21:56:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 5\n",
      "[21:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 6\n",
      "[21:56:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[21:56:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[21:56:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[21:56:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[21:57:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[21:57:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[21:57:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_psar_down', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.058823529411764705\n",
      " 1    591\n",
      " 0    581\n",
      "-1    579\n",
      "Name: class_column, dtype: int64\n",
      " 1    591\n",
      " 0    581\n",
      "-1    579\n",
      "Name: class_column, dtype: int64\n",
      "1751\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[21:59:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 2\n",
      "[22:00:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[22:00:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[22:00:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 5\n",
      "[22:00:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 6\n",
      "[22:00:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[22:00:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[22:00:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[22:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[22:00:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[22:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[22:00:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_ichimoku_conv', 'trend_ichimoku_b', 'trend_psar_down', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5294117647058824\n",
      " 1    601\n",
      "-1    585\n",
      " 0    582\n",
      "Name: class_column, dtype: int64\n",
      " 1    601\n",
      "-1    585\n",
      " 0    582\n",
      "Name: class_column, dtype: int64\n",
      "1768\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.0\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[22:03:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 2\n",
      "[22:03:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[22:03:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[22:03:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 5\n",
      "[22:03:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[22:03:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[22:04:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[22:04:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[22:04:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[22:04:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[22:04:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[22:04:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_sma_slow', 'trend_ichimoku_a', 'trend_psar_down', 'momentum_kama', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    602\n",
      "-1    592\n",
      " 0    591\n",
      "Name: class_column, dtype: int64\n",
      " 1    602\n",
      "-1    592\n",
      " 0    591\n",
      "Name: class_column, dtype: int64\n",
      "1785\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[22:07:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[22:07:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[22:07:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[22:07:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[22:07:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 6\n",
      "[22:07:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[22:07:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[22:07:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[22:07:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[22:07:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[22:08:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[22:08:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_sma_slow', 'trend_ichimoku_conv', 'trend_psar_down', 'momentum_kama', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "start_data = splited_dataframe[:100]\n",
    "next_data = splited_dataframe[100:105]\n",
    "print(len(next_data))\n",
    "score = defaultdict(list)\n",
    "points = defaultdict(list)\n",
    "points_train = defaultdict(list)\n",
    "score_train = defaultdict(list)\n",
    "step_headers = []\n",
    "i = 0\n",
    "print(\"start\")\n",
    "for idx, day in enumerate(next_data):\n",
    "    start_data.append(day)\n",
    "    data_set = pd.concat(start_data)\n",
    "    data_set = create_class(data_set)\n",
    "    print(data_set['class_column'].value_counts())\n",
    "    y = data_set['class_column']\n",
    "    features = [x for x in data_set.columns if x not in ['class_column']]\n",
    "    x = data_set[features]\n",
    "    x_train = x.iloc[:-17]\n",
    "    y_train = y.iloc[:-17]\n",
    "    x_test = x.iloc[-17:]\n",
    "    y_test = y.iloc[-17:]\n",
    "\n",
    "    print(len(data_set))\n",
    "\n",
    "    step_headers.append(f'<{i}>')\n",
    "    i = i + 1\n",
    "    predictions_train = dict()\n",
    "    predictions = dict()\n",
    "\n",
    "    for k, v in classifiers.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v, x_train, y_train)\n",
    "        predictions_train[k] = v.predict(x_train)\n",
    "        score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k], y_train.values))\n",
    "        points[k].append(count_correct(predictions[k], y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "    rfe = RFE(classifiers['RandomForestClassifier 5'], 10)\n",
    "    fited = rfe.fit(x_train, y_train)\n",
    "    names = x.columns\n",
    "    columns = []\n",
    "    for i in range(len(fited.support_)):\n",
    "        if fited.support_[i]:\n",
    "            columns.append(names[i])\n",
    "\n",
    "    print(\"Columns with predictive power:\", columns)\n",
    "    columns = columns + ['high', 'low', 'volume', 'open']\n",
    "    x_test_cropped = x_test[columns]\n",
    "    x_train_cropped = x_train[columns]\n",
    "    for k, v in classifiers_boosted.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v, x_train_cropped, y_train)\n",
    "        predictions_train[k] = v.predict(x_train_cropped)\n",
    "        score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test_cropped)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k], y_train.values))\n",
    "        points[k].append(count_correct(predictions[k], y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+-----------+----------+----------+----------+----------+\n",
      "|    | Classifier type               |      <0> |      <90> |     <90> |     <90> |     <90> |     mean |\n",
      "|----+-------------------------------+----------+-----------+----------+----------+----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.294118 | 0.0588235 | 0.647059 | 0        | 0.294118 | 0.258824 |\n",
      "|  1 | DecisionTreeClassifier 2      | 0.117647 | 0.0588235 | 0.588235 | 0.117647 | 0.470588 | 0.270588 |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.411765 | 0.117647  | 0.411765 | 0.352941 | 0.470588 | 0.352941 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.176471 | 0.0588235 | 0.294118 | 0.470588 | 0.470588 | 0.294118 |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.176471 | 0.0588235 | 0.294118 | 0.529412 | 0.470588 | 0.305882 |\n",
      "|  5 | RandomForestClassifier 4      | 0.176471 | 0.0588235 | 0.588235 | 0        | 0.470588 | 0.258824 |\n",
      "|  6 | RandomForestClassifier 5      | 0.176471 | 0.0588235 | 0.411765 | 0        | 0.647059 | 0.258824 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.882353 | 0.0588235 | 0.529412 | 0.941176 | 0.529412 | 0.588235 |\n",
      "|  8 | GradientBoostingClassifier 2  | 0.764706 | 0.0588235 | 0.470588 | 0.294118 | 0.470588 | 0.411765 |\n",
      "|  9 | GradientBoostingClassifier 3  | 0.823529 | 0.0588235 | 0.529412 | 0.588235 | 0.823529 | 0.564706 |\n",
      "| 10 | XGBClassifier 1               | 0.941176 | 0.0588235 | 0.352941 | 0.764706 | 0.529412 | 0.529412 |\n",
      "| 11 | XGBClassifier 2               | 0.941176 | 0.0588235 | 0.411765 | 0.647059 | 0.588235 | 0.529412 |\n",
      "| 12 | XGBClassifier 3               | 0.882353 | 0.0588235 | 0.411765 | 0.647059 | 0.588235 | 0.517647 |\n",
      "| 13 | XGBClassifier 4               | 0.764706 | 0.0588235 | 0.352941 | 0.882353 | 0.705882 | 0.552941 |\n",
      "| 14 | XGBClassifier 5               | 0.882353 | 0.0588235 | 0.411765 | 0.470588 | 0.529412 | 0.470588 |\n",
      "| 15 | XGBClassifier 6               | 0.882353 | 0.0588235 | 0.411765 | 0.647059 | 0.705882 | 0.541176 |\n",
      "| 16 | XGBRFClassifier 1             | 0.529412 | 0.0588235 | 0.470588 | 0.882353 | 0.647059 | 0.517647 |\n",
      "| 17 | XGBRFClassifier 2             | 0.529412 | 0.0588235 | 0.470588 | 0.823529 | 0.647059 | 0.505882 |\n",
      "| 18 | XGBRFClassifier 3             | 0.529412 | 0.0588235 | 0.294118 | 0.529412 | 0.470588 | 0.376471 |\n",
      "| 19 | XGBRFClassifier 4             | 0.529412 | 0.0588235 | 0.352941 | 0.882353 | 0.470588 | 0.458824 |\n",
      "| 20 | XGBRFClassifier 5             | 0.529412 | 0.0588235 | 0.411765 | 0.882353 | 0.470588 | 0.470588 |\n",
      "| 21 | XGBRFClassifier 6             | 0.529412 | 0.0588235 | 0.470588 | 0.882353 | 0.647059 | 0.517647 |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.941176 | 0.0588235 | 0.470588 | 0.941176 | 0.470588 | 0.576471 |\n",
      "| 23 | GradientBoostingClassifier 2S | 1        | 0.0588235 | 0.470588 | 0.823529 | 0.470588 | 0.564706 |\n",
      "| 24 | GradientBoostingClassifier 3S | 1        | 0.0588235 | 0.529412 | 0.705882 | 0.470588 | 0.552941 |\n",
      "+----+-------------------------------+----------+-----------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers2 = [\"Classifier type\", ] + step_headers\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "accuracy_df = pd.DataFrame(score_df['Accuracy'].tolist(), index=score_df.index, columns=step_headers)\n",
    "score_df = score_df.drop('Accuracy', 1)\n",
    "f_out = pd.merge(score_df, accuracy_df, how='left', left_index=True, right_index=True)\n",
    "f_out['mean'] = f_out.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out, headers2, tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "f_out.to_csv(filename_to_export, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'DecisionTreeClassifier 1': [0.8994117647058824,\n              0.9056493884682586,\n              0.8800461361014994,\n              0.8520845231296402,\n              0.8054298642533937],\n             'DecisionTreeClassifier 2': [0.9994117647058823,\n              0.9988351776354106,\n              1.0,\n              0.9902912621359223,\n              0.9932126696832579],\n             'DecisionTreeClassifier 3': [0.7964705882352942,\n              0.8683750728013978,\n              0.8391003460207612,\n              0.8280982295830954,\n              0.8438914027149321],\n             'DecisionTreeClassifier 4': [0.9358823529411765,\n              0.8648806057076296,\n              0.8662053056516724,\n              0.8920616790405482,\n              0.8636877828054299],\n             'DecisionTreeClassifier 5': [0.9970588235294118,\n              0.9976703552708212,\n              0.9844290657439446,\n              0.9937178754997145,\n              0.9751131221719457],\n             'RandomForestClassifier 4': [0.5994117647058823,\n              0.6179382644146768,\n              0.6274509803921569,\n              0.6259280411193604,\n              0.6385746606334841],\n             'RandomForestClassifier 5': [0.5305882352941177,\n              0.5381479324403029,\n              0.5230680507497116,\n              0.5316961736150772,\n              0.5254524886877828],\n             'GradientBoostingClassifier 1': [0.9794117647058823,\n              0.9737914967967385,\n              0.9694348327566321,\n              0.9674471730439749,\n              0.9717194570135747],\n             'GradientBoostingClassifier 2': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'GradientBoostingClassifier 3': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 1': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 2': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 3': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 4': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 5': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 6': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBRFClassifier 1': [0.9952941176470588,\n              0.9947582993593477,\n              0.9953863898500577,\n              0.9925756710451171,\n              0.994343891402715],\n             'XGBRFClassifier 2': [0.9976470588235294,\n              0.9970879440885265,\n              0.9976931949250288,\n              0.9971444888635066,\n              0.998868778280543],\n             'XGBRFClassifier 3': [0.6911764705882353,\n              0.7041351193942924,\n              0.723760092272203,\n              0.7058823529411765,\n              0.708710407239819],\n             'XGBRFClassifier 4': [0.9023529411764706,\n              0.9062317996505533,\n              0.9042675893886967,\n              0.903483723586522,\n              0.8993212669683258],\n             'XGBRFClassifier 5': [0.99,\n              0.988351776354106,\n              0.9867358708189158,\n              0.9868646487721302,\n              0.9898190045248869],\n             'XGBRFClassifier 6': [0.9894117647058823,\n              0.9877693651718114,\n              0.986159169550173,\n              0.9851513420902341,\n              0.9909502262443439],\n             'GradientBoostingClassifier 1S': [0.9405882352941176,\n              0.9435061153174141,\n              0.9377162629757786,\n              0.9474585950885208,\n              0.9445701357466063],\n             'GradientBoostingClassifier 2S': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'GradientBoostingClassifier 3S': [1.0, 1.0, 1.0, 1.0, 1.0]})"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n",
      "|    | Classifier type               |      <0> |     <90> |     <90> |     <90> |     <90> |     mean |\n",
      "|----+-------------------------------+----------+----------+----------+----------+----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.899412 | 0.905649 | 0.880046 | 0.852085 | 0.80543  | 0.868524 |\n",
      "|  1 | DecisionTreeClassifier 2      | 0.999412 | 0.998835 | 1        | 0.990291 | 0.993213 | 0.99635  |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.796471 | 0.868375 | 0.8391   | 0.828098 | 0.843891 | 0.835187 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.935882 | 0.864881 | 0.866205 | 0.892062 | 0.863688 | 0.884544 |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.997059 | 0.99767  | 0.984429 | 0.993718 | 0.975113 | 0.989598 |\n",
      "|  5 | RandomForestClassifier 4      | 0.599412 | 0.617938 | 0.627451 | 0.625928 | 0.638575 | 0.621861 |\n",
      "|  6 | RandomForestClassifier 5      | 0.530588 | 0.538148 | 0.523068 | 0.531696 | 0.525452 | 0.529791 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.979412 | 0.973791 | 0.969435 | 0.967447 | 0.971719 | 0.972361 |\n",
      "|  8 | GradientBoostingClassifier 2  | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "|  9 | GradientBoostingClassifier 3  | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 10 | XGBClassifier 1               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 11 | XGBClassifier 2               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 12 | XGBClassifier 3               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 13 | XGBClassifier 4               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 14 | XGBClassifier 5               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 15 | XGBClassifier 6               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 16 | XGBRFClassifier 1             | 0.995294 | 0.994758 | 0.995386 | 0.992576 | 0.994344 | 0.994472 |\n",
      "| 17 | XGBRFClassifier 2             | 0.997647 | 0.997088 | 0.997693 | 0.997144 | 0.998869 | 0.997688 |\n",
      "| 18 | XGBRFClassifier 3             | 0.691176 | 0.704135 | 0.72376  | 0.705882 | 0.70871  | 0.706733 |\n",
      "| 19 | XGBRFClassifier 4             | 0.902353 | 0.906232 | 0.904268 | 0.903484 | 0.899321 | 0.903131 |\n",
      "| 20 | XGBRFClassifier 5             | 0.99     | 0.988352 | 0.986736 | 0.986865 | 0.989819 | 0.988354 |\n",
      "| 21 | XGBRFClassifier 6             | 0.989412 | 0.987769 | 0.986159 | 0.985151 | 0.99095  | 0.987888 |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.940588 | 0.943506 | 0.937716 | 0.947459 | 0.94457  | 0.942768 |\n",
      "| 23 | GradientBoostingClassifier 2S | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 24 | GradientBoostingClassifier 3S | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "headers2 = [\"Classifier type\", ] + step_headers\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "accuracy_df_train = pd.DataFrame(score_df_train['Accuracy'].tolist(), index=score_df_train.index, columns=step_headers)\n",
    "score_df_train = score_df_train.drop('Accuracy', 1)\n",
    "f_out_train = pd.merge(score_df_train, accuracy_df_train, how='left', left_index=True, right_index=True)\n",
    "f_out_train['mean'] = f_out_train.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out_train, headers2, tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "f_out_train.to_csv(filename_to_export_train, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "                              0_x                       0_y  \\\n0        DecisionTreeClassifier 1   (3, 2, 0, 14, 3, 0, 17)   \n1        DecisionTreeClassifier 2   (0, 2, 0, 14, 3, 0, 17)   \n2        DecisionTreeClassifier 3   (4, 3, 0, 14, 3, 0, 17)   \n3        DecisionTreeClassifier 4   (0, 3, 0, 14, 3, 0, 17)   \n4        DecisionTreeClassifier 5   (0, 3, 0, 14, 3, 0, 17)   \n5        RandomForestClassifier 4   (0, 3, 0, 14, 3, 0, 17)   \n6        RandomForestClassifier 5   (0, 3, 0, 14, 3, 0, 17)   \n7    GradientBoostingClassifier 1  (12, 3, 0, 14, 3, 0, 17)   \n8    GradientBoostingClassifier 2  (10, 3, 0, 14, 3, 0, 17)   \n9    GradientBoostingClassifier 3  (11, 3, 0, 14, 3, 0, 17)   \n10                XGBClassifier 1  (13, 3, 0, 14, 3, 0, 17)   \n11                XGBClassifier 2  (13, 3, 0, 14, 3, 0, 17)   \n12                XGBClassifier 3  (12, 3, 0, 14, 3, 0, 17)   \n13                XGBClassifier 4  (10, 3, 0, 14, 3, 0, 17)   \n14                XGBClassifier 5  (12, 3, 0, 14, 3, 0, 17)   \n15                XGBClassifier 6  (12, 3, 0, 14, 3, 0, 17)   \n16              XGBRFClassifier 1   (6, 3, 0, 14, 3, 0, 17)   \n17              XGBRFClassifier 2   (6, 3, 0, 14, 3, 0, 17)   \n18              XGBRFClassifier 3   (6, 3, 0, 14, 3, 0, 17)   \n19              XGBRFClassifier 4   (6, 3, 0, 14, 3, 0, 17)   \n20              XGBRFClassifier 5   (6, 3, 0, 14, 3, 0, 17)   \n21              XGBRFClassifier 6   (6, 3, 0, 14, 3, 0, 17)   \n22  GradientBoostingClassifier 1S  (13, 3, 0, 14, 3, 0, 17)   \n23  GradientBoostingClassifier 2S  (14, 3, 0, 14, 3, 0, 17)   \n24  GradientBoostingClassifier 3S  (14, 3, 0, 14, 3, 0, 17)   \n\n                          1                       2                         3  \\\n0   (0, 0, 1, 1, 1, 15, 17)  (0, 9, 2, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)   \n1   (0, 0, 1, 1, 1, 15, 17)  (0, 9, 1, 3, 9, 5, 17)   (2, 0, 0, 15, 0, 2, 17)   \n2   (1, 1, 0, 1, 1, 15, 17)  (1, 2, 4, 3, 9, 5, 17)   (6, 0, 0, 15, 0, 2, 17)   \n3   (1, 0, 0, 1, 1, 15, 17)  (0, 0, 5, 3, 9, 5, 17)   (7, 0, 1, 15, 0, 2, 17)   \n4   (1, 0, 0, 1, 1, 15, 17)  (0, 0, 5, 3, 9, 5, 17)   (7, 0, 2, 15, 0, 2, 17)   \n5   (1, 0, 0, 1, 1, 15, 17)  (0, 9, 1, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)   \n6   (1, 0, 0, 1, 1, 15, 17)  (0, 5, 2, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)   \n7   (1, 0, 0, 1, 1, 15, 17)  (3, 2, 4, 3, 9, 5, 17)  (15, 0, 1, 15, 0, 2, 17)   \n8   (1, 0, 0, 1, 1, 15, 17)  (1, 3, 4, 3, 9, 5, 17)   (5, 0, 0, 15, 0, 2, 17)   \n9   (1, 0, 0, 1, 1, 15, 17)  (3, 3, 3, 3, 9, 5, 17)  (10, 0, 0, 15, 0, 2, 17)   \n10  (1, 0, 0, 1, 1, 15, 17)  (3, 2, 1, 3, 9, 5, 17)  (13, 0, 0, 15, 0, 2, 17)   \n11  (1, 0, 0, 1, 1, 15, 17)  (3, 3, 1, 3, 9, 5, 17)  (11, 0, 0, 15, 0, 2, 17)   \n12  (1, 0, 0, 1, 1, 15, 17)  (2, 4, 1, 3, 9, 5, 17)  (11, 0, 0, 15, 0, 2, 17)   \n13  (1, 0, 0, 1, 1, 15, 17)  (1, 4, 1, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)   \n14  (1, 0, 0, 1, 1, 15, 17)  (2, 3, 2, 3, 9, 5, 17)   (8, 0, 0, 15, 0, 2, 17)   \n15  (1, 0, 0, 1, 1, 15, 17)  (3, 2, 2, 3, 9, 5, 17)  (10, 0, 1, 15, 0, 2, 17)   \n16  (1, 0, 0, 1, 1, 15, 17)  (1, 4, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)   \n17  (1, 0, 0, 1, 1, 15, 17)  (1, 4, 3, 3, 9, 5, 17)  (14, 0, 0, 15, 0, 2, 17)   \n18  (1, 0, 0, 1, 1, 15, 17)  (0, 0, 5, 3, 9, 5, 17)   (7, 0, 2, 15, 0, 2, 17)   \n19  (1, 0, 0, 1, 1, 15, 17)  (1, 2, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)   \n20  (1, 0, 0, 1, 1, 15, 17)  (1, 3, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)   \n21  (1, 0, 0, 1, 1, 15, 17)  (1, 4, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)   \n22  (1, 0, 0, 1, 1, 15, 17)  (3, 0, 5, 3, 9, 5, 17)  (15, 0, 1, 15, 0, 2, 17)   \n23  (1, 0, 0, 1, 1, 15, 17)  (1, 2, 5, 3, 9, 5, 17)  (14, 0, 0, 15, 0, 2, 17)   \n24  (1, 0, 0, 1, 1, 15, 17)  (3, 1, 5, 3, 9, 5, 17)  (11, 0, 1, 15, 0, 2, 17)   \n\n                         4  \n0   (1, 2, 2, 8, 7, 2, 17)  \n1   (1, 7, 0, 8, 7, 2, 17)  \n2   (1, 7, 0, 8, 7, 2, 17)  \n3   (8, 0, 0, 8, 7, 2, 17)  \n4   (8, 0, 0, 8, 7, 2, 17)  \n5   (1, 7, 0, 8, 7, 2, 17)  \n6   (4, 7, 0, 8, 7, 2, 17)  \n7   (2, 7, 0, 8, 7, 2, 17)  \n8   (8, 0, 0, 8, 7, 2, 17)  \n9   (7, 7, 0, 8, 7, 2, 17)  \n10  (8, 1, 0, 8, 7, 2, 17)  \n11  (5, 5, 0, 8, 7, 2, 17)  \n12  (8, 2, 0, 8, 7, 2, 17)  \n13  (8, 4, 0, 8, 7, 2, 17)  \n14  (8, 1, 0, 8, 7, 2, 17)  \n15  (6, 6, 0, 8, 7, 2, 17)  \n16  (4, 7, 0, 8, 7, 2, 17)  \n17  (4, 7, 0, 8, 7, 2, 17)  \n18  (8, 0, 0, 8, 7, 2, 17)  \n19  (8, 0, 0, 8, 7, 2, 17)  \n20  (8, 0, 0, 8, 7, 2, 17)  \n21  (4, 7, 0, 8, 7, 2, 17)  \n22  (8, 0, 0, 8, 7, 2, 17)  \n23  (8, 0, 0, 8, 7, 2, 17)  \n24  (8, 0, 0, 8, 7, 2, 17)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0_x</th>\n      <th>0_y</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 1</td>\n      <td>(3, 2, 0, 14, 3, 0, 17)</td>\n      <td>(0, 0, 1, 1, 1, 15, 17)</td>\n      <td>(0, 9, 2, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 2, 2, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeClassifier 2</td>\n      <td>(0, 2, 0, 14, 3, 0, 17)</td>\n      <td>(0, 0, 1, 1, 1, 15, 17)</td>\n      <td>(0, 9, 1, 3, 9, 5, 17)</td>\n      <td>(2, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DecisionTreeClassifier 3</td>\n      <td>(4, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 1, 0, 1, 1, 15, 17)</td>\n      <td>(1, 2, 4, 3, 9, 5, 17)</td>\n      <td>(6, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier 4</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DecisionTreeClassifier 5</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 2, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RandomForestClassifier 4</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(0, 9, 1, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RandomForestClassifier 5</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(0, 5, 2, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GradientBoostingClassifier 1</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 2, 4, 3, 9, 5, 17)</td>\n      <td>(15, 0, 1, 15, 0, 2, 17)</td>\n      <td>(2, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GradientBoostingClassifier 2</td>\n      <td>(10, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 3, 4, 3, 9, 5, 17)</td>\n      <td>(5, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GradientBoostingClassifier 3</td>\n      <td>(11, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 3, 3, 3, 9, 5, 17)</td>\n      <td>(10, 0, 0, 15, 0, 2, 17)</td>\n      <td>(7, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBClassifier 1</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 2, 1, 3, 9, 5, 17)</td>\n      <td>(13, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 1, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGBClassifier 2</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 3, 1, 3, 9, 5, 17)</td>\n      <td>(11, 0, 0, 15, 0, 2, 17)</td>\n      <td>(5, 5, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGBClassifier 3</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(2, 4, 1, 3, 9, 5, 17)</td>\n      <td>(11, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 2, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGBClassifier 4</td>\n      <td>(10, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 4, 1, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 4, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGBClassifier 5</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(2, 3, 2, 3, 9, 5, 17)</td>\n      <td>(8, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 1, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGBClassifier 6</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 2, 2, 3, 9, 5, 17)</td>\n      <td>(10, 0, 1, 15, 0, 2, 17)</td>\n      <td>(6, 6, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>XGBRFClassifier 1</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>XGBRFClassifier 2</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(14, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>XGBRFClassifier 3</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 2, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>XGBRFClassifier 4</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 2, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>XGBRFClassifier 5</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 3, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>XGBRFClassifier 6</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>GradientBoostingClassifier 1S</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 0, 5, 3, 9, 5, 17)</td>\n      <td>(15, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>GradientBoostingClassifier 2S</td>\n      <td>(14, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(1, 2, 5, 3, 9, 5, 17)</td>\n      <td>(14, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>GradientBoostingClassifier 3S</td>\n      <td>(14, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 0, 0, 1, 1, 15, 17)</td>\n      <td>(3, 1, 5, 3, 9, 5, 17)</td>\n      <td>(11, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_points_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result__points_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "points\n",
    "df_points = pd.DataFrame(points.items())\n",
    "df_temp = pd.DataFrame(df_points[1].tolist(), index=score_df_train.index)\n",
    "df_points = df_points.drop(df_points.columns[1], axis=1)\n",
    "df_points = pd.merge(df_points, df_temp, how='left', left_index=True, right_index=True)\n",
    "df_points.to_csv(filename_to_export)\n",
    "df_points\n",
    "\n",
    "# df_points_train = pd(points_train.items())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "                              0_x                       0_y  \\\n0        DecisionTreeClassifier 1   (3, 2, 0, 14, 3, 0, 17)   \n1        DecisionTreeClassifier 2   (0, 2, 0, 14, 3, 0, 17)   \n2        DecisionTreeClassifier 3   (4, 3, 0, 14, 3, 0, 17)   \n3        DecisionTreeClassifier 4   (0, 3, 0, 14, 3, 0, 17)   \n4        DecisionTreeClassifier 5   (0, 3, 0, 14, 3, 0, 17)   \n5        RandomForestClassifier 4   (0, 3, 0, 14, 3, 0, 17)   \n6        RandomForestClassifier 5   (0, 3, 0, 14, 3, 0, 17)   \n7    GradientBoostingClassifier 1  (12, 3, 0, 14, 3, 0, 17)   \n8    GradientBoostingClassifier 2  (10, 3, 0, 14, 3, 0, 17)   \n9    GradientBoostingClassifier 3  (11, 3, 0, 14, 3, 0, 17)   \n10                XGBClassifier 1  (13, 3, 0, 14, 3, 0, 17)   \n11                XGBClassifier 2  (13, 3, 0, 14, 3, 0, 17)   \n12                XGBClassifier 3  (12, 3, 0, 14, 3, 0, 17)   \n13                XGBClassifier 4  (10, 3, 0, 14, 3, 0, 17)   \n14                XGBClassifier 5  (12, 3, 0, 14, 3, 0, 17)   \n15                XGBClassifier 6  (12, 3, 0, 14, 3, 0, 17)   \n16              XGBRFClassifier 1   (6, 3, 0, 14, 3, 0, 17)   \n17              XGBRFClassifier 2   (6, 3, 0, 14, 3, 0, 17)   \n18              XGBRFClassifier 3   (6, 3, 0, 14, 3, 0, 17)   \n19              XGBRFClassifier 4   (6, 3, 0, 14, 3, 0, 17)   \n20              XGBRFClassifier 5   (6, 3, 0, 14, 3, 0, 17)   \n21              XGBRFClassifier 6   (6, 3, 0, 14, 3, 0, 17)   \n22  GradientBoostingClassifier 1S  (13, 3, 0, 14, 3, 0, 17)   \n23  GradientBoostingClassifier 2S  (14, 3, 0, 14, 3, 0, 17)   \n24  GradientBoostingClassifier 3S  (14, 3, 0, 14, 3, 0, 17)   \n\n                       2_x                       3_x                     4_x  \\\n0   (0, 9, 2, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)  (1, 2, 2, 8, 7, 2, 17)   \n1   (0, 9, 1, 3, 9, 5, 17)   (2, 0, 0, 15, 0, 2, 17)  (1, 7, 0, 8, 7, 2, 17)   \n2   (1, 2, 4, 3, 9, 5, 17)   (6, 0, 0, 15, 0, 2, 17)  (1, 7, 0, 8, 7, 2, 17)   \n3   (0, 0, 5, 3, 9, 5, 17)   (7, 0, 1, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n4   (0, 0, 5, 3, 9, 5, 17)   (7, 0, 2, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n5   (0, 9, 1, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)  (1, 7, 0, 8, 7, 2, 17)   \n6   (0, 5, 2, 3, 9, 5, 17)   (0, 0, 0, 15, 0, 2, 17)  (4, 7, 0, 8, 7, 2, 17)   \n7   (3, 2, 4, 3, 9, 5, 17)  (15, 0, 1, 15, 0, 2, 17)  (2, 7, 0, 8, 7, 2, 17)   \n8   (1, 3, 4, 3, 9, 5, 17)   (5, 0, 0, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n9   (3, 3, 3, 3, 9, 5, 17)  (10, 0, 0, 15, 0, 2, 17)  (7, 7, 0, 8, 7, 2, 17)   \n10  (3, 2, 1, 3, 9, 5, 17)  (13, 0, 0, 15, 0, 2, 17)  (8, 1, 0, 8, 7, 2, 17)   \n11  (3, 3, 1, 3, 9, 5, 17)  (11, 0, 0, 15, 0, 2, 17)  (5, 5, 0, 8, 7, 2, 17)   \n12  (2, 4, 1, 3, 9, 5, 17)  (11, 0, 0, 15, 0, 2, 17)  (8, 2, 0, 8, 7, 2, 17)   \n13  (1, 4, 1, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)  (8, 4, 0, 8, 7, 2, 17)   \n14  (2, 3, 2, 3, 9, 5, 17)   (8, 0, 0, 15, 0, 2, 17)  (8, 1, 0, 8, 7, 2, 17)   \n15  (3, 2, 2, 3, 9, 5, 17)  (10, 0, 1, 15, 0, 2, 17)  (6, 6, 0, 8, 7, 2, 17)   \n16  (1, 4, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)  (4, 7, 0, 8, 7, 2, 17)   \n17  (1, 4, 3, 3, 9, 5, 17)  (14, 0, 0, 15, 0, 2, 17)  (4, 7, 0, 8, 7, 2, 17)   \n18  (0, 0, 5, 3, 9, 5, 17)   (7, 0, 2, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n19  (1, 2, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n20  (1, 3, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n21  (1, 4, 3, 3, 9, 5, 17)  (15, 0, 0, 15, 0, 2, 17)  (4, 7, 0, 8, 7, 2, 17)   \n22  (3, 0, 5, 3, 9, 5, 17)  (15, 0, 1, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n23  (1, 2, 5, 3, 9, 5, 17)  (14, 0, 0, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n24  (3, 1, 5, 3, 9, 5, 17)  (11, 0, 1, 15, 0, 2, 17)  (8, 0, 0, 8, 7, 2, 17)   \n\n                                       0  \\\n0   (503, 489, 537, 568, 559, 573, 1700)   \n1   (567, 559, 573, 568, 559, 573, 1700)   \n2   (436, 421, 497, 568, 559, 573, 1700)   \n3   (534, 535, 522, 568, 559, 573, 1700)   \n4   (566, 558, 571, 568, 559, 573, 1700)   \n5   (351, 290, 378, 568, 559, 573, 1700)   \n6   (340, 209, 353, 568, 559, 573, 1700)   \n7   (549, 553, 563, 568, 559, 573, 1700)   \n8   (568, 559, 573, 568, 559, 573, 1700)   \n9   (568, 559, 573, 568, 559, 573, 1700)   \n10  (568, 559, 573, 568, 559, 573, 1700)   \n11  (568, 559, 573, 568, 559, 573, 1700)   \n12  (568, 559, 573, 568, 559, 573, 1700)   \n13  (568, 559, 573, 568, 559, 573, 1700)   \n14  (568, 559, 573, 568, 559, 573, 1700)   \n15  (568, 559, 573, 568, 559, 573, 1700)   \n16  (565, 557, 570, 568, 559, 573, 1700)   \n17  (567, 558, 571, 568, 559, 573, 1700)   \n18  (399, 370, 406, 568, 559, 573, 1700)   \n19  (500, 507, 527, 568, 559, 573, 1700)   \n20  (559, 555, 569, 568, 559, 573, 1700)   \n21  (559, 555, 568, 568, 559, 573, 1700)   \n22  (532, 533, 534, 568, 559, 573, 1700)   \n23  (568, 559, 573, 568, 559, 573, 1700)   \n24  (568, 559, 573, 568, 559, 573, 1700)   \n\n                                       1  \\\n0   (524, 540, 491, 587, 569, 561, 1717)   \n1   (587, 568, 560, 587, 569, 561, 1717)   \n2   (539, 497, 455, 587, 569, 561, 1717)   \n3   (501, 526, 458, 587, 569, 561, 1717)   \n4   (587, 568, 558, 587, 569, 561, 1717)   \n5   (385, 333, 343, 587, 569, 561, 1717)   \n6   (385, 209, 330, 587, 569, 561, 1717)   \n7   (569, 560, 543, 587, 569, 561, 1717)   \n8   (587, 569, 561, 587, 569, 561, 1717)   \n9   (587, 569, 561, 587, 569, 561, 1717)   \n10  (587, 569, 561, 587, 569, 561, 1717)   \n11  (587, 569, 561, 587, 569, 561, 1717)   \n12  (587, 569, 561, 587, 569, 561, 1717)   \n13  (587, 569, 561, 587, 569, 561, 1717)   \n14  (587, 569, 561, 587, 569, 561, 1717)   \n15  (587, 569, 561, 587, 569, 561, 1717)   \n16  (584, 568, 556, 587, 569, 561, 1717)   \n17  (585, 569, 558, 587, 569, 561, 1717)   \n18  (427, 412, 370, 587, 569, 561, 1717)   \n19  (526, 521, 509, 587, 569, 561, 1717)   \n20  (578, 564, 555, 587, 569, 561, 1717)   \n21  (578, 564, 554, 587, 569, 561, 1717)   \n22  (551, 546, 523, 587, 569, 561, 1717)   \n23  (587, 569, 561, 587, 569, 561, 1717)   \n24  (587, 569, 561, 587, 569, 561, 1717)   \n\n                                     2_y  \\\n0   (509, 483, 534, 588, 570, 576, 1734)   \n1   (588, 570, 576, 588, 570, 576, 1734)   \n2   (464, 498, 493, 588, 570, 576, 1734)   \n3   (489, 542, 471, 588, 570, 576, 1734)   \n4   (577, 561, 569, 588, 570, 576, 1734)   \n5   (387, 330, 371, 588, 570, 576, 1734)   \n6   (368, 201, 338, 588, 570, 576, 1734)   \n7   (567, 558, 556, 588, 570, 576, 1734)   \n8   (588, 570, 576, 588, 570, 576, 1734)   \n9   (588, 570, 576, 588, 570, 576, 1734)   \n10  (588, 570, 576, 588, 570, 576, 1734)   \n11  (588, 570, 576, 588, 570, 576, 1734)   \n12  (588, 570, 576, 588, 570, 576, 1734)   \n13  (588, 570, 576, 588, 570, 576, 1734)   \n14  (588, 570, 576, 588, 570, 576, 1734)   \n15  (588, 570, 576, 588, 570, 576, 1734)   \n16  (583, 568, 575, 588, 570, 576, 1734)   \n17  (586, 569, 575, 588, 570, 576, 1734)   \n18  (444, 428, 383, 588, 570, 576, 1734)   \n19  (525, 526, 517, 588, 570, 576, 1734)   \n20  (575, 565, 571, 588, 570, 576, 1734)   \n21  (573, 566, 571, 588, 570, 576, 1734)   \n22  (554, 540, 532, 588, 570, 576, 1734)   \n23  (588, 570, 576, 588, 570, 576, 1734)   \n24  (588, 570, 576, 588, 570, 576, 1734)   \n\n                                     3_y                                   4_y  \n0   (466, 501, 525, 586, 585, 580, 1751)  (455, 400, 569, 594, 585, 589, 1768)  \n1   (576, 580, 578, 586, 585, 580, 1751)  (590, 581, 585, 594, 585, 589, 1768)  \n2   (498, 464, 488, 586, 585, 580, 1751)  (492, 496, 504, 594, 585, 589, 1768)  \n3   (561, 558, 443, 586, 585, 580, 1751)  (497, 487, 543, 594, 585, 589, 1768)  \n4   (584, 583, 573, 586, 585, 580, 1751)  (564, 575, 585, 594, 585, 589, 1768)  \n5   (375, 356, 365, 586, 585, 580, 1751)  (393, 353, 383, 594, 585, 589, 1768)  \n6   (328, 276, 327, 586, 585, 580, 1751)  (347, 235, 347, 594, 585, 589, 1768)  \n7   (563, 574, 557, 586, 585, 580, 1751)  (574, 576, 568, 594, 585, 589, 1768)  \n8   (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n9   (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n10  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n11  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n12  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n13  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n14  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n15  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n16  (577, 583, 578, 586, 585, 580, 1751)  (588, 583, 587, 594, 585, 589, 1768)  \n17  (583, 585, 578, 586, 585, 580, 1751)  (594, 585, 587, 594, 585, 589, 1768)  \n18  (416, 429, 391, 586, 585, 580, 1751)  (425, 434, 394, 594, 585, 589, 1768)  \n19  (521, 539, 522, 586, 585, 580, 1751)  (525, 534, 531, 594, 585, 589, 1768)  \n20  (572, 581, 575, 586, 585, 580, 1751)  (584, 581, 585, 594, 585, 589, 1768)  \n21  (572, 580, 573, 586, 585, 580, 1751)  (585, 581, 586, 594, 585, 589, 1768)  \n22  (547, 564, 548, 586, 585, 580, 1751)  (559, 561, 550, 594, 585, 589, 1768)  \n23  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  \n24  (586, 585, 580, 586, 585, 580, 1751)  (594, 585, 589, 594, 585, 589, 1768)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0_x</th>\n      <th>0_y</th>\n      <th>2_x</th>\n      <th>3_x</th>\n      <th>4_x</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2_y</th>\n      <th>3_y</th>\n      <th>4_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 1</td>\n      <td>(3, 2, 0, 14, 3, 0, 17)</td>\n      <td>(0, 9, 2, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 2, 2, 8, 7, 2, 17)</td>\n      <td>(503, 489, 537, 568, 559, 573, 1700)</td>\n      <td>(524, 540, 491, 587, 569, 561, 1717)</td>\n      <td>(509, 483, 534, 588, 570, 576, 1734)</td>\n      <td>(466, 501, 525, 586, 585, 580, 1751)</td>\n      <td>(455, 400, 569, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeClassifier 2</td>\n      <td>(0, 2, 0, 14, 3, 0, 17)</td>\n      <td>(0, 9, 1, 3, 9, 5, 17)</td>\n      <td>(2, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n      <td>(567, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 568, 560, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(576, 580, 578, 586, 585, 580, 1751)</td>\n      <td>(590, 581, 585, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DecisionTreeClassifier 3</td>\n      <td>(4, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 2, 4, 3, 9, 5, 17)</td>\n      <td>(6, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n      <td>(436, 421, 497, 568, 559, 573, 1700)</td>\n      <td>(539, 497, 455, 587, 569, 561, 1717)</td>\n      <td>(464, 498, 493, 588, 570, 576, 1734)</td>\n      <td>(498, 464, 488, 586, 585, 580, 1751)</td>\n      <td>(492, 496, 504, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier 4</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(534, 535, 522, 568, 559, 573, 1700)</td>\n      <td>(501, 526, 458, 587, 569, 561, 1717)</td>\n      <td>(489, 542, 471, 588, 570, 576, 1734)</td>\n      <td>(561, 558, 443, 586, 585, 580, 1751)</td>\n      <td>(497, 487, 543, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DecisionTreeClassifier 5</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 2, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(566, 558, 571, 568, 559, 573, 1700)</td>\n      <td>(587, 568, 558, 587, 569, 561, 1717)</td>\n      <td>(577, 561, 569, 588, 570, 576, 1734)</td>\n      <td>(584, 583, 573, 586, 585, 580, 1751)</td>\n      <td>(564, 575, 585, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RandomForestClassifier 4</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(0, 9, 1, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(1, 7, 0, 8, 7, 2, 17)</td>\n      <td>(351, 290, 378, 568, 559, 573, 1700)</td>\n      <td>(385, 333, 343, 587, 569, 561, 1717)</td>\n      <td>(387, 330, 371, 588, 570, 576, 1734)</td>\n      <td>(375, 356, 365, 586, 585, 580, 1751)</td>\n      <td>(393, 353, 383, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RandomForestClassifier 5</td>\n      <td>(0, 3, 0, 14, 3, 0, 17)</td>\n      <td>(0, 5, 2, 3, 9, 5, 17)</td>\n      <td>(0, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n      <td>(340, 209, 353, 568, 559, 573, 1700)</td>\n      <td>(385, 209, 330, 587, 569, 561, 1717)</td>\n      <td>(368, 201, 338, 588, 570, 576, 1734)</td>\n      <td>(328, 276, 327, 586, 585, 580, 1751)</td>\n      <td>(347, 235, 347, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GradientBoostingClassifier 1</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 2, 4, 3, 9, 5, 17)</td>\n      <td>(15, 0, 1, 15, 0, 2, 17)</td>\n      <td>(2, 7, 0, 8, 7, 2, 17)</td>\n      <td>(549, 553, 563, 568, 559, 573, 1700)</td>\n      <td>(569, 560, 543, 587, 569, 561, 1717)</td>\n      <td>(567, 558, 556, 588, 570, 576, 1734)</td>\n      <td>(563, 574, 557, 586, 585, 580, 1751)</td>\n      <td>(574, 576, 568, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GradientBoostingClassifier 2</td>\n      <td>(10, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 3, 4, 3, 9, 5, 17)</td>\n      <td>(5, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GradientBoostingClassifier 3</td>\n      <td>(11, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 3, 3, 3, 9, 5, 17)</td>\n      <td>(10, 0, 0, 15, 0, 2, 17)</td>\n      <td>(7, 7, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBClassifier 1</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 2, 1, 3, 9, 5, 17)</td>\n      <td>(13, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 1, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGBClassifier 2</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 3, 1, 3, 9, 5, 17)</td>\n      <td>(11, 0, 0, 15, 0, 2, 17)</td>\n      <td>(5, 5, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGBClassifier 3</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(2, 4, 1, 3, 9, 5, 17)</td>\n      <td>(11, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 2, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGBClassifier 4</td>\n      <td>(10, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 4, 1, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 4, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGBClassifier 5</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(2, 3, 2, 3, 9, 5, 17)</td>\n      <td>(8, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 1, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGBClassifier 6</td>\n      <td>(12, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 2, 2, 3, 9, 5, 17)</td>\n      <td>(10, 0, 1, 15, 0, 2, 17)</td>\n      <td>(6, 6, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>XGBRFClassifier 1</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n      <td>(565, 557, 570, 568, 559, 573, 1700)</td>\n      <td>(584, 568, 556, 587, 569, 561, 1717)</td>\n      <td>(583, 568, 575, 588, 570, 576, 1734)</td>\n      <td>(577, 583, 578, 586, 585, 580, 1751)</td>\n      <td>(588, 583, 587, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>XGBRFClassifier 2</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(14, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n      <td>(567, 558, 571, 568, 559, 573, 1700)</td>\n      <td>(585, 569, 558, 587, 569, 561, 1717)</td>\n      <td>(586, 569, 575, 588, 570, 576, 1734)</td>\n      <td>(583, 585, 578, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 587, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>XGBRFClassifier 3</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(0, 0, 5, 3, 9, 5, 17)</td>\n      <td>(7, 0, 2, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(399, 370, 406, 568, 559, 573, 1700)</td>\n      <td>(427, 412, 370, 587, 569, 561, 1717)</td>\n      <td>(444, 428, 383, 588, 570, 576, 1734)</td>\n      <td>(416, 429, 391, 586, 585, 580, 1751)</td>\n      <td>(425, 434, 394, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>XGBRFClassifier 4</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 2, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(500, 507, 527, 568, 559, 573, 1700)</td>\n      <td>(526, 521, 509, 587, 569, 561, 1717)</td>\n      <td>(525, 526, 517, 588, 570, 576, 1734)</td>\n      <td>(521, 539, 522, 586, 585, 580, 1751)</td>\n      <td>(525, 534, 531, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>XGBRFClassifier 5</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 3, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(559, 555, 569, 568, 559, 573, 1700)</td>\n      <td>(578, 564, 555, 587, 569, 561, 1717)</td>\n      <td>(575, 565, 571, 588, 570, 576, 1734)</td>\n      <td>(572, 581, 575, 586, 585, 580, 1751)</td>\n      <td>(584, 581, 585, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>XGBRFClassifier 6</td>\n      <td>(6, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 4, 3, 3, 9, 5, 17)</td>\n      <td>(15, 0, 0, 15, 0, 2, 17)</td>\n      <td>(4, 7, 0, 8, 7, 2, 17)</td>\n      <td>(559, 555, 568, 568, 559, 573, 1700)</td>\n      <td>(578, 564, 554, 587, 569, 561, 1717)</td>\n      <td>(573, 566, 571, 588, 570, 576, 1734)</td>\n      <td>(572, 580, 573, 586, 585, 580, 1751)</td>\n      <td>(585, 581, 586, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>GradientBoostingClassifier 1S</td>\n      <td>(13, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 0, 5, 3, 9, 5, 17)</td>\n      <td>(15, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(532, 533, 534, 568, 559, 573, 1700)</td>\n      <td>(551, 546, 523, 587, 569, 561, 1717)</td>\n      <td>(554, 540, 532, 588, 570, 576, 1734)</td>\n      <td>(547, 564, 548, 586, 585, 580, 1751)</td>\n      <td>(559, 561, 550, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>GradientBoostingClassifier 2S</td>\n      <td>(14, 3, 0, 14, 3, 0, 17)</td>\n      <td>(1, 2, 5, 3, 9, 5, 17)</td>\n      <td>(14, 0, 0, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>GradientBoostingClassifier 3S</td>\n      <td>(14, 3, 0, 14, 3, 0, 17)</td>\n      <td>(3, 1, 5, 3, 9, 5, 17)</td>\n      <td>(11, 0, 1, 15, 0, 2, 17)</td>\n      <td>(8, 0, 0, 8, 7, 2, 17)</td>\n      <td>(568, 559, 573, 568, 559, 573, 1700)</td>\n      <td>(587, 569, 561, 587, 569, 561, 1717)</td>\n      <td>(588, 570, 576, 588, 570, 576, 1734)</td>\n      <td>(586, 585, 580, 586, 585, 580, 1751)</td>\n      <td>(594, 585, 589, 594, 585, 589, 1768)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_points_train = pd.DataFrame(points_train.items())\n",
    "df_temp = pd.DataFrame(df_points_train[1].tolist(), index=score_df_train.index)\n",
    "df_points_train = df_points.drop(df_points_train.columns[1], axis=1)\n",
    "df_points_train = pd.merge(df_points_train, df_temp, how='left', left_index=True, right_index=True)\n",
    "df_points_train.to_csv(filename_to_export_train)\n",
    "df_points_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-33-717401bdd4be>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test_points.csv\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"wb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moutfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m    \u001B[0mwriter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcsv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m    \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriterow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpoints\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m    \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriterows\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mpoints\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"test_points.csv\", \"wr\") as outfile:\n",
    "   writer = csv.writer(outfile)\n",
    "   writer.writerow(points.keys())\n",
    "   writer.writerows(zip(*points.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"test_points_train.csv\", \"wr\")  as outfile:\n",
    "   writer = csv.writer(outfile)\n",
    "   writer.writerow(points_train.keys())\n",
    "   writer.writerows(zip(*points_train.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "magisterka_analiza",
   "language": "python",
   "display_name": "Python magisterka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}