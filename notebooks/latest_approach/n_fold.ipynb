{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from finta import TA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "from xgboost import XGBClassifier\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "WINDOW = 8 # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        # (optional, default is '1d')\n",
    "INTERVAL = '1y'     # use \"period\" instead of start/end\n",
    "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "        # (optional, default is '1mo')\n",
    "symbol = 'AAPL'      # Symbol of the desired stock\n",
    "\n",
    "# one day 16 rows of data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 Open        High         Low       Close  \\\nDatetime                                                                    \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.975000   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.977500   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.245000   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.112500   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.322500   \n...                               ...         ...         ...         ...   \n2021-05-12 14:30:00-04:00  122.510002  122.959999  122.260002  122.470001   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.820000   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.400000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.460000   \n2021-05-12 18:00:00-04:00  122.460000  122.770000  122.450000  122.540000   \n\n                            Adj Close    Volume  \nDatetime                                         \n2020-05-13 04:00:00-04:00   77.975000         0  \n2020-05-13 05:00:00-04:00   77.977500         0  \n2020-05-13 06:00:00-04:00   78.245000         0  \n2020-05-13 07:00:00-04:00   78.112500         0  \n2020-05-13 08:00:00-04:00   78.322500         0  \n...                               ...       ...  \n2021-05-12 14:30:00-04:00  122.470001  10802679  \n2021-05-12 15:30:00-04:00  122.820000  14469408  \n2021-05-12 16:00:00-04:00  122.400000  10328874  \n2021-05-12 17:00:00-04:00  122.460000         0  \n2021-05-12 18:00:00-04:00  122.540000         0  \n\n[4186 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.975000</td>\n      <td>77.975000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.977500</td>\n      <td>77.977500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.245000</td>\n      <td>78.245000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.112500</td>\n      <td>78.112500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.322500</td>\n      <td>78.322500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 14:30:00-04:00</th>\n      <td>122.510002</td>\n      <td>122.959999</td>\n      <td>122.260002</td>\n      <td>122.470001</td>\n      <td>122.470001</td>\n      <td>10802679</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.820000</td>\n      <td>122.820000</td>\n      <td>14469408</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.400000</td>\n      <td>122.400000</td>\n      <td>10328874</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.460000</td>\n      <td>122.460000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.770000</td>\n      <td>122.450000</td>\n      <td>122.540000</td>\n      <td>122.540000</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4186 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download(  # or pdr.get_data_yahoo(...\n",
    "        tickers = symbol,\n",
    "\n",
    "\n",
    "        period = INTERVAL,\n",
    "\n",
    "        interval = FETCH_INTERVAL,\n",
    "\n",
    "        # group by ticker (to access via data['SPY'])\n",
    "        # (optional, default is 'column')\n",
    "        group_by = 'ticker',\n",
    "\n",
    "        # adjust all OHLC automatically\n",
    "        # (optional, default is False)\n",
    "        # auto_adjust = True,\n",
    "\n",
    "        # download pre/post regular market hours data\n",
    "        # (optional, default is False)\n",
    "        prepost = True,\n",
    "\n",
    "        # use threads for mass downloading? (True/False/Integer)\n",
    "        # (optional, default is True)\n",
    "        threads = True,\n",
    "\n",
    "        # proxy URL scheme use use when downloading?\n",
    "        # (optional, default is None)\n",
    "        proxy = None\n",
    "    )\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n",
    "data.head(10)\n",
    "important_columns = ['open', 'high', 'low','close','volume']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_diffs(diff_number, col_name):\n",
    "    new_col_name = f'{col_name}_{diff_number}'\n",
    "    data[new_col_name] = data[col_name].diff(diff_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# for name in important_columns:\n",
    "#     for i in range(1, 11):\n",
    "#         calculate_diffs(i, name)\n",
    "#\n",
    "# data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Datetime'>"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['close'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "                              open     high      low    close  Adj Close  \\\nDatetime                                                                   \n2020-05-13 04:00:00-04:00  77.7525  77.9750  77.6875  77.9750    77.9750   \n2020-05-13 05:00:00-04:00  77.9750  78.2475  77.9000  77.9775    77.9775   \n2020-05-13 06:00:00-04:00  77.9975  78.2700  77.9975  78.2450    78.2450   \n2020-05-13 07:00:00-04:00  78.2050  78.2975  78.0175  78.1125    78.1125   \n2020-05-13 08:00:00-04:00  78.1175  78.5000  78.0050  78.3225    78.3225   \n\n                           volume  close_pct  \nDatetime                                      \n2020-05-13 04:00:00-04:00       0        NaN  \n2020-05-13 05:00:00-04:00       0   0.000032  \n2020-05-13 06:00:00-04:00       0   0.003430  \n2020-05-13 07:00:00-04:00       0  -0.001693  \n2020-05-13 08:00:00-04:00       0   0.002688  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.7525</td>\n      <td>77.9750</td>\n      <td>77.6875</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.9750</td>\n      <td>78.2475</td>\n      <td>77.9000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n      <td>0.000032</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.9975</td>\n      <td>78.2700</td>\n      <td>77.9975</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n      <td>0.003430</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.2050</td>\n      <td>78.2975</td>\n      <td>78.0175</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n      <td>-0.001693</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.1175</td>\n      <td>78.5000</td>\n      <td>78.0050</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n      <td>0.002688</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close_pct'] = data['close'].pct_change()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "              open         high          low        close    Adj Close  \\\ncount  4186.000000  4186.000000  4186.000000  4186.000000  4186.000000   \nmean    115.493901   116.038469   114.848343   115.491461   115.491461   \nstd      16.806775    17.647017    16.758043    16.794203    16.794203   \nmin      75.087502    75.537500    58.360000    75.300000    75.300000   \n25%     109.368125   109.759900   108.671249   109.333414   109.333414   \n50%     119.645451   120.100000   118.811249   119.634950   119.634950   \n75%     128.275025   128.703751   127.591800   128.269627   128.269627   \nmax     144.900000   438.440000   144.590000   144.910000   144.910000   \n\n             volume    close_pct  \ncount  4.186000e+03  4185.000000  \nmean   5.084074e+06     0.000123  \nstd    8.587058e+06     0.005410  \nmin    0.000000e+00    -0.051319  \n25%    0.000000e+00    -0.001888  \n50%    0.000000e+00     0.000089  \n75%    8.382402e+06     0.002265  \nmax    9.845401e+07     0.051457  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4186.000000</td>\n      <td>4186.000000</td>\n      <td>4186.000000</td>\n      <td>4186.000000</td>\n      <td>4186.000000</td>\n      <td>4.186000e+03</td>\n      <td>4185.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>115.493901</td>\n      <td>116.038469</td>\n      <td>114.848343</td>\n      <td>115.491461</td>\n      <td>115.491461</td>\n      <td>5.084074e+06</td>\n      <td>0.000123</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>16.806775</td>\n      <td>17.647017</td>\n      <td>16.758043</td>\n      <td>16.794203</td>\n      <td>16.794203</td>\n      <td>8.587058e+06</td>\n      <td>0.005410</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>75.087502</td>\n      <td>75.537500</td>\n      <td>58.360000</td>\n      <td>75.300000</td>\n      <td>75.300000</td>\n      <td>0.000000e+00</td>\n      <td>-0.051319</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>109.368125</td>\n      <td>109.759900</td>\n      <td>108.671249</td>\n      <td>109.333414</td>\n      <td>109.333414</td>\n      <td>0.000000e+00</td>\n      <td>-0.001888</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>119.645451</td>\n      <td>120.100000</td>\n      <td>118.811249</td>\n      <td>119.634950</td>\n      <td>119.634950</td>\n      <td>0.000000e+00</td>\n      <td>0.000089</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>128.275025</td>\n      <td>128.703751</td>\n      <td>127.591800</td>\n      <td>128.269627</td>\n      <td>128.269627</td>\n      <td>8.382402e+06</td>\n      <td>0.002265</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>144.900000</td>\n      <td>438.440000</td>\n      <td>144.590000</td>\n      <td>144.910000</td>\n      <td>144.910000</td>\n      <td>9.845401e+07</td>\n      <td>0.051457</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data = add_all_ta_features(\n",
    "        data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    # data['normVol'] = data['volume'] / data['volume'].ewm(5).mean()\n",
    "    # for i in range(1,50):\n",
    "    #     data[f'close{i}'] = data['close'].shift(i)\n",
    "    # Remove columns that won't be used as features\n",
    "    # del (data['Adj Close'])\n",
    "\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.975000   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.977500   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.245000   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.112500   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.322500   \n...                               ...         ...         ...         ...   \n2021-05-12 14:30:00-04:00  122.510002  122.959999  122.260002  122.470001   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.820000   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.400000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.460000   \n2021-05-12 18:00:00-04:00  122.460000  122.770000  122.450000  122.540000   \n\n                            Adj Close    volume  close_pct  \nDatetime                                                    \n2020-05-13 04:00:00-04:00   77.975000         0        NaN  \n2020-05-13 05:00:00-04:00   77.977500         0   0.000032  \n2020-05-13 06:00:00-04:00   78.245000         0   0.003430  \n2020-05-13 07:00:00-04:00   78.112500         0  -0.001693  \n2020-05-13 08:00:00-04:00   78.322500         0   0.002688  \n...                               ...       ...        ...  \n2021-05-12 14:30:00-04:00  122.470001  10802679  -0.000327  \n2021-05-12 15:30:00-04:00  122.820000  14469408   0.002858  \n2021-05-12 16:00:00-04:00  122.400000  10328874  -0.003420  \n2021-05-12 17:00:00-04:00  122.460000         0   0.000490  \n2021-05-12 18:00:00-04:00  122.540000         0   0.000653  \n\n[4186 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.975000</td>\n      <td>77.975000</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.977500</td>\n      <td>77.977500</td>\n      <td>0</td>\n      <td>0.000032</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.245000</td>\n      <td>78.245000</td>\n      <td>0</td>\n      <td>0.003430</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.112500</td>\n      <td>78.112500</td>\n      <td>0</td>\n      <td>-0.001693</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.322500</td>\n      <td>78.322500</td>\n      <td>0</td>\n      <td>0.002688</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 14:30:00-04:00</th>\n      <td>122.510002</td>\n      <td>122.959999</td>\n      <td>122.260002</td>\n      <td>122.470001</td>\n      <td>122.470001</td>\n      <td>10802679</td>\n      <td>-0.000327</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.820000</td>\n      <td>122.820000</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.400000</td>\n      <td>122.400000</td>\n      <td>10328874</td>\n      <td>-0.003420</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.460000</td>\n      <td>122.460000</td>\n      <td>0</td>\n      <td>0.000490</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.770000</td>\n      <td>122.450000</td>\n      <td>122.540000</td>\n      <td>122.540000</td>\n      <td>0</td>\n      <td>0.000653</td>\n    </tr>\n  </tbody>\n</table>\n<p>4186 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['open', 'high', 'low', 'close', 'Adj Close', 'volume', 'close_pct'], dtype='object')"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "def create_class_column(row, lowest_threshold, higher_threshold):\n",
    "    if row['close_shift'] - row['close'] > higher_threshold:\n",
    "        return 1\n",
    "    if row['close_shift'] - row['close'] < lowest_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.975000   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.977500   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.245000   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.112500   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.322500   \n...                               ...         ...         ...         ...   \n2021-05-12 14:30:00-04:00  122.510002  122.959999  122.260002  122.470001   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.820000   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.400000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.460000   \n2021-05-12 18:00:00-04:00  122.460000  122.770000  122.450000  122.540000   \n\n                            Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                   \n2020-05-13 04:00:00-04:00   77.975000         0        NaN  0.000000e+00   \n2020-05-13 05:00:00-04:00   77.977500         0   0.000032  0.000000e+00   \n2020-05-13 06:00:00-04:00   78.245000         0   0.003430  0.000000e+00   \n2020-05-13 07:00:00-04:00   78.112500         0  -0.001693  0.000000e+00   \n2020-05-13 08:00:00-04:00   78.322500         0   0.002688  0.000000e+00   \n...                               ...       ...        ...           ...   \n2021-05-12 14:30:00-04:00  122.470001  10802679  -0.000327  6.306123e+08   \n2021-05-12 15:30:00-04:00  122.820000  14469408   0.002858  6.359432e+08   \n2021-05-12 16:00:00-04:00  122.400000  10328874  -0.003420  6.271393e+08   \n2021-05-12 17:00:00-04:00  122.460000         0   0.000490  6.271393e+08   \n2021-05-12 18:00:00-04:00  122.540000         0   0.000653  6.271393e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_ao  \\\nDatetime                                           ...                \n2020-05-13 04:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 05:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 06:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 07:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 08:00:00-04:00           0    0.000000  ...     0.000000   \n...                               ...         ...  ...          ...   \n2021-05-12 14:30:00-04:00  -513565942   -0.125334  ...    -1.667697   \n2021-05-12 15:30:00-04:00  -499096534   -0.139414  ...    -1.656080   \n2021-05-12 16:00:00-04:00  -509425408   -0.278977  ...    -1.677024   \n2021-05-12 17:00:00-04:00  -509425408   -0.324334  ...    -2.339323   \n2021-05-12 18:00:00-04:00  -509425408   -0.387034  ...    -2.239411   \n\n                           momentum_kama  momentum_roc  momentum_ppo  \\\nDatetime                                                               \n2020-05-13 04:00:00-04:00      77.975000      0.000000      0.000000   \n2020-05-13 05:00:00-04:00      77.976139      0.000000      0.000000   \n2020-05-13 06:00:00-04:00      78.094432      0.000000      0.000000   \n2020-05-13 07:00:00-04:00      78.102394      0.000000      0.000000   \n2020-05-13 08:00:00-04:00      78.196385      0.000000      0.000000   \n...                                  ...           ...           ...   \n2021-05-12 14:30:00-04:00     122.932708     -2.585109     20.805708   \n2021-05-12 15:30:00-04:00     122.911653     -1.900959     23.803451   \n2021-05-12 16:00:00-04:00     122.764612     -2.166094     22.216890   \n2021-05-12 17:00:00-04:00     122.702171     -1.961412     11.687435   \n2021-05-12 18:00:00-04:00     122.674592     -2.116782      2.065133   \n\n                           momentum_ppo_signal  momentum_ppo_hist  others_dr  \\\nDatetime                                                                       \n2020-05-13 04:00:00-04:00             0.000000           0.000000 -32.484186   \n2020-05-13 05:00:00-04:00             0.000000           0.000000   0.003206   \n2020-05-13 06:00:00-04:00             0.000000           0.000000   0.343048   \n2020-05-13 07:00:00-04:00             0.000000           0.000000  -0.169340   \n2020-05-13 08:00:00-04:00             0.000000           0.000000   0.268843   \n...                                        ...                ...        ...   \n2021-05-12 14:30:00-04:00             5.964983          14.840725  -0.032651   \n2021-05-12 15:30:00-04:00             9.532677          14.270774   0.285783   \n2021-05-12 16:00:00-04:00            12.069519          10.147371  -0.341964   \n2021-05-12 17:00:00-04:00            11.993102          -0.305667   0.049020   \n2021-05-12 18:00:00-04:00            10.007509          -7.942376   0.065327   \n\n                           others_dlr  others_cr  close_shift  \nDatetime                                                       \n2020-05-13 04:00:00-04:00    0.000000   0.000000    76.611198  \n2020-05-13 05:00:00-04:00    0.003206   0.003206    76.374977  \n2020-05-13 06:00:00-04:00    0.342461   0.346265    76.417747  \n2020-05-13 07:00:00-04:00   -0.169483   0.176339    76.184998  \n2020-05-13 08:00:00-04:00    0.268482   0.445656    76.919998  \n...                               ...        ...          ...  \n2021-05-12 14:30:00-04:00   -0.032656  57.063163          NaN  \n2021-05-12 15:30:00-04:00    0.285375  57.512023          NaN  \n2021-05-12 16:00:00-04:00   -0.342550  56.973389          NaN  \n2021-05-12 17:00:00-04:00    0.049008  57.050337          NaN  \n2021-05-12 18:00:00-04:00    0.065306  57.152934          NaN  \n\n[4186 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_ao</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.975000</td>\n      <td>77.975000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>77.975000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-32.484186</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>76.611198</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.977500</td>\n      <td>77.977500</td>\n      <td>0</td>\n      <td>0.000032</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>77.976139</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>76.374977</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.245000</td>\n      <td>78.245000</td>\n      <td>0</td>\n      <td>0.003430</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.094432</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.343048</td>\n      <td>0.342461</td>\n      <td>0.346265</td>\n      <td>76.417747</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.112500</td>\n      <td>78.112500</td>\n      <td>0</td>\n      <td>-0.001693</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.102394</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.169340</td>\n      <td>-0.169483</td>\n      <td>0.176339</td>\n      <td>76.184998</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.322500</td>\n      <td>78.322500</td>\n      <td>0</td>\n      <td>0.002688</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.196385</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.268843</td>\n      <td>0.268482</td>\n      <td>0.445656</td>\n      <td>76.919998</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 14:30:00-04:00</th>\n      <td>122.510002</td>\n      <td>122.959999</td>\n      <td>122.260002</td>\n      <td>122.470001</td>\n      <td>122.470001</td>\n      <td>10802679</td>\n      <td>-0.000327</td>\n      <td>6.306123e+08</td>\n      <td>-513565942</td>\n      <td>-0.125334</td>\n      <td>...</td>\n      <td>-1.667697</td>\n      <td>122.932708</td>\n      <td>-2.585109</td>\n      <td>20.805708</td>\n      <td>5.964983</td>\n      <td>14.840725</td>\n      <td>-0.032651</td>\n      <td>-0.032656</td>\n      <td>57.063163</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.820000</td>\n      <td>122.820000</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.139414</td>\n      <td>...</td>\n      <td>-1.656080</td>\n      <td>122.911653</td>\n      <td>-1.900959</td>\n      <td>23.803451</td>\n      <td>9.532677</td>\n      <td>14.270774</td>\n      <td>0.285783</td>\n      <td>0.285375</td>\n      <td>57.512023</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.400000</td>\n      <td>122.400000</td>\n      <td>10328874</td>\n      <td>-0.003420</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.278977</td>\n      <td>...</td>\n      <td>-1.677024</td>\n      <td>122.764612</td>\n      <td>-2.166094</td>\n      <td>22.216890</td>\n      <td>12.069519</td>\n      <td>10.147371</td>\n      <td>-0.341964</td>\n      <td>-0.342550</td>\n      <td>56.973389</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.460000</td>\n      <td>122.460000</td>\n      <td>0</td>\n      <td>0.000490</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.324334</td>\n      <td>...</td>\n      <td>-2.339323</td>\n      <td>122.702171</td>\n      <td>-1.961412</td>\n      <td>11.687435</td>\n      <td>11.993102</td>\n      <td>-0.305667</td>\n      <td>0.049020</td>\n      <td>0.049008</td>\n      <td>57.050337</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.770000</td>\n      <td>122.450000</td>\n      <td>122.540000</td>\n      <td>122.540000</td>\n      <td>0</td>\n      <td>0.000653</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.387034</td>\n      <td>...</td>\n      <td>-2.239411</td>\n      <td>122.674592</td>\n      <td>-2.116782</td>\n      <td>2.065133</td>\n      <td>10.007509</td>\n      <td>-7.942376</td>\n      <td>0.065327</td>\n      <td>0.065306</td>\n      <td>57.152934</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4186 rows × 91 columns</p>\n</div>"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _get_indicator_data(data)\n",
    "data['close_shift'] = data.shift(-WINDOW)['close']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    1397\n",
      "-1    1396\n",
      " 0    1393\n",
      "Name: class_column, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.975000   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.977500   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.245000   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.112500   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.322500   \n...                               ...         ...         ...         ...   \n2021-05-12 14:30:00-04:00  122.510002  122.959999  122.260002  122.470001   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.820000   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.400000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.460000   \n2021-05-12 18:00:00-04:00  122.460000  122.770000  122.450000  122.540000   \n\n                            Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                   \n2020-05-13 04:00:00-04:00   77.975000         0        NaN  0.000000e+00   \n2020-05-13 05:00:00-04:00   77.977500         0   0.000032  0.000000e+00   \n2020-05-13 06:00:00-04:00   78.245000         0   0.003430  0.000000e+00   \n2020-05-13 07:00:00-04:00   78.112500         0  -0.001693  0.000000e+00   \n2020-05-13 08:00:00-04:00   78.322500         0   0.002688  0.000000e+00   \n...                               ...       ...        ...           ...   \n2021-05-12 14:30:00-04:00  122.470001  10802679  -0.000327  6.306123e+08   \n2021-05-12 15:30:00-04:00  122.820000  14469408   0.002858  6.359432e+08   \n2021-05-12 16:00:00-04:00  122.400000  10328874  -0.003420  6.271393e+08   \n2021-05-12 17:00:00-04:00  122.460000         0   0.000490  6.271393e+08   \n2021-05-12 18:00:00-04:00  122.540000         0   0.000653  6.271393e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_kama  \\\nDatetime                                           ...                  \n2020-05-13 04:00:00-04:00           0    0.000000  ...      77.975000   \n2020-05-13 05:00:00-04:00           0    0.000000  ...      77.976139   \n2020-05-13 06:00:00-04:00           0    0.000000  ...      78.094432   \n2020-05-13 07:00:00-04:00           0    0.000000  ...      78.102394   \n2020-05-13 08:00:00-04:00           0    0.000000  ...      78.196385   \n...                               ...         ...  ...            ...   \n2021-05-12 14:30:00-04:00  -513565942   -0.125334  ...     122.932708   \n2021-05-12 15:30:00-04:00  -499096534   -0.139414  ...     122.911653   \n2021-05-12 16:00:00-04:00  -509425408   -0.278977  ...     122.764612   \n2021-05-12 17:00:00-04:00  -509425408   -0.324334  ...     122.702171   \n2021-05-12 18:00:00-04:00  -509425408   -0.387034  ...     122.674592   \n\n                           momentum_roc  momentum_ppo  momentum_ppo_signal  \\\nDatetime                                                                     \n2020-05-13 04:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 05:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 06:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 07:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 08:00:00-04:00      0.000000      0.000000             0.000000   \n...                                 ...           ...                  ...   \n2021-05-12 14:30:00-04:00     -2.585109     20.805708             5.964983   \n2021-05-12 15:30:00-04:00     -1.900959     23.803451             9.532677   \n2021-05-12 16:00:00-04:00     -2.166094     22.216890            12.069519   \n2021-05-12 17:00:00-04:00     -1.961412     11.687435            11.993102   \n2021-05-12 18:00:00-04:00     -2.116782      2.065133            10.007509   \n\n                           momentum_ppo_hist  others_dr  others_dlr  \\\nDatetime                                                              \n2020-05-13 04:00:00-04:00           0.000000 -32.484186    0.000000   \n2020-05-13 05:00:00-04:00           0.000000   0.003206    0.003206   \n2020-05-13 06:00:00-04:00           0.000000   0.343048    0.342461   \n2020-05-13 07:00:00-04:00           0.000000  -0.169340   -0.169483   \n2020-05-13 08:00:00-04:00           0.000000   0.268843    0.268482   \n...                                      ...        ...         ...   \n2021-05-12 14:30:00-04:00          14.840725  -0.032651   -0.032656   \n2021-05-12 15:30:00-04:00          14.270774   0.285783    0.285375   \n2021-05-12 16:00:00-04:00          10.147371  -0.341964   -0.342550   \n2021-05-12 17:00:00-04:00          -0.305667   0.049020    0.049008   \n2021-05-12 18:00:00-04:00          -7.942376   0.065327    0.065306   \n\n                           others_cr  close_shift  class_column  \nDatetime                                                         \n2020-05-13 04:00:00-04:00   0.000000    76.611198            -1  \n2020-05-13 05:00:00-04:00   0.003206    76.374977            -1  \n2020-05-13 06:00:00-04:00   0.346265    76.417747            -1  \n2020-05-13 07:00:00-04:00   0.176339    76.184998            -1  \n2020-05-13 08:00:00-04:00   0.445656    76.919998            -1  \n...                              ...          ...           ...  \n2021-05-12 14:30:00-04:00  57.063163          NaN             0  \n2021-05-12 15:30:00-04:00  57.512023          NaN             0  \n2021-05-12 16:00:00-04:00  56.973389          NaN             0  \n2021-05-12 17:00:00-04:00  57.050337          NaN             0  \n2021-05-12 18:00:00-04:00  57.152934          NaN             0  \n\n[4186 rows x 92 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n      <th>class_column</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.975000</td>\n      <td>77.975000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>77.975000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-32.484186</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>76.611198</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.977500</td>\n      <td>77.977500</td>\n      <td>0</td>\n      <td>0.000032</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>77.976139</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>76.374977</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.245000</td>\n      <td>78.245000</td>\n      <td>0</td>\n      <td>0.003430</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.094432</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.343048</td>\n      <td>0.342461</td>\n      <td>0.346265</td>\n      <td>76.417747</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.112500</td>\n      <td>78.112500</td>\n      <td>0</td>\n      <td>-0.001693</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.102394</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.169340</td>\n      <td>-0.169483</td>\n      <td>0.176339</td>\n      <td>76.184998</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.322500</td>\n      <td>78.322500</td>\n      <td>0</td>\n      <td>0.002688</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.196385</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.268843</td>\n      <td>0.268482</td>\n      <td>0.445656</td>\n      <td>76.919998</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 14:30:00-04:00</th>\n      <td>122.510002</td>\n      <td>122.959999</td>\n      <td>122.260002</td>\n      <td>122.470001</td>\n      <td>122.470001</td>\n      <td>10802679</td>\n      <td>-0.000327</td>\n      <td>6.306123e+08</td>\n      <td>-513565942</td>\n      <td>-0.125334</td>\n      <td>...</td>\n      <td>122.932708</td>\n      <td>-2.585109</td>\n      <td>20.805708</td>\n      <td>5.964983</td>\n      <td>14.840725</td>\n      <td>-0.032651</td>\n      <td>-0.032656</td>\n      <td>57.063163</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.820000</td>\n      <td>122.820000</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.139414</td>\n      <td>...</td>\n      <td>122.911653</td>\n      <td>-1.900959</td>\n      <td>23.803451</td>\n      <td>9.532677</td>\n      <td>14.270774</td>\n      <td>0.285783</td>\n      <td>0.285375</td>\n      <td>57.512023</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.400000</td>\n      <td>122.400000</td>\n      <td>10328874</td>\n      <td>-0.003420</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.278977</td>\n      <td>...</td>\n      <td>122.764612</td>\n      <td>-2.166094</td>\n      <td>22.216890</td>\n      <td>12.069519</td>\n      <td>10.147371</td>\n      <td>-0.341964</td>\n      <td>-0.342550</td>\n      <td>56.973389</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.460000</td>\n      <td>122.460000</td>\n      <td>0</td>\n      <td>0.000490</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.324334</td>\n      <td>...</td>\n      <td>122.702171</td>\n      <td>-1.961412</td>\n      <td>11.687435</td>\n      <td>11.993102</td>\n      <td>-0.305667</td>\n      <td>0.049020</td>\n      <td>0.049008</td>\n      <td>57.050337</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.770000</td>\n      <td>122.450000</td>\n      <td>122.540000</td>\n      <td>122.540000</td>\n      <td>0</td>\n      <td>0.000653</td>\n      <td>6.271393e+08</td>\n      <td>-509425408</td>\n      <td>-0.387034</td>\n      <td>...</td>\n      <td>122.674592</td>\n      <td>-2.116782</td>\n      <td>2.065133</td>\n      <td>10.007509</td>\n      <td>-7.942376</td>\n      <td>0.065327</td>\n      <td>0.065306</td>\n      <td>57.152934</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4186 rows × 92 columns</p>\n</div>"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_class(df):\n",
    "    higher_threshold = 1.5\n",
    "    lowest_threshold = -1.5\n",
    "    last_values_higher = []\n",
    "    last_values_lower = []\n",
    "    df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)), axis=1)\n",
    "    while True:\n",
    "        class_counts = df['class_column'].value_counts()\n",
    "        if abs(class_counts[0] - class_counts[1]) < 15 and abs(class_counts[0] - class_counts[-1]) < 15:\n",
    "            break\n",
    "\n",
    "        if len(last_values_higher) == 3:\n",
    "            last_values_higher.pop(0)\n",
    "        if len(last_values_lower) == 3:\n",
    "            last_values_lower.pop(0)\n",
    "\n",
    "        last_values_higher.append(higher_threshold)\n",
    "        last_values_lower.append(lowest_threshold)\n",
    "        if class_counts[0] > class_counts[1]:\n",
    "            higher_threshold -= 0.01\n",
    "        if class_counts[0] > class_counts[-1]:\n",
    "            lowest_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[1]:\n",
    "            higher_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[-1]:\n",
    "            lowest_threshold -= 0.01\n",
    "\n",
    "        if higher_threshold in last_values_higher and lowest_threshold in last_values_lower:\n",
    "            break\n",
    "        df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)),\n",
    "                                          axis=1)\n",
    "    print(df['class_column'].value_counts())\n",
    "    return df\n",
    "\n",
    "\n",
    "data = create_class(data)\n",
    "\n",
    "data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\Desktop\\repo\\magisterka_analiza\\data\\results\\train_test\\AAPL_1y_8_diff_13_05_2021 00_26_31_full.csv\n"
     ]
    }
   ],
   "source": [
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\{symbol}_{INTERVAL}_{WINDOW}_diff_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}_full.csv'\n",
    "data.to_csv(filename_to_export, index=True)\n",
    "print(filename_to_export)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1397\n-1    1396\n 0    1393\nName: class_column, dtype: int64"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Class divide\n",
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['close_shift'])\n",
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1397\n-1    1395\n 0    1385\nName: class_column, dtype: int64"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size = 17):\n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def train_model(model,train_x, train_y):\n",
    "    model.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "data": {
      "text/plain": "246"
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataframe = split_dataframe(data, 17)\n",
    "len(splited_dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column', 'close_shift']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "# x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "classifiers = dict()\n",
    "\n",
    "classifiers['DecisionTreeClassifier 1'] = DecisionTreeClassifier(max_depth=10, random_state=0,criterion='gini',splitter='best')\n",
    "classifiers['DecisionTreeClassifier 2'] = DecisionTreeClassifier(max_depth=20, random_state=0,criterion='gini',splitter='best')\n",
    "classifiers['DecisionTreeClassifier 3'] = DecisionTreeClassifier(max_depth=10, random_state=0,criterion='gini',splitter='random')\n",
    "classifiers['DecisionTreeClassifier 4'] = DecisionTreeClassifier(max_depth=10, random_state=0,criterion='entropy',splitter='best')\n",
    "classifiers['DecisionTreeClassifier 5'] = DecisionTreeClassifier(max_depth=15, random_state=0,criterion='entropy',splitter='best')\n",
    "classifiers['RandomForestClassifier 4'] = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0,criterion='gini', n_jobs = -1)\n",
    "classifiers['RandomForestClassifier 5'] = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0,criterion='entropy', n_jobs = -1)\n",
    "classifiers['GradientBoostingClassifier 1'] = GradientBoostingClassifier(n_estimators=100,random_state=0,criterion='friedman_mse',max_depth=3, learning_rate=0.1)\n",
    "classifiers['GradientBoostingClassifier 2'] = GradientBoostingClassifier(n_estimators=100,random_state=0,criterion='friedman_mse',max_depth=10, learning_rate=0.3)\n",
    "classifiers['GradientBoostingClassifier 3'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=3, learning_rate=0.1)\n",
    "classifiers['XGBClassifier 1'] = xgb.XGBClassifier(nthread =-1,max_depth=10,n_estimators=1000, eta =0.2)\n",
    "classifiers['XGBClassifier 2'] = xgb.XGBClassifier(nthread =-1,max_depth=14,n_estimators=1000, eta =0.3)\n",
    "classifiers['XGBClassifier 3'] = xgb.XGBClassifier(nthread =-1,max_depth=14,n_estimators=1000, eta =0.2)\n",
    "classifiers['XGBClassifier 4'] = xgb.XGBClassifier(nthread =-1,max_depth=10,n_estimators=1000, eta =0.5)\n",
    "classifiers['XGBClassifier 5'] = xgb.XGBClassifier(nthread =-1,max_depth=6,n_estimators=1000, eta =0.3)\n",
    "classifiers['XGBClassifier 6'] = xgb.XGBClassifier(nthread =-1,max_depth=3,n_estimators=1000, eta =0.3)\n",
    "classifiers['XGBRFClassifier 1'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=12,n_estimators =100,eta=0.4)\n",
    "classifiers['XGBRFClassifier 2'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=14,n_estimators =100,eta=0.4)\n",
    "classifiers['XGBRFClassifier 3'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=3,n_estimators =1000,eta=0.2)\n",
    "classifiers['XGBRFClassifier 4'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=6,n_estimators =1000,eta=0.2)\n",
    "classifiers['XGBRFClassifier 5'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=10,n_estimators =1000,eta=0.2)\n",
    "classifiers['XGBRFClassifier 6'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1,max_depth=10,n_estimators =100,eta=0.4)\n",
    "classifiers_boosted = dict()\n",
    "classifiers_boosted['GradientBoostingClassifier 1S'] = GradientBoostingClassifier(n_estimators=100,random_state=0,criterion='friedman_mse',max_depth=3, learning_rate=0.1)\n",
    "classifiers_boosted['GradientBoostingClassifier 2S'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=3, learning_rate=0.3)\n",
    "classifiers_boosted['GradientBoostingClassifier 3S'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=2, learning_rate=0.5)\n",
    "# classifiers_boosted['GradientBoostingClassifier 4S'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=2, learning_rate=0.8)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "def count_correct(pred_list, original_list):\n",
    "    correct  = 0\n",
    "    all  = len(pred_list)\n",
    "    for idx , el in enumerate(pred_list):\n",
    "        if el == 1:\n",
    "            if original_list[idx] == 1:\n",
    "                correct +=1\n",
    "\n",
    "    return correct, all\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "start\n",
      " 1    582\n",
      "-1    570\n",
      " 0    565\n",
      "Name: class_column, dtype: int64\n",
      " 1    582\n",
      "-1    570\n",
      " 0    565\n",
      "Name: class_column, dtype: int64\n",
      "1717\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[00:28:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[00:28:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[00:28:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[00:28:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[00:28:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[00:28:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[00:28:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[00:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[00:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[00:29:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[00:29:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[00:29:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_macd_signal', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.35294117647058826\n",
      " 1    585\n",
      "-1    578\n",
      " 0    571\n",
      "Name: class_column, dtype: int64\n",
      " 1    585\n",
      "-1    578\n",
      " 0    571\n",
      "Name: class_column, dtype: int64\n",
      "1734\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[00:32:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[00:32:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[00:32:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[00:32:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[00:32:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[00:32:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[00:32:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[00:32:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[00:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[00:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[00:33:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[00:33:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volume_nvi', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_trix', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    590\n",
      "-1    581\n",
      " 0    580\n",
      "Name: class_column, dtype: int64\n",
      " 1    590\n",
      "-1    581\n",
      " 0    580\n",
      "Name: class_column, dtype: int64\n",
      "1751\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7647058823529411\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[00:35:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 2\n",
      "[00:35:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[00:36:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[00:36:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[00:36:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[00:36:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[00:36:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[00:36:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[00:36:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[00:36:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[00:36:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[00:36:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_trix', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    599\n",
      " 0    585\n",
      "-1    584\n",
      "Name: class_column, dtype: int64\n",
      " 1    599\n",
      " 0    585\n",
      "-1    584\n",
      "Name: class_column, dtype: int64\n",
      "1768\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.9411764705882353\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "1.0\n",
      "Calculate:  XGBClassifier 1\n",
      "[00:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 2\n",
      "[00:39:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 3\n",
      "[00:39:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 4\n",
      "[00:39:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[00:40:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[00:40:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[00:40:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[00:40:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[00:40:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[00:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[00:40:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[00:40:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbl', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.6470588235294118\n",
      " 1    604\n",
      " 0    594\n",
      "-1    587\n",
      "Name: class_column, dtype: int64\n",
      " 1    604\n",
      " 0    594\n",
      "-1    587\n",
      "Name: class_column, dtype: int64\n",
      "1785\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[00:43:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[00:43:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[00:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[00:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[00:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[00:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[00:44:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[00:44:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[00:44:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[00:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[00:44:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[00:44:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "start_data = splited_dataframe[:100]\n",
    "next_data = splited_dataframe[100:105]\n",
    "print(len(next_data))\n",
    "score = defaultdict(list)\n",
    "points = defaultdict(list)\n",
    "points_train = defaultdict(list)\n",
    "score_train = defaultdict(list)\n",
    "step_headers = []\n",
    "i = 0\n",
    "print(\"start\")\n",
    "for idx, day in enumerate(next_data):\n",
    "    start_data.append(day)\n",
    "    data_set = pd.concat(start_data)\n",
    "    data_set = create_class(data_set)\n",
    "    print(data_set['class_column'].value_counts())\n",
    "    y = data_set['class_column']\n",
    "    features = [x for x in data_set.columns if x not in ['class_column']]\n",
    "    x = data_set[features]\n",
    "    x_train= x.iloc[:-17]\n",
    "    y_train= y.iloc[:-17]\n",
    "    x_test =x.iloc[-17:]\n",
    "    y_test=y.iloc[-17:]\n",
    "\n",
    "    print(len(data_set))\n",
    "\n",
    "    step_headers.append(f'<{i}>')\n",
    "    i= i +1\n",
    "    predictions_train= dict()\n",
    "    predictions= dict()\n",
    "\n",
    "    for k,v in classifiers.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v,x_train,y_train)\n",
    "        predictions_train[k] = v.predict(x_train)\n",
    "        score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k],y_train.values))\n",
    "        points[k].append(count_correct(predictions[k],y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "    rfe = RFE(classifiers['RandomForestClassifier 5'],10)\n",
    "    fited = rfe.fit(x_train, y_train)\n",
    "    names = x.columns\n",
    "    columns=[]\n",
    "    for i in range(len(fited.support_)):\n",
    "        if fited.support_[i]:\n",
    "            columns.append(names[i])\n",
    "\n",
    "    print(\"Columns with predictive power:\", columns )\n",
    "    columns = columns + ['high', 'low', 'volume', 'open']\n",
    "    x_test_cropped = x_test[columns]\n",
    "    x_train_cropped = x_train[columns]\n",
    "    for k,v in classifiers_boosted.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v,x_train_cropped,y_train)\n",
    "        predictions_train[k] = v.predict(x_train_cropped)\n",
    "        score_train[k].append( accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test_cropped)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k],y_train.values))\n",
    "        points[k].append(count_correct(predictions[k],y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n",
      "|    | Classifier type               |      <0> |     <90> |     <90> |     <90> |     <90> |     mean |\n",
      "|----+-------------------------------+----------+----------+----------+----------+----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.411765 | 0.176471 | 0.235294 | 0.470588 | 0.176471 | 0.294118 |\n",
      "|  1 | DecisionTreeClassifier 2      | 0.529412 | 0.117647 | 0.352941 | 0.411765 | 0.176471 | 0.317647 |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.470588 | 0.411765 | 0.294118 | 0.941176 | 0.176471 | 0.458824 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.529412 | 0.352941 | 0.352941 | 0.411765 | 0.235294 | 0.376471 |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.352941 | 0.470588 | 0.529412 | 0.411765 | 0.352941 | 0.423529 |\n",
      "|  5 | RandomForestClassifier 4      | 0.529412 | 0.470588 | 0.764706 | 0.470588 | 0.294118 | 0.505882 |\n",
      "|  6 | RandomForestClassifier 5      | 0.529412 | 0.647059 | 0.764706 | 0.470588 | 0.117647 | 0.505882 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.764706 | 0.705882 | 0.529412 | 0.882353 | 0.529412 | 0.682353 |\n",
      "|  8 | GradientBoostingClassifier 2  | 0.647059 | 0.705882 | 0.529412 | 0.823529 | 0.588235 | 0.658824 |\n",
      "|  9 | GradientBoostingClassifier 3  | 0.588235 | 0.705882 | 0.588235 | 1        | 0.823529 | 0.741176 |\n",
      "| 10 | XGBClassifier 1               | 0.529412 | 0.647059 | 0.470588 | 0.941176 | 0.647059 | 0.647059 |\n",
      "| 11 | XGBClassifier 2               | 0.588235 | 0.705882 | 0.470588 | 0.882353 | 0.647059 | 0.658824 |\n",
      "| 12 | XGBClassifier 3               | 0.588235 | 0.588235 | 0.529412 | 0.823529 | 0.647059 | 0.635294 |\n",
      "| 13 | XGBClassifier 4               | 0.529412 | 0.705882 | 0.588235 | 0.823529 | 0.764706 | 0.682353 |\n",
      "| 14 | XGBClassifier 5               | 0.705882 | 0.705882 | 0.470588 | 0.882353 | 0.647059 | 0.682353 |\n",
      "| 15 | XGBClassifier 6               | 0.529412 | 0.823529 | 0.470588 | 0.941176 | 0.882353 | 0.729412 |\n",
      "| 16 | XGBRFClassifier 1             | 0.529412 | 0.411765 | 0.411765 | 0.470588 | 0.588235 | 0.482353 |\n",
      "| 17 | XGBRFClassifier 2             | 0.529412 | 0.411765 | 0.411765 | 0.411765 | 0.588235 | 0.470588 |\n",
      "| 18 | XGBRFClassifier 3             | 0.529412 | 0.176471 | 0.294118 | 0.176471 | 0.235294 | 0.282353 |\n",
      "| 19 | XGBRFClassifier 4             | 0.588235 | 0.352941 | 0.352941 | 0.411765 | 0.647059 | 0.470588 |\n",
      "| 20 | XGBRFClassifier 5             | 0.470588 | 0.352941 | 0.411765 | 0.411765 | 0.588235 | 0.447059 |\n",
      "| 21 | XGBRFClassifier 6             | 0.588235 | 0.352941 | 0.411765 | 0.411765 | 0.588235 | 0.470588 |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.588235 | 0.941176 | 0.470588 | 0.470588 | 0.823529 | 0.658824 |\n",
      "| 23 | GradientBoostingClassifier 2S | 0.352941 | 0.764706 | 0.411765 | 0.705882 | 0.882353 | 0.623529 |\n",
      "| 24 | GradientBoostingClassifier 3S | 0.352941 | 0.764706 | 0.294118 | 0.647059 | 1        | 0.611765 |\n",
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers2 = [\"Classifier type\",] + step_headers\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "accuracy_df = pd.DataFrame(score_df['Accuracy'].tolist(), index= score_df.index, columns=step_headers)\n",
    "score_df = score_df.drop('Accuracy', 1)\n",
    "f_out = pd.merge(score_df, accuracy_df, how='left', left_index=True, right_index=True)\n",
    "f_out['mean'] = f_out.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out,headers2 , tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "f_out.to_csv(filename_to_export, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'DecisionTreeClassifier 1': [0.92,\n              0.9295282469423413,\n              0.933679354094579,\n              0.9189034837235865,\n              0.9298642533936652],\n             'DecisionTreeClassifier 2': [1.0,\n              1.0,\n              0.9994232987312572,\n              0.9988577955454027,\n              0.998868778280543],\n             'DecisionTreeClassifier 3': [0.8323529411764706,\n              0.8124635993011066,\n              0.8137254901960784,\n              0.7966876070816676,\n              0.7997737556561086],\n             'DecisionTreeClassifier 4': [0.9129411764705883,\n              0.8980780430984275,\n              0.9002306805074971,\n              0.9120502569960023,\n              0.8981900452488688],\n             'DecisionTreeClassifier 5': [0.9988235294117647,\n              0.9924286546301689,\n              0.9930795847750865,\n              1.0,\n              0.9932126696832579],\n             'RandomForestClassifier 4': [0.6276470588235294,\n              0.6208503203261503,\n              0.6141868512110726,\n              0.6213592233009708,\n              0.6119909502262444],\n             'RandomForestClassifier 5': [0.5488235294117647,\n              0.5270821199767035,\n              0.5288350634371396,\n              0.5316961736150772,\n              0.5192307692307693],\n             'GradientBoostingClassifier 1': [0.9394117647058824,\n              0.9341875364006988,\n              0.9388696655132641,\n              0.9354654483152485,\n              0.9326923076923077],\n             'GradientBoostingClassifier 2': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'GradientBoostingClassifier 3': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 1': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 2': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 3': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 4': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 5': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBClassifier 6': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'XGBRFClassifier 1': [0.9935294117647059,\n              0.9947582993593477,\n              0.9919261822376009,\n              0.9942889777270132,\n              0.9920814479638009],\n             'XGBRFClassifier 2': [0.9970588235294118,\n              0.9982527664531159,\n              0.9965397923875432,\n              0.9977155910908052,\n              0.9971719457013575],\n             'XGBRFClassifier 3': [0.658235294117647,\n              0.6598718695398952,\n              0.6464821222606689,\n              0.6493432324386065,\n              0.6515837104072398],\n             'XGBRFClassifier 4': [0.8676470588235294,\n              0.8707047175305765,\n              0.8748558246828143,\n              0.8686464877213022,\n              0.871606334841629],\n             'XGBRFClassifier 5': [0.9794117647058823,\n              0.9813628421665695,\n              0.9803921568627451,\n              0.9771559109080525,\n              0.9768099547511312],\n             'XGBRFClassifier 6': [0.9758823529411764,\n              0.9842748980780431,\n              0.9803921568627451,\n              0.981724728726442,\n              0.9773755656108597],\n             'GradientBoostingClassifier 1S': [0.8605882352941177,\n              0.9132207338380897,\n              0.9146482122260668,\n              0.8532267275842376,\n              0.8467194570135747],\n             'GradientBoostingClassifier 2S': [1.0, 1.0, 1.0, 1.0, 1.0],\n             'GradientBoostingClassifier 3S': [1.0, 1.0, 1.0, 1.0, 1.0]})"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n",
      "|    | Classifier type               |      <0> |     <90> |     <90> |     <90> |     <90> |     mean |\n",
      "|----+-------------------------------+----------+----------+----------+----------+----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.92     | 0.929528 | 0.933679 | 0.918903 | 0.929864 | 0.926395 |\n",
      "|  1 | DecisionTreeClassifier 2      | 1        | 1        | 0.999423 | 0.998858 | 0.998869 | 0.99943  |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.832353 | 0.812464 | 0.813725 | 0.796688 | 0.799774 | 0.811001 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.912941 | 0.898078 | 0.900231 | 0.91205  | 0.89819  | 0.904298 |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.998824 | 0.992429 | 0.99308  | 1        | 0.993213 | 0.995509 |\n",
      "|  5 | RandomForestClassifier 4      | 0.627647 | 0.62085  | 0.614187 | 0.621359 | 0.611991 | 0.619207 |\n",
      "|  6 | RandomForestClassifier 5      | 0.548824 | 0.527082 | 0.528835 | 0.531696 | 0.519231 | 0.531134 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.939412 | 0.934188 | 0.93887  | 0.935465 | 0.932692 | 0.936125 |\n",
      "|  8 | GradientBoostingClassifier 2  | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "|  9 | GradientBoostingClassifier 3  | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 10 | XGBClassifier 1               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 11 | XGBClassifier 2               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 12 | XGBClassifier 3               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 13 | XGBClassifier 4               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 14 | XGBClassifier 5               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 15 | XGBClassifier 6               | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 16 | XGBRFClassifier 1             | 0.993529 | 0.994758 | 0.991926 | 0.994289 | 0.992081 | 0.993317 |\n",
      "| 17 | XGBRFClassifier 2             | 0.997059 | 0.998253 | 0.99654  | 0.997716 | 0.997172 | 0.997348 |\n",
      "| 18 | XGBRFClassifier 3             | 0.658235 | 0.659872 | 0.646482 | 0.649343 | 0.651584 | 0.653103 |\n",
      "| 19 | XGBRFClassifier 4             | 0.867647 | 0.870705 | 0.874856 | 0.868646 | 0.871606 | 0.870692 |\n",
      "| 20 | XGBRFClassifier 5             | 0.979412 | 0.981363 | 0.980392 | 0.977156 | 0.97681  | 0.979027 |\n",
      "| 21 | XGBRFClassifier 6             | 0.975882 | 0.984275 | 0.980392 | 0.981725 | 0.977376 | 0.97993  |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.860588 | 0.913221 | 0.914648 | 0.853227 | 0.846719 | 0.877681 |\n",
      "| 23 | GradientBoostingClassifier 2S | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 24 | GradientBoostingClassifier 3S | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "headers2 = [\"Classifier type\",] + step_headers\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "accuracy_df_train = pd.DataFrame(score_df_train['Accuracy'].tolist(), index= score_df_train.index, columns=step_headers)\n",
    "score_df_train = score_df_train.drop('Accuracy', 1)\n",
    "f_out_train = pd.merge(score_df_train, accuracy_df_train, how='left', left_index=True, right_index=True)\n",
    "f_out_train['mean'] = f_out_train.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out_train,headers2 , tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "f_out_train.to_csv(filename_to_export_train, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'DecisionTreeClassifier 1': [(4, 17),\n              (2, 17),\n              (1, 17),\n              (5, 17),\n              (3, 17)],\n             'DecisionTreeClassifier 2': [(4, 17),\n              (2, 17),\n              (2, 17),\n              (4, 17),\n              (3, 17)],\n             'DecisionTreeClassifier 3': [(5, 17),\n              (1, 17),\n              (2, 17),\n              (4, 17),\n              (3, 17)],\n             'DecisionTreeClassifier 4': [(3, 17),\n              (0, 17),\n              (4, 17),\n              (0, 17),\n              (4, 17)],\n             'DecisionTreeClassifier 5': [(3, 17),\n              (0, 17),\n              (4, 17),\n              (0, 17),\n              (6, 17)],\n             'RandomForestClassifier 4': [(5, 17),\n              (3, 17),\n              (8, 17),\n              (4, 17),\n              (5, 17)],\n             'RandomForestClassifier 5': [(5, 17),\n              (7, 17),\n              (8, 17),\n              (4, 17),\n              (2, 17)],\n             'GradientBoostingClassifier 1': [(4, 17),\n              (3, 17),\n              (3, 17),\n              (4, 17),\n              (9, 17)],\n             'GradientBoostingClassifier 2': [(4, 17),\n              (4, 17),\n              (3, 17),\n              (3, 17),\n              (10, 17)],\n             'GradientBoostingClassifier 3': [(5, 17),\n              (3, 17),\n              (5, 17),\n              (5, 17),\n              (14, 17)],\n             'XGBClassifier 1': [(4, 17), (2, 17), (2, 17), (4, 17), (11, 17)],\n             'XGBClassifier 2': [(4, 17), (3, 17), (2, 17), (4, 17), (11, 17)],\n             'XGBClassifier 3': [(4, 17), (1, 17), (3, 17), (2, 17), (11, 17)],\n             'XGBClassifier 4': [(4, 17), (3, 17), (4, 17), (3, 17), (13, 17)],\n             'XGBClassifier 5': [(4, 17), (3, 17), (2, 17), (4, 17), (11, 17)],\n             'XGBClassifier 6': [(4, 17), (5, 17), (2, 17), (4, 17), (15, 17)],\n             'XGBRFClassifier 1': [(4, 17),\n              (0, 17),\n              (3, 17),\n              (4, 17),\n              (10, 17)],\n             'XGBRFClassifier 2': [(4, 17),\n              (0, 17),\n              (3, 17),\n              (4, 17),\n              (10, 17)],\n             'XGBRFClassifier 3': [(4, 17),\n              (0, 17),\n              (0, 17),\n              (0, 17),\n              (4, 17)],\n             'XGBRFClassifier 4': [(4, 17),\n              (0, 17),\n              (2, 17),\n              (4, 17),\n              (11, 17)],\n             'XGBRFClassifier 5': [(4, 17),\n              (0, 17),\n              (3, 17),\n              (4, 17),\n              (10, 17)],\n             'XGBRFClassifier 6': [(4, 17),\n              (0, 17),\n              (3, 17),\n              (4, 17),\n              (10, 17)],\n             'GradientBoostingClassifier 1S': [(4, 17),\n              (7, 17),\n              (2, 17),\n              (5, 17),\n              (14, 17)],\n             'GradientBoostingClassifier 2S': [(3, 17),\n              (4, 17),\n              (4, 17),\n              (5, 17),\n              (15, 17)],\n             'GradientBoostingClassifier 3S': [(3, 17),\n              (4, 17),\n              (2, 17),\n              (5, 17),\n              (17, 17)]})"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "magisterka_analiza",
   "language": "python",
   "display_name": "Python magisterka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}