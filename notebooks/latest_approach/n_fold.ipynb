{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from finta import TA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "from xgboost import XGBClassifier\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "WINDOW = 8  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '1y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "\n",
    "# one day 16 rows of data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 Open        High         Low     Close  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.9750   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.9775   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.2450   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.1125   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.3225   \n...                               ...         ...         ...       ...   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.8200   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.4000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.4600   \n2021-05-12 18:00:00-04:00  122.460000  122.950000  122.350000  122.7500   \n2021-05-12 19:00:00-04:00  122.750000  123.000000  122.640100  122.7900   \n\n                           Adj Close    Volume  \nDatetime                                        \n2020-05-13 04:00:00-04:00    77.9750         0  \n2020-05-13 05:00:00-04:00    77.9775         0  \n2020-05-13 06:00:00-04:00    78.2450         0  \n2020-05-13 07:00:00-04:00    78.1125         0  \n2020-05-13 08:00:00-04:00    78.3225         0  \n...                              ...       ...  \n2021-05-12 15:30:00-04:00   122.8200  14469408  \n2021-05-12 16:00:00-04:00   122.4000         0  \n2021-05-12 17:00:00-04:00   122.4600         0  \n2021-05-12 18:00:00-04:00   122.7500         0  \n2021-05-12 19:00:00-04:00   122.7900         0  \n\n[4187 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.8200</td>\n      <td>122.8200</td>\n      <td>14469408</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.4000</td>\n      <td>122.4000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.4600</td>\n      <td>122.4600</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.950000</td>\n      <td>122.350000</td>\n      <td>122.7500</td>\n      <td>122.7500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 19:00:00-04:00</th>\n      <td>122.750000</td>\n      <td>123.000000</td>\n      <td>122.640100</td>\n      <td>122.7900</td>\n      <td>122.7900</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4187 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download(  # or pdr.get_data_yahoo(...\n",
    "    tickers=symbol,\n",
    "\n",
    "    period=INTERVAL,\n",
    "\n",
    "    interval=FETCH_INTERVAL,\n",
    "\n",
    "    # group by ticker (to access via data['SPY'])\n",
    "    # (optional, default is 'column')\n",
    "    group_by='ticker',\n",
    "\n",
    "    # adjust all OHLC automatically\n",
    "    # (optional, default is False)\n",
    "    # auto_adjust = True,\n",
    "\n",
    "    # download pre/post regular market hours data\n",
    "    # (optional, default is False)\n",
    "    prepost=True,\n",
    "\n",
    "    # use threads for mass downloading? (True/False/Integer)\n",
    "    # (optional, default is True)\n",
    "    threads=True,\n",
    "\n",
    "    # proxy URL scheme use use when downloading?\n",
    "    # (optional, default is None)\n",
    "    proxy=None\n",
    ")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n",
    "data.head(10)\n",
    "important_columns = ['open', 'high', 'low', 'close', 'volume']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_diffs(diff_number, col_name):\n",
    "    new_col_name = f'{col_name}_{diff_number}'\n",
    "    data[new_col_name] = data[col_name].diff(diff_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# for name in important_columns:\n",
    "#     for i in range(1, 11):\n",
    "#         calculate_diffs(i, name)\n",
    "#\n",
    "# data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Datetime'>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEECAYAAADOJIhPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/W0lEQVR4nO2dd5xU1fXAv2e279Jh6WXpVbogKiIWUDESjSZ2E1tMTDRqNGhs0RgxRmOM0cTYY2KNRn+iWLBgBVGRpnQEpCN968zc3x/vvdk3ZXdnd6fP+X4+fHjvvvtmzpmdeefec889R4wxKIqiKNmJJ9kCKIqiKMlDjYCiKEoWo0ZAURQli1EjoCiKksWoEVAURclicpMtQGPo0KGDKSsrS7YYiqIoacVnn322wxhTGulaWhmBsrIyFixYkGwxFEVR0goR+aaua+oOUhRFyWLUCCiKomQxagQURVGyGDUCiqIoWYwaAUVRlCxGjYCiKEoWo0ZAUZRm4fX5eeSDteytrEm2KEoTUCOgKEqzeH/VDm55ZRnDb34j2aIoTUCNgKIoTWbDd+Vc98LiZIuhNAM1AoqiNJkrnlnI5j2VAHRpXZhkaZSmoEZAUZQm4xiA0GMlfVAjoChKk/l2d0WyRVCaiRoBRVGULEaNgKIoShajRkBRlCbh95ug8+L8nCRJojQHNQKKojSJT9d9FzhuV5KPL8QoKOmBGgFFUZrE859tDBy3Lsqjyuvnj7O/TqJESlOImREQkUdEZJuILIlw7dciYkSkg6vtWhFZJSLLRWRqrORQFCX+lFd7eXXxZr4/sit/OPkgTjioMwD3v7s6yZIpjSWWM4HHgONCG0WkB3AssN7VNgQ4HRhq33O/iKhDUVHShNlLtnCg2seZ43tx5vie5HjUqZCuxOwvZ4yZC3wX4dKfgWsAt8NwOvC0MabKGLMWWAWMi5UsiqLElwXf7KJ1UR4Hl7UFwCNJFkhpMnE13yJyEvCtMebLkEvdgA2u8412W6TXuFhEFojIgu3bt8dJUkVRGkNljY8WBbmIWE//HFErkK7EzQiISDHwW+DGSJcjtEUMLTDGPGiMGWuMGVtaWhpLERVFaSJVXj8FebWPD49OBdKW3Di+dl+gN/ClPVroDnwuIuOwRv49XH27A5viKIuiKDGkqsZPQW7tMl6OGoG0JW4zAWPMYmNMR2NMmTGmDOvBP9oYswV4GThdRApEpDfQH5gfL1kUJdvZU1FDtdcfs9er8vooyK19fKg7KH2JZYjoU8DHwEAR2SgiF9TV1xizFHgWWAbMBi41xvhiJYuiKMGM+N0bXPLkZzF7vSqvP8gI1OcOWr5lHwOuf42Nu8pj9v7pzqbdFXy0akeyxQBiGx10hjGmizEmzxjT3RjzcMj1MmPMDtf5bcaYvsaYgcaY12Ilh5I6/OvjdZTNmEWNL3YjUKXxODOAt7/eFrPXtNYEoovq/uPsr6n2+nlr2daYvX+6M+3e9znzoXls3Zv89Nsa3KvEjdte/Qogpm4IpfF8sX4XAPm5sfm5P//ZRr7csJv8nNrRf33OoDm28encupDKGh9nPzSPe+esjIks6cqucqse82/+uyjJkqgRUOJIZY318PeovzipzLRTOUwaEJvoOqecpDtXUF3J49Zs3x84fuLjbxh0w2w+WLWDu99cERNZlOajRkCJOyZy9K+SIMralwDQuVVsyj/W+MONe8vCvMDxateD/6i73gscf7R6Z9DrXP70F0Hn+yprMCY7viuti6zPa2DnlkmWRI2AomQ8zoP1X598E5P1mVb2A3+Oa42hRWFttPnFTywAYPOe+quOvbRwU+DB/5956zno5jd48Ytvmy1fOnDyKGtvbEFO8h/ByZdAyXiyZHCXstT4av8A9729qtmv17FlQVhbi4JaI7DhO+vhP+H2t4P6fH3rcRSFLCafdN+HnPfop1z3ouVi2pQF5SqXbdrLx/asyJcCPw41AoqS4bhH/xu+a36YZu8Olnvp/rNGB9pauWYC1RFmG22K8yjMy+GrW49j7e0nBNrX7jjA3BXb+eVR/QAoyo/n/tXEUlnjixgUccK977N86z4AUiFwTo2AEneSP9bJbtxGYFi31s1+vUqvn5E92nDCQV0CbX1KW3DcUCud9IWH9+bcR4L3fj730wmBYwkJFPj1lAFcdnR/ACqqvc2WLxk8+ck3lM2YRdmMWcz5aitLN+1h0A2zGXC9Ff1+4eOfcvPLS6msCd4O5fMn3wqoEVDiwvZ9VYHjbFnsS1W8riieW15ZFngQVVT7wkpERkNFtTfMrZPjEe49YxRgjfrnrghO9ti/U/AC6EczjgocF+fnkusRRKDal37flW37Krn+f7VlVC54fAG/fq429LOyxsdbX23jsY/W8eNHg42jzgSUjGXbvuRvglEsqr1+erQrCpy//OUmanx+Bt84m1tnLWv061XU+CKGhObau4bXR+FycucaKszLQUTIy/Gk5cbCzbvDv+tfbd4bON62t3ZA9Mma4Gz7fnuAVOPzU56kWZAaASUuiGv7UPqN7TKLGp+fLq1qjYDfb/hi/W4AHv1wXaNfr6LaR2EEI+DxCB6BuSsaTofQoUVBwDBNHdoJsIxVOi4M+xuY6R5x5zuBY2c9xcFru4POePAThtz4euyFiwI1Akpc8Lp8neoNShwbd5Xz7e4KPl+/K+D22VvppUPL/ECfDbvKOefheUDkSJ+GqKzxh7mDHHJzPGxxpUJ48oLxvHrZxLB+OR7h/WuOYt3MabRvUSvDSwvTL5lwTYgL69LJfZl33dFh/c4Y14O3r5oUtDD+5CfrWbN9Pwu+2RV3OetCjYASF0J/GEpiOPyOdzhs5tuccv9HXPfiYvx+w+7yaloV5vH8Jdbi7N/eWU3n1tbGsan2Yu6+ypqgdZy68Pr87NhfRZuivIjXQ7OJHt6/A0O6tmqOSinJM5+up2zGLP72zqow99fVUwfRqVUhh/RpF9Sel+NBRBAR/vuzQwPtJ933YUJkrgs1AkpcCNoopPYgKcxdsZ0LHv+UHfurGdq1FXsrawLXnv3pBNqX5Ad2c59y/0ccfNtbDb7m8q37qPL6Oah75CijCnv2cev0ocz/bfhouD6OsNNarN+Z+tlGP11njdzvfH05v34utHCixdJNe4PO3eZxTK+2geP9VbVrATv2N2yIY40aASUu/OI/XzTcSYkrO/ZX8+Gqndz6/WGcfUgvJvTpQFFeDi9dehidWhUiIjjBQSu3WakefA1EC9388tKo+h05sCMdWzYuTcVPDisD0iOooFsbaz3jV8f0r7PPFccMCDqfvy7Y5XP7KQeF3TNvTaQy7fFFjYASd3YeSPzoJhuJFO75/M8mcM4hvRARivKtzVojerQBQCQ8fPe8R+bXGzbqLPgP7lK/i6dHu+JGSk9gnSEdss6WV3vJz/EwfWRwafTXf3VE4Pj8w3vz1pWTAufXHDcwqO+EPu3DXvezJKwNqBFQ4o47hlqJHyu27Qs6/+VR/RjevU2d/T0Cu8uDK459sGoHfa57lQXrvuOh99eEGYlJAy2XTWiUi8Olk/ty/bTBTZLfSXVdlQZhojU+Q2GeJyis860rjwhLCNevYws+ufZo7jx1OJMHdgy6VhbhM9xXWcO8NTspmzGL5Vv2hV2PB7GsLPaIiGwTkSWutltFZJGILBSRN0Skq+vatSKySkSWi8jUWMmhpAZun2do9kglPjg5exwaSuFdUe3jtSVbArtaAYZ1s0b4p/79Y34/6yt6X/tq0D3OLCG3jkpiV08dxIUT+zRadoB8O5nayq378KawITDG8NhH66j0+unaujb0NtcT+XHauXUhp43tEfHaYz85mFcvm8i6mdPo37EF+yq9vLZkC2AZ5EQQy5nAY8BxIW13GmOGG2NGAq8ANwKIyBDgdGCofc/9IhJdmSIlLUjGtDbbcVIQOGGf7Vvk19edvZXhm5P+cc7Yeu9xdh/Ho7C8Y7T+8OrX9Pvtaym70/y5zzYCltuqbUk+xwy2RvjFBY1/hB05sGMgeqooPyewsA7wXsiu63gRs2xNxpi5IlIW0uZeHi+hNk5kOvC0MaYKWCsiq4BxWDWKlTQn1KfclFh0pfE4D+h/XziePRU1QbOxaCm2/fJ9S0tYvf1A2HW/MXgkPP9PLAhdEK6s8VNUR7GaZNKqMDg89p/njmXznspGL4SHkuMR/PYsA6zorkNvn8OD546NSc6nuoj7moCI3CYiG4CzsGcCQDdgg6vbRrst0v0Xi8gCEVmwfXtiLKPSPLxNyEejNB+fa5Q+tqxdox/UL116WOCh6zYA7jUDr9/EZRYQiVXb9jfcKQkcqAqeQYkIXdsU1dE7enJEeH9lrQtofO92bNpTyYl//aDZr10fcTcCxpjfGmN6AP8GfmE3R/oWRXxyGGMeNMaMNcaMLS2NTXk8Jb6EbqNvV1K/W0JpOsYY/jpnJRXVPrw+x1/ftJ91u5J8CiLUIX518ebAsT+ORiB05nLrK43Pa5QI4hXL7wn5XB8/f1xc3ifsfRPyLhb/AX5gH28E3Csl3YH02y+uRMRtBIZ1a0WrOnaXKs2n97WvctebK5j4x3dqZwI5TXtIF+R5Is4efvXMQgAWrPuOf8xdE5QXKpa0LMwLbBgDWPztnri8T3O5/TWrZvPfzx7dQM/G4Y40OmpQRwrzcjisX3vGNsGt1xjiagRExL2T4iTga/v4ZeB0ESkQkd5Af2B+6P1KamOM4fKnvwhUSXJwbyRqXZTX4MYipWn82VWsfXCXlgE3XF2ROw1RWEc+IIdT/24t2VWE5MSPJfmucosVNT627Im8cWzF1n3sTMLuWjfHDevScKdGsGNfdeC4f6cWABTk5lAV530TsQwRfQprYXegiGwUkQuAmSKyREQWAVOAywGMMUuBZ4FlwGzgUmNM/L5ZSlyo8vp5aeEmzn1kXlC7kzvuhhOHkOvx6BpBHDhQ5eUvc1YGzicP7BhI2tdUd43jCrry2AFh115zuYTii/VdmXH8IAD+78vIDoIpf57LmN+/lfABRlPqL0SL2830iT2wKsj1hBWiiTUxMwLGmDOMMV2MMXnGmO7GmIeNMT8wxgyzw0S/Z4z51tX/NmNMX2PMQGPMa/W9tpKa1PVwd9xBOWKNSlM55jtdCS3IXuPzu9YEojMCD5wV7M5wRuF9S61R6PHDOnOQHZVyw0u1G/7G9Q5OjBZLLj6iLwCnjulO39ISbnv1K174fGOd/Zck2GXk5F+68cQhMX9t9+/JcaEW5HrSZyagZB813sgjT6d4tscj5OaIuoPiQGjGz/JqX1B0UDQcf1AXLpnUN3DurAf0tFM+jOrZJvDA31NREygmf8a4yBufYsG43u1YN3MaHVoUsKvceuBe+WzkBG3Q8Ia4WLPP3lvRsjB+tZAnDywN5BWy3EFpMhNQso8a2/2Q6/Hwx9lfUzZjFlA7ZfaIkOtJfrWoj1db2/CTkaExXoQ+hP4yZyW3vfoV0LjooJ9P7hvWdlD31rx15RFceHgfPl1nJTS7dHI/Tj/YevgPCCkVGS9mXXZ4xHb3JrLcJi6CN5U/2J9xaIbQWPLoT8bRva1liAvzPFTW6ExASVGcmgH7q7zc/+5qwMo3v9lezNtdXk2OJ/kzgX++vwYgUE0rE6i2DWukTJSNeTA6PfNC7unXsSUej3DehDJaFeby8yP78eupA3nm4kMY2jV+G5fcdHGlZCibMYs9FdbMwO02iXZT8SX/+oxhNzWvcteHq3YEUjocNahjA70bzxPnj+Ou00YEtRXk5aTPmoCSfdRE8FVW+/w88uFaAJ5dsJHcHEn6wnBix4qJocoeHf5obA96tCsKSlscWtilPkrycxnTqy1/P3tMxOs/GNOdRTdPJT/XQ2FeDuMjZL5MFE6dAfdD8eEP1kZ17+ylW4Ly9jeFsx6qDYCIVGO5uRwxoJQfjOke1NaiIJcqr59XFsUvgl6NgNJk3vpqa1jbkBtfD5QIbFOcZy8MJ9cIOCPjZLulYkm1z09ejuCxyzRe7sprH7rpqD48HqvK1dGDO8VDzJjSwnaBHaiqNQL/rWfROJ64C/TEEyf/Uzzrc6gRUJrE3soa7ntnVb19xvduZ7mDkpwIrMIeNRfmpefX3evzh7kEqr3+oJj6TMWpfQC1Rrw536eK6ti4Vib2T0z2AneeoljJHkrmf4uUuPDge2vYXR55NPTsTyfwnwvH8+upA/GIxDW2Ohrm2tkY89L0oXneo/MZdMPsoLZqrz+Qfz+TGdOzdresU5eiOd+nwTfObrhTA7xxxREJ+y65AwCemr8+Lu+R+d8iJeZs3FXOfe+s4oSDOvPKL8MjONqV5HFovw4U5OaQmwIzAYdku6WawjvLt/HhqvB6DNliBNqV1I6E56+1IpVC15gam3K6qSGX3doUMe2gLgmLjgIrlYZD6zilX8n8b5ESc9Z/Zy3QTRnSOSgmfc5Vk7h+2uDAZiOwfM6+FHn4VqfZmoAxhp88+mnEa9U+PwW5qZdmOda0DEnb/MC7qwPRZoPsKl4N/V1fWhi8sW7rnqaFClfU+BKeDLGVayYQr+9v/HY8KBmLM6Lu3rYoKFlc39IWQQYArEgVnQlEz6bdFdT4/PRqX8I9b60MurZq2376dbQ+30gzAatmcMJETQihGWnvmP11oM3ZMFdZU79B3BySf+jb3RX0bG/F4e8pr8HjCTc2YPngt++rCvQtr/bGJSqoPtzJF+NVe1lnAkqjcRbo8nI8DS5WpcI+AQcnt04qc+jMt5l057tAeLH2Y+5+L3BcFWFh+KMZR/HCzw+Nu4yJ5On5G8La7nx9OVAboVPVQBx9Wfvgz3F3uZWo7ZEP1jLiljcYc+tbEe+bdu/7HHHnO1TW+PD7DZU1/jCjFG/cawLx2jmsRkBpNE7loyqvnwGdW9KyIJf/XDg+Yl+nWlIqUJPCM4Fqr5/+v62t53vXG8vp1Cq8IluV18c3Ow/w1ldbw2YCXVoXMbpnfNMOJ5rtEXZ5O3rPON4qaN/QjtrQ6+X2wOUWu15Btc/P858Fh5ruKa9hzQ6rsM70+z5k1K1vAvDP96PblxArilyZXXUmoKQMTvWjHfuraFWYx+LfTeXQfh0i9s3xJH+zmEMqJ7J7d/m2ICP117dXBX703dvW7px9aeGmwEzh293BheUzkdCdzGA9DP91wbiAkayMMEIe8bs3uPTfn1vXQ2YK5RFmDqHZSp3dyQDLt+4LzCb+cvrIxinQTESEz284FoA/vbGigd5NQ42A0mSiiVP3iGAMHPWnd+MvUATc4YTOyC4VKc4PX55zRqzu0f01zy8KHMdrZJhKRPLVd2tTxGF9O1BorwMs3hieSXRPRQ2z7PTXjhG4eupAAN63Q4bdqR96dygJut+96fr6aYN56ReHs27mNKaPjFgFN67EezFajYDSaC4/2tqdOq5PwymFneihZD2A3YvSD85dg89vUmaNwk2kcM8P7BlXq6LI8RttizO/Ytsj5x0c1jZ1aGc8HgkUwbnqueAso3e+/nXQuTPyP/+w3gC8scza6e72t4eW1XTvLnfuSyY/P7Ivj/x4bFxeW42A0mhKCqwfXzRpfJO9HhD6wO973auM/N0bMXv9o+96l5G3NP/1lm0KH80+s8BaFK3xBuvguCSmDO3c7PdNdXq2L2bdzGnc86ORgbYpQ60UF+4d4C9+Yfn0n12wgb+9szrQXlnj44+zl4f1L6/24vMb+nQooWVhblDO/rIZszjqLmsR3knNkWyuOW4QRw2KT2oPDRFVGo3zXI/mpxEa5phoIo369zUzkZiD1+dn9fbYzHBu/j9rkfKa4wayv9LLdweq8foNyzbtDct5dNKIrrQuymPSgMSkLkgF3DOl/naYbJc2tWslVzzzJYf17cB/5gXvqr3KVYvAXT/549U7eWWR5S5qX5LPO8u38diMdfx6SnBVtd8cNyh2SqQoMTMCIvIIcCKwzRgzzG67E/geUA2sBn5ijNltX7sWuADwAZcZY5qX51VJGM5jNcH1PJpEXYvSby3byoVPLOCz64+hfYvwKJxoWFJPTvlV2/bhN43Pvd+nQ0lY7dqd+6t4wVVJTEQ4cmDsUxmnMu5RvPP36uYyAgAPf7iWhRt2B7U56wJDu7YKand/d3eVV7PzgBU2Grr4etKIrs2SOx2IpTvoMeC4kLY3gWHGmOHACuBaABEZApwODLXvuV9EMn/7Y4bgzASicQfVNZKq8voSsrBZV/pgJ931V5v3Nfm1P1q9o85rx9w9lyl/nstvX1wc1Ws5/ulIU363kXr8/HGNlDIzKK9jP8oxg2uN4T/eWxOxz7qZ05h12cSgtvMfWwBYxqG+JaJE7xBOBrGsMTwX+C6k7Q1jjPMr/ARwkmVPB542xlQZY9YCq4Ds/HanIY3x83duXfsAc28sG3j9bCb+8e2YyhWJffaGovEhdXFXbN0PQEEzMot+vLo2p8/+Ki9lM2ZRNmNWkPvm3/OiS/rVpjiPk0d1azAfUDa5gNwcM7gTB5e15flLJgS19+vYuJnWL4/qFzi+8cQhYcbBeS+H3DRNOtgYEqnh+YBTUL4b4N4KuNFuC0NELhaRBSKyYPv27XEWUWkM0biD3LOF215dFnRt6974l3t0qoldd8LgoHan1GRTs0FWeX2B0otAUNWqe+eEp3toiIpqH0X1pCS478xRPPrj8EiZbKEwL4fnLjmUsWXBxvzv762u447IOJFtRw3qyPmHW1E/h7ii3NoW5/HPcyMX2MlUEmIEROS3gBf4t9MUoVvE4aUx5kFjzFhjzNjS0uwcBaUaTly2RLE07DYCT36yvl4XSqwxxvCvj79hUOeWDO8euSRiQykHIvHwB2sZdMNsKmv89CktCQsv/OvbwXUW3Oke6qKi2kdxXt1G4MThXZkch5KG6c710wbXe31Il+C1gNwcD+tmTuPh82rDLU8aUTv+/PXUgUELyNlA3I2AiJyHtWB8lqnN+boR6OHq1h2IX/00JabMXmrVWc2JInTOna+nd4cSrn5uUUQ/fWWNL5DTJVZ8vn43yzbv5ZwJvcJ+2M7DoSmZGW99ZVlgXeSwvh2i2ncw5c91GwJjDOU19c8ElMicOb5nndeeu2QCL/3isIjX3N8HZ1fywWVtOXOc9XoT+3fggbNGx1DS1CWuRkBEjgN+A5xkjCl3XXoZOF1ECkSkN9AfmB9PWZTY4N6CH40RcJcC/NNpw9m8p4Lv/fWDsH4n3fcBI295MzZC2jz5yTe0LMjl+yG7PHu2K+ZXdjnGcx6ez5chESX1sTMkl03bkvywCKSivByuOnYAPz+yb6Btxdb9lFdHXqSu8voxBjUCTcC90/r7I4MjedoU5UXl7htl78i+aGKfgHH41wXjOf6gLvXdljHEzAiIyFPAx8BAEdkoIhcA9wEtgTdFZKGI/B3AGLMUeBZYBswGLjXGxCdFnhJT9lVaD7JLJ/dtoKeFezF4TK92dGxZyNoIu4edhdpY8d2BamYt2swPxnSnpCA4EnruNZMD6YEBpv/tw6hf1y37IX3akWsbwlyPBPLL3H/2aH55dH+uCYmM2lVeE1ioduN8RvW5g5SGOffQsibd169jC9befkJWbL6LRMz2CRhjzojQ/HA9/W8DbovV+yuJwUmsFW38+4GQ0e+WvZV19GyCLOU1PP7xOn52ZN+wEd/Xm/dS7fMzZUjkXZah+edXb98fVgshEu7Zz/mH9Walvejr9RtOGtmNcyf0okMd+w5uf/UrXlm0mSfOH8cRriifCnt2pTOB5hGay6oxrv1sWwdwk/nxT0pMcXK4t4qQ2CsS8SqODXDPnBXc/eYK3rRzwbhxdgW3duXX+fGhZZwyynINhYZiXvHMwqje0+3eMgTvQ/hg5fYwA3D3D0cEjp0dqp+v3wXAJ2t28vT89YEY+KIISeSUhnnqokN48eeHkhsh46jSMGoElKip8fn58SPW0k2LwugeWD88uEfDnZqIE4P/73nfhF3bb7utWhbUGoGbTxrK3XYOmtBR4yJXJsrt+6qYvWQLy7fsY1nIrmD3Q7+yxscD79aGKH4ZIZvlKaO788FvJgMwfWRXPFKbnOz0Bz9hxguLA4aySN1BTWJC3/aM6tmWnCwezTcHHXooUbNlTyV77YdrtGX2onGxuFm0cTfDu7dpsJ/fbwI7jp1C7HfM/pqF63fz6E8OZt5aq60uYxVpk9gds7/mN8cN4roXFwfNLtbNnBY4dhuB8b3bB93/zMWHRHyv7m2LWXzzFFoW5vHa4i34TXB8+5XPLgTUCDSXVC4alMroTECJig9X7eDhD2qrKpXEyHXxr4/XBZ2fdF90i7SRcgI98O5qPl6zk189vZBnF1hZJZ2Mp6FEqoXgjOrnrdkZds1hv+0O+/yGY+ncupDLXDtQB3Sue50kkBdfrLxFM1+rTXfsrCsU5evPsTmUdSimS+vCZIuRdui3TqmTz9fvCoSEnvXQvEBZSSAs4qapzF5quV0aizt1hXubv/OaDnUVIC/I9dC6KI/bTzko0Da6Zxt2HagOzHYi8fjHluvJMS5XHFubdTKadRKP1D70L5rYm3Uzp7HmDyfwyi8PZ0yvhuszKHVTnJ/Lx9ceTZ9AgRh1D0WDuoOUiGzbW8kp93/E0K6tOC5C6FzLKNcEGqIor/GvM/D614Lyv7/1VfjCcEOICF/eNAWAa1+wkrxV+/z8OqRASShOiKhjXBobVeLeQT2mlxWf7vEIw7pF3tGsKPFGZwJKRJywxaWb9nLXm8Hpdd+7+shAVafmUlKQw6MfNq54d1WE7KNrtjd9n8ELPz8UgCXf7mXO19ua/DrR4M6GObSrPviV5KNGQKGyxkdVSLHu+lIh9GpfUue1xlKcn8vTn25ouGMDuAuDO7x39ZFR3euu4dtUBjaybsAZ43rQo11xwx2VRuOE/2qwUHSoO0hh0A2zgeAomNDRdp/SEtbEqIqWm8JGpnL212GcVmwNXle467QRjTJWkweW8s7yhrPUdm5VyMT+HYLaPvjN5Ebnnd+5P7Z5kpRaHjxnLM99tsG1NqDUh84ElIi4jcDlR/fn7auO5KzxPXnygvHNet1xYXn9G7covOCbXRHb750TnLlzSEglqYZ49Cfj+Pjaoxrsd6DKWxvpY9O9bXFQDptoqKvimdJ8erYv5qop2ZcNtKmoEVACOHnv/z3vm6CCKZ3tsLvbTj6Iw0NGwdHw5hVH8PZVkwD4xeR+QdecGP9oefnLbyO2f7u7gh+O7R44H9ylcUYAoEvronqvG2PYX+2lRR1hp9Fw56nDgdhFVylKc1EjoAS4Y/bXfHegmt++uIQ7Zltx7H1LSzh5VMR6P1HTv1NL+tibxkKLpoMVmul2Rbk5UOUNZN+s8fmZtWgzhXkeSluG5+e5aGKfZskJ8HQdG77AWtQ1Jvrd0pE4dUx3rp82mFunD23yayhKLFEjoAQY3bMto28NTuf82E/GxSwSCGqNwKiebQJtk0OKprtTLg+96XWG3Pg6Pr/hmucXsau8hr+eMZon7Fq77nKB/Ru5OBuJQ/q0r9MgObuFWxRElzcpEiLChRP70KY482vXKumBzkmVAO1Kgh9uJ4/qFvMIFscX3qV1IV/YbUeGGIG9Fd4wH/vNLy/lxS8sV9CkAaXk51oVoiqqfbyxdCs3nTQEgDeuOCJmexhCcdJoN2cmoCiphn6blQChuVcaG/ESDc56qHvT1FB7Efewfu35cNVOVm3bH1iHcPjfF7VrAe4MoEX5Obx+xRGB82hTXDcFZybQUv35Sgah7iAlwPX/WxJ0Hk3lsMbihHi6jYDHfp8LD7d8+mc/PI/PQqKAyu3Naz+d1Hy/fzQ4hsa99uBkJtVFXSWTiGVlsUdEZJuILHG1nSYiS0XELyJjQ/pfKyKrRGS5iEyNlRxK7IiHEehTasVuH9Knfdg1d1jqDx74KKgWgbN5rV8js5I2FacYzfZ9VYE1ivvftcJQW6gRUDKIWM4EHgOOC2lbApwCzHU3isgQ4HRgqH3P/SKieXQTzNa9lYGwUId1M6cxrsyK5f+HK91xrBjevQ0fzTiKM8aF1xnIzw02Om8sq00E58gUWgwmXtz9w5GU2Omyr35+EQAf2WGziZJBURJBLMtLzhWRspC2ryBikq3pwNPGmCpgrYisAsZh1ShWEsT4P8wJOn/wnDEALPp2N1Drv481XdtEjsfv2LJ2HWBkjzZB3xsnrUU0hcNjQX6uhwP2TCQ0y2meVrBSMohkzWu7AZ+4zjfabUqCWLhhd9B52+K8QKHtyhrLLbP45ilxleH33x9G51a1D353Js2TR3XjD7O+Cpw7VbsSZQTcbN5dwVF/ejdw3r2t5vxRModkzWsjDaUijjtF5GIRWSAiC7Zvbzi3ixIdv3r6i6Dzy47uH9YnND1CrDn7kF4cU0cheL8xnDOhV1h7MkbhJ4/uxpodtXmT4rFWoijJIllGYCPgdgp3BzZF6miMedAYM9YYM7a0tDQhwmUDoa6enxzWO3B8y/ShHDO4I8nEb6CVHY/foUVtqGqkimDxYoideuLJT9Yn7D0VJdEkywi8DJwuIgUi0hvoD8xPkixZyc79VYFjJ6+Pw7kTynjovIMTLVIQfr8JbCxzVwfLTaARcOoMuLnSVUlMUTKBWIaIPoW1sDtQRDaKyAUicrKIbAQmALNE5HUAY8xS4FlgGTAbuNQY46vrtZXY86ODewJW3p4+CQq7jIY//sBKsOY3JhAW6nYBrdsZ+3TWdREpXUanVuE5ixQlnYlldNAZdVx6sY7+twG3xer9lcbh1Oj9z0V1J0xLBieN7Mo1/13E3sqawJpE0A7hGOYxipZBnVvytR0hVNTIlNGKkupowHOWUuX107ooL6bJ4WKBExX6t3dWu2YCtV/Trm0KI90WN7665The/sXhPG4nrJs0QNellMxChzVZxpY9lfzz/TU8NT81FztzXHsDvHYuI2cmMHlgKWN6tYt4X7wosjeMTRpQWmd2UUVJZ9QIZBmn3P8hm/ZUJluMOnHnFPL6/UFt4yOkmlAUpXmoEcgy2hTns6u8hkU3T6HaG17gJdl4XDH489Z+R65HAsnk7n9nFZdM6pss0RQlI9E1gSyjTXEeQ7u2Ii/Hk7LZMJ3NWPPXfhe0MWtvpbeuWxRFaSJqBLIMr9+k/I7X1X84gdMPtvYSVnn9jOjRBoDh3VvXc5eiKE1BjUCW4fObpOTfaSynja3dUH7XadbegT4dSpIljqJkLKnpD1DiRjrMBMDaxObQr2NLHvnxWMb31oVhRYk1agSyDK/PT24aGIHQ9ONHDYqcaE5RlOaR+n4BJab4/IZczYevKIqNzgSyDK/fkOtJD9v/2uUT2b6vquGOiqI0GTUCWUSNz8+qbfvZvLsi2aJExeAurRjcJdlSKEpmkx5DQiUm7NxfDRAom6goiqJGIItYuGFXskVQFCXFUCOQRdz08tJki6AoSoqhRiCLmGoXklcURXFQI5BFtLBzBZ0yuluSJVEUJVWIZXnJR0Rkm4gscbW1E5E3RWSl/X9b17VrRWSViCwXkamxkkOpG6e2vFPCUVEUJZYzgceA40LaZgBzjDH9gTn2OSIyBDgdGGrfc7+IpFaJqwzEbwwFuZ6EFmtXFCW1idnTwBgzF/gupHk68Lh9/DjwfVf708aYKmPMWmAVMC5WsmQ6yzbtZW9lTaPvMya4aIuiKEq8h4SdjDGbAez/O9rt3YANrn4b7bYwRORiEVkgIgu2b98eV2HThRPufZ+zH5oX1r63sobldkH0SPj9hjRIG6QoSgJJll8g0qPIRGjDGPOgMWasMWZsaakW+XZYtHFPoBC7w5n//ISp98yt8x6/zgQURQkh3kZgq4h0AbD/32a3bwR6uPp1BzbFWZaMwJjaB3/f614Ncgst+XYvAJU1kXcE+41BbYCiKG7ibQReBs6zj88DXnK1ny4iBSLSG+gPzI+zLBlByOCfDd+Vh/Wpa73AGBNUw1dRFCVmCeRE5CngSKCDiGwEbgJmAs+KyAXAeuA0AGPMUhF5FlgGeIFLjTGa0CYK/CbYCkgEz9qKLfspyc8NqyGs7iBFUUKJmREwxpxRx6Wj6+h/G3BbrN4/WwixAdz08hKeu+TQIBfQ2Q/PY1Dnlsz+1RFBff1GF4YVRQlGU0mnGaEzgU/X7WLDd+UU5AV79r6OECXkN+EVuxRFyW5011CaEcnfP/GP77C3ouF9A0ZnAoqihKBGIA0wxrBtbyXGGP7x3pqIffaEGIHubYvC+ljuILUCiqLUou6gNOC+t1dx15srKGtfzLqd4dFAAL/8zxcA/GhsD55ZsCGQLM6N30TeoKEoSvaiM4E04K43VwCwbmc5E/q0j9hnh1017KIj+jB9ZFfKI1QPq/L6KcjTFE2KotSiM4E049oTBnHSfR8Gzpf//jh8fsPaHQd4f+UO+nQoIdfjCdpNbIxh4Ybd/N+Xm+hbWpIMsRVFSVHUCKQ4obt/+3dsySmju/HC599yzOCOFORaI/uhXVsztGtrAHI9EjAC1V4/A65/LXD/6u0HEiS5oijpgLqDUhi/3zD8d28Ezq86dgBF+TmcOro7AJMGdox4X06O4LWNwJY9lfEXVFGUtEVnAilMeY2Paq8fgL+dOZppw7sAcGi/Drx2+UQGdW4Z8T5rJmDdt22fGgFFUepGjUAKc/VzXwaOvfZD3WFwl1Z13pfjqZ0J7CoPDh3t17FFDCVUFCXdUXdQCvPaki2B4+L86O21e01gV3l10LVnfzohNsIpipIRqBFIMHvKayJm/myIY4d0irrv60u3Ul7tY+2OA6zcWps+4pVfHk67kvxGv7eiKJmLuoMSzNR75rJlbyXrZk6rt9/O/VWB45d/cVij3mO9bWQm/+ldAI4a1JGHzxureYMURQlDZwIJZsvehhdq/X7Dr55ZGDgf3r1No95jaNfa9YLzJvTiH+eMUQOgKEpE1AgkCROaExrYtLuCshmzOPXvH/H+yh2M6tmGt648IsLd9fPnH40MHP9u+jDycvTPrChKZPTpkCQeeG81P3vys0AI6LJNezl05tsAfL5+N9MO6sILPzuUfh0jh4HWh/r9FUWJFl0TSBJ/nL0cgJb/W8wfTx3BCfe+H3T99h8c1GQXTttiywgc2jdyniFFURSHhBgBEbkcuAgrieU/jTH3iEg74BmgDFgH/NAYsysR8qQSAzq15P2V24Pa7jx1OK0K85r8mjkeaXDhWVEUBRLgDhKRYVgGYBwwAjhRRPoDM4A5xpj+wBz7PKNZHqHaV7+OLfjV0wsD5x/OOIrTxvZIoFSKomQziVgTGAx8YowpN8Z4gfeAk4HpwON2n8eB7ydAlqQSKTLo4zU72XmgdkNXtzbhxWAURVHiRSKMwBLgCBFpLyLFwAlAD6CTMWYzgP1/xGxoInKxiCwQkQXbt2+P1CVtcEpAjitrF2j7x3trGNbNCul8+uJDkiKXoijZS9yNgDHmK+AO4E1gNvAl4G3E/Q8aY8YaY8aWlpbGScr4sHN/FXsra1ixdR9VXh/b9lkbwP565ihumT4UgJNGdKVTy0L6d2zBIXUUjFEURYkXCVkYNsY8DDwMICJ/ADYCW0WkizFms4h0AbYlQpZ4sHFXOf/74lt+fmQ/PK5K7mN+/1bg+Idju9OjbTFghXCeNqYHN760lB7tivjbO6sTLrOiKAokaJ+AiHS0/+8JnAI8BbwMnGd3OQ94KRGyxINrX1jMn95Ywcpt+wNtoYXfn12wkf1V1gQo1yPk51ofvRoARVGSSaL2CfxXRNoDNcClxphdIjITeFZELgDWA6clSJaY42TqrHBVARvhKgYDcMHhvfnH3DUAiAg5AqeM7kbrojwe/XBdwmRVFEVxkyh30MQIbTuBoxPx/vGmOM/6GF/5chMje7SJ2GdvyMwA4O4fjgTgpu8NjZdoiqIo9aJpI2JAj3aWr/+hD9aGXRthG4XnPtsIwG9PGJwwuRRFURpCjUAMWLV9f1jbYf3aM7BTS166tDYN9J2nDueiI/okUjRFUZR6USPQTHx+w5cbdgPQssByC1V5fXyxfneYa0h3AiuKkmpoArlmcsfsrwPH+6q8XPnsQs4/rDfl1T7atbASuT3644Pp1b44WSIqiqLUic4EmoknJNPnC59/y7e7KwDoW2oVdZ88qCN9SrXAu6IoqYcagWYyxFXFy6EwLweA3h109K8oSmqjRqCZ1NhFYdz4/VbVsNBZgqIoSqqhRqCZeP3hRmCj7Q4qztclF0VRUhs1As2kOsJM4MmPvwGgRaEaAUVRUhs1As1kf5WVKuLa4wfRpjiPPqUlLN9qFY9poTMBRVFSHDUCzaDG5w+EiJ53aBkLb5zCJUf0DVwvKchJlmiKoihRoUagCcxesoUz//kJd7+5ItCWn2N9lCeO6EKLglwK8zzk5ujHqyhKaqP+iibwxMfr+Gj1Tj5avTPQ5tQRKM7P5fSDe/D+yh3JEk9RFCVq1Ag0gc6tCynM83B4vw689dU2Jg0Irnh27QmDucYOE1UURUll1Ag0Ep/fUFnjo0fbYh44ewyPfLCWHx9WFtQnxyPkeHSPgKIoqY86rUNYtW0/4257i/U7y8OuXfzEAvpe9yqvLt7C3soa8nI8/HRSXwpydQFYUZT0JFHlJa8QkaUiskREnhKRQhFpJyJvishK+/+2iZClPnx+w/T7PmDbvirmrtzO8i37Atc+XLWDN5ZtDZxv3VuVDBEVRVFiStyNgIh0Ay4DxhpjhgE5wOnADGCOMaY/MMc+Tyr3vb2KA9VW3P9T89cz9Z65zFtjLf6e9dC8ZIqmKIoSFxLlDsoFikQkFygGNgHTgcft648D30+QLEFUVPuo8vqYv/Y7/jJnBZMHWou8SzftBeBHD35CebU30L+DnR768fPHJV5YRVGUGBP3hWFjzLci8iesYvIVwBvGmDdEpJMxZrPdZ7OIdIx0v4hcDFwM0LNnz5jKVl7tZciNrwfOO7TI557TR4UVif+XnQYC4PzDe/PzI/vFVA5FUZRkkQh3UFusUX9voCtQIiJnR3u/MeZBY8xYY8zY0tLShm9oBG4DAHDZ0f1pXZQX1u/216xdwccP68zFE7U8pKIomUMi3EHHAGuNMduNMTXAC8ChwFYR6QJg/78tnkKs3LqPPeU1gfMpf34vrM+UIZ3D2k4Z1Y0+pSUAnDOhl+4CVhQlo0jEE209cIiIFIuIAEcDXwEvA+fZfc4DXoqnEMf+eS5T7ql98K/YGl4cvlOrAgD+dNqIQNsdpw7nmYsncOWxAxjdM+kBTIqiKDElEWsC80TkeeBzwAt8ATwItACeFZELsAzFafGSYX+VtbDrDussysuhosYX1E/sIjDTDurC+yu3c90Jg8nL8VDasoDLju4fL/EURVGSRkJ2DBtjbgJuCmmuwpoVxJ0teyoCxze/vJQbTxxCy8JcivNzeODsMWzeUxGIBgIoys/hL6ePSoRoiqIoSSUr0kb06VBb5P2xj9ZxaN/2bNtnzQrG9W4HwPSR3ZIim6IoSjLJilVOT0gen4v/9RkAfzl9ZBKkURRFSR2yYiYQiTlXTaJvaYuGOyqKomQwWTETCGVEjzZqABRFUcgiI3DSiK6B4zXbwsNDFUVRspGsMQIXuXb6Xqi7fhVFUYAsMgLDurVivB0JdPxB4TuDFUVRspGsWRgWEZ756YRki6EoipJSZM1MQFEURQlHjYCiKEoWo0ZAURQli1EjoCiKksWoEVAURcli1AgoiqJkMWoEFEVRshgxxiRbhqgRke3ANyHNHYAdSRAnHmSSLpBZ+qguqUUm6OCQCF16GWMiFmlPKyMQCRFZYIwZm2w5YkEm6QKZpY/qklpkgg4OydZF3UGKoihZjBoBRVGULCYTjMCDyRYghmSSLpBZ+qguqUUm6OCQVF3Sfk1AURRFaTqZMBNQFEVRmogaAUVRlCxGjYDSLEREki2DoihNJy2MgIgcKSIRNzqkGyJylYhMsY8z4QHa0jlId33SXX6HDNKjnes4rXVK5WdYShsBETlOROYCZwFVyZanOYjIFBF5HfgNcC6ASeNVeRE5VkQ+AP4kItdA+uojItNF5HFgRLJlaQ4ZpIfzu79HRO6CtP5upfwzLOXKS9oWX4AfAf8ALjDGPJdcqZqGrUsecCMwCbgdyAcOFpE8wJuOX24R6Q7cDMwE3gWeFpH2xpjfiIikk04iMhm4FagBJojIN8aYXUkWK2qczzsT9MAalF4AnI/1W/kCeEJEjjfGvJZM+RpDuj3DUmom4HyhjTF+YBPwBLDKvnaaiHS3H54pPz106VINvGSMmWiMeRXYBZxujKlJs4el+/MeBCw2xvyfMWYf8DfgChHpbz+QUvpvE8JaYApwNTAeGJ5ccaInxOCuBaaSxnoYY3zAB8DhxpiXgEpgG7BURDxO3ySK2iDp+AxLGSMgIr8AXhCRK0WkA9aXYRHwgIh8DfwQ+Ctwv3NLciRtGJcuV4hIF2PMp3Z7njHmPWCNiByfXCmjJ0SfVsAK4HARmWB36QgsBa5PlozRIiI/F5Ef2McCbDDGbDHGvA1sBSaJSLekChkFIX+TzsaYdcaYzWmuRxdjzDJjjFdERgP/A8qwXKh3O7ckR9KGSdtnmDEm6f+Ak4FPgcnAo1gjy4FAV6xp4Si7XztgOzAm2TI3Qpf7gBH2NbF1eAiYkmxZm6jPA0AnrGn7Y8CHwH+A3sCXQFmyZa5Dj5bA34EtwH4g1273ULtpcjjwJHBKyL2SbPmj+I6NdF1Pez3s71NP+7gE2A2MTbbMjdAlbZ5hqTITGA88YIx5B8vXvA642hizCfidMeYLAGPMd1ijgxbJETMqQnVZC1wO1uKWrUMR1pcFZ5qbwkTS53fGmIeBi4ArjDFnAuuB+cDeZAlaH8ZyW71njOkMvIL1IwXrwWjsPouwfsjDROQoEfmN3Z5qbrtIf5PLnItprofzW1lrjFlvHx8AngVaJUnOaEjbZ1hCH0ChPjDX+RrgTABjzDfA/wEtReQkY0ylq/8NwFDg68RIXDeN0GUWUCIiJ7m6PwmME5FCY/kOk04j9HkZaCsiJxtrXWO+3e9WrBHbvgSJXCf16PKy/f+vgDPsNQyfiOS6+jwFXAg8g5XnPWV8t438jqWrHsUheiAi12P97pclUs5oSOdnmEOiR6F57hPXyOR5oFxEptvnm7GiToYAiMhEEXkHGAD8wBizNTHi1kujdXF9YYqApwFfAuSMlsbqMxBARPqLyEvAMKxZQU1ixK2XiLoYYw6IiMcYswXLL/uQ3e41xhgRKQHuBRYDw40xV7vvTwYikuMcR/sdE5EWwF9IYz3se44XKwx5AHCq/XdLOk3UJRWfYUCCjICITBCR54A7RWSI8yGKiBOiugt4EfiZvbq+B2u6VGRfXwdcaow5xxizOREy10UzdClwfWFeMsb8MxUemM3Qp9C+vgXrb3NSsr/Y9eiSE+p2M8bMAHrb93QSkYNtt8Nlxphpyfye2TLdYsvpc7U7g4g6/yb2d6wSuDyN9XB+918Blxhjzk2R331zdFlHijzDQom7ERCRjlgLPq8CO7F8fueDNQKzuxUBr2NZzwdFpCswCqi2+20wxiR9KthMXZzrQV+iZNJMfWrsfvuMMRsTLHoYDejiM8b47RFya9dtd2AtbL8PFNt9tyVS7lBE5DzgceB6Efmh3ZZry+YMIhr6m3jTXA/nd7/OGLMk0bKHEiNdUuIZFhET/1XzY4GnTO0q/1SshblBdtvvsT68UVgr57/HmkbdD+TEW75s1SXT9IlCl1uB2cBE+/x4LL/sn4C8ZMvv0uMYoDvW3oX1rvYc+/+b0+Fvkil6ZJouEfWLwwf2feA6YJp9XgqsBPra5+2Am7BGYcVY4YV9Q16jONkfTKbpkmn6NFcXLF9tjxTS40T7PAfbKGHFmd/q6tsxVf8mmaJHpukSlb4x/OBKsUKf5gKXYO30O9W+NhO4xz72AIcD/wTaue73JPvDyERdMk2fGOiSEiOzOvQ42b6Wb/8/FNgDdIpwf0r8TTJFj0zTpVF6x/ADPAQrLtY5Pwf4yD4egTUVP8Y+H4wVrleSih9eJumSafpkii716WGfO66Gh4BH7ePjky13puqRabo05l+zFoZF5FyxUqQWA59h5clwQqiWYaUSACtU7WmsrID9gKOxds/mAZgUiJXPJF0gs/TJFF2i0GOxfS6AATDGXAicJyK7gBGhUU7JIFP0gMzSpak0Oouo/WF0xvKD+YHVWDtHLzfGbBWRHGNtwBmMHYlh//gesyM4ZmAlILvIGLM7Nmo0jUzSBTJLn0zRpZF6tIVAxIkRkV7An7Gily41SYyUyRQ9ILN0iQmNnC4506EBwJP2cS5WUqQXQvo8AfzQPu7seo38ZE9/Mk2XTNMnU3Rphh6l9v9tgHGqh+oSz39RzQTsmNhbgBwReRUrh4cPrJhkEbkM2CQik4yVJROsJF1r7Q0Wp4jIccaYjcZKrZw0MkkXyCx9MkWXGOlxgrFy58yP8BYJIVP0gMzSJdY06MsSkUlYvrK2WHmxncIVk0VkHASmSrdgxcs6/rTzsbZStwImm9TYUJQxukBm6ZMpusRQj/UJF95FpugBmaVLXIhi+jQROMd1fj/wM+DHwGd2mwfLx/Ys0AvoC9wDjE72VCdTdck0fTJFF9UjtfTINF3i8vlE8QEWAwXU+snOAm63jxcCv7SPxwJPJ1uhbNEl0/TJFF1Uj9T7l0m6xONfg+4gY0y5MabK1Oa7ORarKALAT4DBIvIKVurazyB1UtaGkkm6QGbpkym6qB6pRybpEg+iDhG1fWQGq6qUk5d9H9b26mHAWmPMt5CSxSuCyCRdILP0yRRdVI/UI5N0iSWN2eTgx9p0swMYblvOGwC/MeYD58NLEzJJF8gsfTJFF9Uj9cgkXWJHY3xHWNuq/VhJlC5Iti+rOf8ySZdM0ydTdFE9Uu9fJukSq39Oge2oEJHuWPk07jbGVDXN7KQGmaQLZJY+maKL6pF6ZJIusaJRRkBRFEXJLNI68ZGiKIrSPNQIKIqiZDFqBBRFUbIYNQKKoihZjBoBRVGULEaNgJJ1iIhPRBaKyFIR+VJErpQGqkOJSJmInBnFawf1E5GxInJvLORWlHigRkDJRiqMMSONMUOx8sicANzUwD1lQINGILSfMWaBMeayJsqpKHFH9wkoWYeI7DfGtHCd9wE+BTpgpRH+F1BiX/6FMeYjEfkEq3D9WuBx4F5gJnAkVobKvxlj/hGh3xfAr40xJ4rIzUBvoAtWZasrsXawHg98C3zPGFMjImOAu4EWWCkOfmyM2Rynj0PJcnQmoGQ9xpg1WL+FjsA24FhjzGjgR1gPe7BqFr9vzyD+DFwA7DHGHAwcDFwkIr0j9AulLzANmA48CbxjjDkIqACmiUgeVqnDU40xY4BHgNviorii0IRC84qSoTipg/OA+0RkJFb5wQF19J+ClYTsVPu8NdAfaKis5Wv2aH8xkAPMttsXY7mSBmJltHzTzmacA+gsQIkbagSUrMd2B/mwZgE3AVuBEVizg8q6bsMqRvJ6yGsd2cDbVQEYY/wiUmNq/bF+rN+jAEuNMRMar4miNB51BylZjYiUAn8H7rMfyK2BzcYYP1aisRy76z6gpevW14Gf2e4bRGSAiJRE6NdYlgOlIjLBft08ERnajNdTlHrRmYCSjRSJyEIs148XayH4bvva/cB/ReQ04B3ggN2+CPCKyJfAY8BfsNw3n9tVqLYD34/Q74vGCGaMqbZdTPeKSGus3+g9wNLGq6koDaPRQYqiKFmMuoMURVGyGDUCiqIoWYwaAUVRlCxGjYCiKEoWo0ZAURQli1EjoCiKksWoEVAURcli/h9bybnBz+AzkwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['close'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "                              open     high      low    close  Adj Close  \\\nDatetime                                                                   \n2020-05-13 04:00:00-04:00  77.7525  77.9750  77.6875  77.9750    77.9750   \n2020-05-13 05:00:00-04:00  77.9750  78.2475  77.9000  77.9775    77.9775   \n2020-05-13 06:00:00-04:00  77.9975  78.2700  77.9975  78.2450    78.2450   \n2020-05-13 07:00:00-04:00  78.2050  78.2975  78.0175  78.1125    78.1125   \n2020-05-13 08:00:00-04:00  78.1175  78.5000  78.0050  78.3225    78.3225   \n\n                           volume  close_pct  \nDatetime                                      \n2020-05-13 04:00:00-04:00       0        NaN  \n2020-05-13 05:00:00-04:00       0   0.000032  \n2020-05-13 06:00:00-04:00       0   0.003430  \n2020-05-13 07:00:00-04:00       0  -0.001693  \n2020-05-13 08:00:00-04:00       0   0.002688  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.7525</td>\n      <td>77.9750</td>\n      <td>77.6875</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.9750</td>\n      <td>78.2475</td>\n      <td>77.9000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n      <td>0.000032</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.9975</td>\n      <td>78.2700</td>\n      <td>77.9975</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n      <td>0.003430</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.2050</td>\n      <td>78.2975</td>\n      <td>78.0175</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n      <td>-0.001693</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.1175</td>\n      <td>78.5000</td>\n      <td>78.0050</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n      <td>0.002688</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close_pct'] = data['close'].pct_change()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "              open         high          low        close    Adj Close  \\\ncount  4187.000000  4187.000000  4187.000000  4187.000000  4187.000000   \nmean    115.495982   116.040516   114.850457   115.493613   115.493613   \nstd      16.805333    17.645432    16.756622    16.792796    16.792796   \nmin      75.087502    75.537500    58.360000    75.300000    75.300000   \n25%     109.368750   109.762300   108.672499   109.335575   109.335575   \n50%     119.650902   120.100000   118.812500   119.639900   119.639900   \n75%     128.270051   128.702501   127.591200   128.269249   128.269249   \nmax     144.900000   438.440000   144.590000   144.910000   144.910000   \n\n             volume    close_pct  \ncount  4.187000e+03  4186.000000  \nmean   5.080393e+06     0.000123  \nstd    8.586368e+06     0.005409  \nmin    0.000000e+00    -0.051319  \n25%    0.000000e+00    -0.001888  \n50%    0.000000e+00     0.000090  \n75%    8.373736e+06     0.002269  \nmax    9.845401e+07     0.051457  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4187.000000</td>\n      <td>4187.000000</td>\n      <td>4187.000000</td>\n      <td>4187.000000</td>\n      <td>4187.000000</td>\n      <td>4.187000e+03</td>\n      <td>4186.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>115.495982</td>\n      <td>116.040516</td>\n      <td>114.850457</td>\n      <td>115.493613</td>\n      <td>115.493613</td>\n      <td>5.080393e+06</td>\n      <td>0.000123</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>16.805333</td>\n      <td>17.645432</td>\n      <td>16.756622</td>\n      <td>16.792796</td>\n      <td>16.792796</td>\n      <td>8.586368e+06</td>\n      <td>0.005409</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>75.087502</td>\n      <td>75.537500</td>\n      <td>58.360000</td>\n      <td>75.300000</td>\n      <td>75.300000</td>\n      <td>0.000000e+00</td>\n      <td>-0.051319</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>109.368750</td>\n      <td>109.762300</td>\n      <td>108.672499</td>\n      <td>109.335575</td>\n      <td>109.335575</td>\n      <td>0.000000e+00</td>\n      <td>-0.001888</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>119.650902</td>\n      <td>120.100000</td>\n      <td>118.812500</td>\n      <td>119.639900</td>\n      <td>119.639900</td>\n      <td>0.000000e+00</td>\n      <td>0.000090</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>128.270051</td>\n      <td>128.702501</td>\n      <td>127.591200</td>\n      <td>128.269249</td>\n      <td>128.269249</td>\n      <td>8.373736e+06</td>\n      <td>0.002269</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>144.900000</td>\n      <td>438.440000</td>\n      <td>144.590000</td>\n      <td>144.910000</td>\n      <td>144.910000</td>\n      <td>9.845401e+07</td>\n      <td>0.051457</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data = add_all_ta_features(\n",
    "        data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    # data['normVol'] = data['volume'] / data['volume'].ewm(5).mean()\n",
    "    # for i in range(1,50):\n",
    "    #     data[f'close{i}'] = data['close'].shift(i)\n",
    "    # Remove columns that won't be used as features\n",
    "    # del (data['Adj Close'])\n",
    "\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low     close  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.9750   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.9775   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.2450   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.1125   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.3225   \n...                               ...         ...         ...       ...   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.8200   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.4000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.4600   \n2021-05-12 18:00:00-04:00  122.460000  122.950000  122.350000  122.7500   \n2021-05-12 19:00:00-04:00  122.750000  123.000000  122.640100  122.7900   \n\n                           Adj Close    volume  close_pct  \nDatetime                                                   \n2020-05-13 04:00:00-04:00    77.9750         0        NaN  \n2020-05-13 05:00:00-04:00    77.9775         0   0.000032  \n2020-05-13 06:00:00-04:00    78.2450         0   0.003430  \n2020-05-13 07:00:00-04:00    78.1125         0  -0.001693  \n2020-05-13 08:00:00-04:00    78.3225         0   0.002688  \n...                              ...       ...        ...  \n2021-05-12 15:30:00-04:00   122.8200  14469408   0.002858  \n2021-05-12 16:00:00-04:00   122.4000         0  -0.003420  \n2021-05-12 17:00:00-04:00   122.4600         0   0.000490  \n2021-05-12 18:00:00-04:00   122.7500         0   0.002368  \n2021-05-12 19:00:00-04:00   122.7900         0   0.000326  \n\n[4187 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n      <td>0.000032</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n      <td>0.003430</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n      <td>-0.001693</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n      <td>0.002688</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.8200</td>\n      <td>122.8200</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.4000</td>\n      <td>122.4000</td>\n      <td>0</td>\n      <td>-0.003420</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.4600</td>\n      <td>122.4600</td>\n      <td>0</td>\n      <td>0.000490</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.950000</td>\n      <td>122.350000</td>\n      <td>122.7500</td>\n      <td>122.7500</td>\n      <td>0</td>\n      <td>0.002368</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 19:00:00-04:00</th>\n      <td>122.750000</td>\n      <td>123.000000</td>\n      <td>122.640100</td>\n      <td>122.7900</td>\n      <td>122.7900</td>\n      <td>0</td>\n      <td>0.000326</td>\n    </tr>\n  </tbody>\n</table>\n<p>4187 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['open', 'high', 'low', 'close', 'Adj Close', 'volume', 'close_pct'], dtype='object')"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def create_class_column(row, lowest_threshold, higher_threshold):\n",
    "    if row['close_shift'] - row['close'] > higher_threshold:\n",
    "        return 1\n",
    "    if row['close_shift'] - row['close'] < lowest_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low     close  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.9750   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.9775   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.2450   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.1125   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.3225   \n...                               ...         ...         ...       ...   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.8200   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.4000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.4600   \n2021-05-12 18:00:00-04:00  122.460000  122.950000  122.350000  122.7500   \n2021-05-12 19:00:00-04:00  122.750000  123.000000  122.640100  122.7900   \n\n                           Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00    77.9750         0        NaN  0.000000e+00   \n2020-05-13 05:00:00-04:00    77.9775         0   0.000032  0.000000e+00   \n2020-05-13 06:00:00-04:00    78.2450         0   0.003430  0.000000e+00   \n2020-05-13 07:00:00-04:00    78.1125         0  -0.001693  0.000000e+00   \n2020-05-13 08:00:00-04:00    78.3225         0   0.002688  0.000000e+00   \n...                              ...       ...        ...           ...   \n2021-05-12 15:30:00-04:00   122.8200  14469408   0.002858  6.359432e+08   \n2021-05-12 16:00:00-04:00   122.4000         0  -0.003420  6.359432e+08   \n2021-05-12 17:00:00-04:00   122.4600         0   0.000490  6.359432e+08   \n2021-05-12 18:00:00-04:00   122.7500         0   0.002368  6.359432e+08   \n2021-05-12 19:00:00-04:00   122.7900         0   0.000326  6.359432e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_ao  \\\nDatetime                                           ...                \n2020-05-13 04:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 05:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 06:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 07:00:00-04:00           0    0.000000  ...     0.000000   \n2020-05-13 08:00:00-04:00           0    0.000000  ...     0.000000   \n...                               ...         ...  ...          ...   \n2021-05-12 15:30:00-04:00  -499096534   -0.139414  ...    -1.694206   \n2021-05-12 16:00:00-04:00  -499096534   -0.227993  ...    -1.715150   \n2021-05-12 17:00:00-04:00  -499096534   -0.272966  ...    -2.377450   \n2021-05-12 18:00:00-04:00  -499096534   -0.337262  ...    -2.270714   \n2021-05-12 19:00:00-04:00  -499096534   -0.337262  ...    -2.123853   \n\n                           momentum_kama  momentum_roc  momentum_ppo  \\\nDatetime                                                               \n2020-05-13 04:00:00-04:00      77.975000      0.000000      0.000000   \n2020-05-13 05:00:00-04:00      77.976113      0.000000      0.000000   \n2020-05-13 06:00:00-04:00      78.093408      0.000000      0.000000   \n2020-05-13 07:00:00-04:00      78.101618      0.000000      0.000000   \n2020-05-13 08:00:00-04:00      78.194854      0.000000      0.000000   \n...                                  ...           ...           ...   \n2021-05-12 15:30:00-04:00     122.794127     -1.947949     23.803451   \n2021-05-12 16:00:00-04:00     122.682449     -2.384560     13.137307   \n2021-05-12 17:00:00-04:00     122.624161     -2.196310      3.390093   \n2021-05-12 18:00:00-04:00     122.638585     -1.870653     -5.517361   \n2021-05-12 19:00:00-04:00     122.649011     -1.799424    -13.657404   \n\n                           momentum_ppo_signal  momentum_ppo_hist  others_dr  \\\nDatetime                                                                       \n2020-05-13 04:00:00-04:00             0.000000           0.000000 -32.485444   \n2020-05-13 05:00:00-04:00             0.000000           0.000000   0.003206   \n2020-05-13 06:00:00-04:00             0.000000           0.000000   0.343048   \n2020-05-13 07:00:00-04:00             0.000000           0.000000  -0.169340   \n2020-05-13 08:00:00-04:00             0.000000           0.000000   0.268843   \n...                                        ...                ...        ...   \n2021-05-12 15:30:00-04:00             9.532677          14.270774   0.285783   \n2021-05-12 16:00:00-04:00            10.253603           2.883705  -0.341964   \n2021-05-12 17:00:00-04:00             8.880901          -5.490808   0.049020   \n2021-05-12 18:00:00-04:00             6.001248         -11.518609   0.236812   \n2021-05-12 19:00:00-04:00             2.069518         -15.726922   0.032587   \n\n                           others_dlr  others_cr  close_shift  \nDatetime                                                       \n2020-05-13 04:00:00-04:00    0.000000   0.000000    76.611198  \n2020-05-13 05:00:00-04:00    0.003206   0.003206    76.374977  \n2020-05-13 06:00:00-04:00    0.342461   0.346265    76.417747  \n2020-05-13 07:00:00-04:00   -0.169483   0.176339    76.184998  \n2020-05-13 08:00:00-04:00    0.268482   0.445656    76.919998  \n...                               ...        ...          ...  \n2021-05-12 15:30:00-04:00    0.285375  57.512023          NaN  \n2021-05-12 16:00:00-04:00   -0.342550  56.973389          NaN  \n2021-05-12 17:00:00-04:00    0.049008  57.050337          NaN  \n2021-05-12 18:00:00-04:00    0.236532  57.422251          NaN  \n2021-05-12 19:00:00-04:00    0.032581  57.473549          NaN  \n\n[4187 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_ao</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>77.975000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-32.485444</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>76.611198</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n      <td>0.000032</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>77.976113</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>76.374977</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n      <td>0.003430</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.093408</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.343048</td>\n      <td>0.342461</td>\n      <td>0.346265</td>\n      <td>76.417747</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n      <td>-0.001693</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.101618</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.169340</td>\n      <td>-0.169483</td>\n      <td>0.176339</td>\n      <td>76.184998</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n      <td>0.002688</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>78.194854</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.268843</td>\n      <td>0.268482</td>\n      <td>0.445656</td>\n      <td>76.919998</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.8200</td>\n      <td>122.8200</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.139414</td>\n      <td>...</td>\n      <td>-1.694206</td>\n      <td>122.794127</td>\n      <td>-1.947949</td>\n      <td>23.803451</td>\n      <td>9.532677</td>\n      <td>14.270774</td>\n      <td>0.285783</td>\n      <td>0.285375</td>\n      <td>57.512023</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.4000</td>\n      <td>122.4000</td>\n      <td>0</td>\n      <td>-0.003420</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.227993</td>\n      <td>...</td>\n      <td>-1.715150</td>\n      <td>122.682449</td>\n      <td>-2.384560</td>\n      <td>13.137307</td>\n      <td>10.253603</td>\n      <td>2.883705</td>\n      <td>-0.341964</td>\n      <td>-0.342550</td>\n      <td>56.973389</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.4600</td>\n      <td>122.4600</td>\n      <td>0</td>\n      <td>0.000490</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.272966</td>\n      <td>...</td>\n      <td>-2.377450</td>\n      <td>122.624161</td>\n      <td>-2.196310</td>\n      <td>3.390093</td>\n      <td>8.880901</td>\n      <td>-5.490808</td>\n      <td>0.049020</td>\n      <td>0.049008</td>\n      <td>57.050337</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.950000</td>\n      <td>122.350000</td>\n      <td>122.7500</td>\n      <td>122.7500</td>\n      <td>0</td>\n      <td>0.002368</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.337262</td>\n      <td>...</td>\n      <td>-2.270714</td>\n      <td>122.638585</td>\n      <td>-1.870653</td>\n      <td>-5.517361</td>\n      <td>6.001248</td>\n      <td>-11.518609</td>\n      <td>0.236812</td>\n      <td>0.236532</td>\n      <td>57.422251</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 19:00:00-04:00</th>\n      <td>122.750000</td>\n      <td>123.000000</td>\n      <td>122.640100</td>\n      <td>122.7900</td>\n      <td>122.7900</td>\n      <td>0</td>\n      <td>0.000326</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.337262</td>\n      <td>...</td>\n      <td>-2.123853</td>\n      <td>122.649011</td>\n      <td>-1.799424</td>\n      <td>-13.657404</td>\n      <td>2.069518</td>\n      <td>-15.726922</td>\n      <td>0.032587</td>\n      <td>0.032581</td>\n      <td>57.473549</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4187 rows Ã— 91 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _get_indicator_data(data)\n",
    "data['close_shift'] = data.shift(-WINDOW)['close']\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    1397\n",
      " 0    1395\n",
      "-1    1395\n",
      "Name: class_column, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 open        high         low     close  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00   77.752500   77.975000   77.687500   77.9750   \n2020-05-13 05:00:00-04:00   77.975000   78.247500   77.900000   77.9775   \n2020-05-13 06:00:00-04:00   77.997500   78.270000   77.997500   78.2450   \n2020-05-13 07:00:00-04:00   78.205000   78.297500   78.017500   78.1125   \n2020-05-13 08:00:00-04:00   78.117500   78.500000   78.005000   78.3225   \n...                               ...         ...         ...       ...   \n2021-05-12 15:30:00-04:00  122.474998  123.059998  122.300003  122.8200   \n2021-05-12 16:00:00-04:00  122.820000  123.090000  122.345000  122.4000   \n2021-05-12 17:00:00-04:00  122.360000  124.524100  113.934780  122.4600   \n2021-05-12 18:00:00-04:00  122.460000  122.950000  122.350000  122.7500   \n2021-05-12 19:00:00-04:00  122.750000  123.000000  122.640100  122.7900   \n\n                           Adj Close    volume  close_pct    volume_adi  \\\nDatetime                                                                  \n2020-05-13 04:00:00-04:00    77.9750         0        NaN  0.000000e+00   \n2020-05-13 05:00:00-04:00    77.9775         0   0.000032  0.000000e+00   \n2020-05-13 06:00:00-04:00    78.2450         0   0.003430  0.000000e+00   \n2020-05-13 07:00:00-04:00    78.1125         0  -0.001693  0.000000e+00   \n2020-05-13 08:00:00-04:00    78.3225         0   0.002688  0.000000e+00   \n...                              ...       ...        ...           ...   \n2021-05-12 15:30:00-04:00   122.8200  14469408   0.002858  6.359432e+08   \n2021-05-12 16:00:00-04:00   122.4000         0  -0.003420  6.359432e+08   \n2021-05-12 17:00:00-04:00   122.4600         0   0.000490  6.359432e+08   \n2021-05-12 18:00:00-04:00   122.7500         0   0.002368  6.359432e+08   \n2021-05-12 19:00:00-04:00   122.7900         0   0.000326  6.359432e+08   \n\n                           volume_obv  volume_cmf  ...  momentum_kama  \\\nDatetime                                           ...                  \n2020-05-13 04:00:00-04:00           0    0.000000  ...      77.975000   \n2020-05-13 05:00:00-04:00           0    0.000000  ...      77.976113   \n2020-05-13 06:00:00-04:00           0    0.000000  ...      78.093408   \n2020-05-13 07:00:00-04:00           0    0.000000  ...      78.101618   \n2020-05-13 08:00:00-04:00           0    0.000000  ...      78.194854   \n...                               ...         ...  ...            ...   \n2021-05-12 15:30:00-04:00  -499096534   -0.139414  ...     122.794127   \n2021-05-12 16:00:00-04:00  -499096534   -0.227993  ...     122.682449   \n2021-05-12 17:00:00-04:00  -499096534   -0.272966  ...     122.624161   \n2021-05-12 18:00:00-04:00  -499096534   -0.337262  ...     122.638585   \n2021-05-12 19:00:00-04:00  -499096534   -0.337262  ...     122.649011   \n\n                           momentum_roc  momentum_ppo  momentum_ppo_signal  \\\nDatetime                                                                     \n2020-05-13 04:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 05:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 06:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 07:00:00-04:00      0.000000      0.000000             0.000000   \n2020-05-13 08:00:00-04:00      0.000000      0.000000             0.000000   \n...                                 ...           ...                  ...   \n2021-05-12 15:30:00-04:00     -1.947949     23.803451             9.532677   \n2021-05-12 16:00:00-04:00     -2.384560     13.137307            10.253603   \n2021-05-12 17:00:00-04:00     -2.196310      3.390093             8.880901   \n2021-05-12 18:00:00-04:00     -1.870653     -5.517361             6.001248   \n2021-05-12 19:00:00-04:00     -1.799424    -13.657404             2.069518   \n\n                           momentum_ppo_hist  others_dr  others_dlr  \\\nDatetime                                                              \n2020-05-13 04:00:00-04:00           0.000000 -32.485444    0.000000   \n2020-05-13 05:00:00-04:00           0.000000   0.003206    0.003206   \n2020-05-13 06:00:00-04:00           0.000000   0.343048    0.342461   \n2020-05-13 07:00:00-04:00           0.000000  -0.169340   -0.169483   \n2020-05-13 08:00:00-04:00           0.000000   0.268843    0.268482   \n...                                      ...        ...         ...   \n2021-05-12 15:30:00-04:00          14.270774   0.285783    0.285375   \n2021-05-12 16:00:00-04:00           2.883705  -0.341964   -0.342550   \n2021-05-12 17:00:00-04:00          -5.490808   0.049020    0.049008   \n2021-05-12 18:00:00-04:00         -11.518609   0.236812    0.236532   \n2021-05-12 19:00:00-04:00         -15.726922   0.032587    0.032581   \n\n                           others_cr  close_shift  class_column  \nDatetime                                                         \n2020-05-13 04:00:00-04:00   0.000000    76.611198            -1  \n2020-05-13 05:00:00-04:00   0.003206    76.374977            -1  \n2020-05-13 06:00:00-04:00   0.346265    76.417747            -1  \n2020-05-13 07:00:00-04:00   0.176339    76.184998            -1  \n2020-05-13 08:00:00-04:00   0.445656    76.919998            -1  \n...                              ...          ...           ...  \n2021-05-12 15:30:00-04:00  57.512023          NaN             0  \n2021-05-12 16:00:00-04:00  56.973389          NaN             0  \n2021-05-12 17:00:00-04:00  57.050337          NaN             0  \n2021-05-12 18:00:00-04:00  57.422251          NaN             0  \n2021-05-12 19:00:00-04:00  57.473549          NaN             0  \n\n[4187 rows x 92 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>...</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n      <th>close_shift</th>\n      <th>class_column</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-05-13 04:00:00-04:00</th>\n      <td>77.752500</td>\n      <td>77.975000</td>\n      <td>77.687500</td>\n      <td>77.9750</td>\n      <td>77.9750</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>77.975000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-32.485444</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>76.611198</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 05:00:00-04:00</th>\n      <td>77.975000</td>\n      <td>78.247500</td>\n      <td>77.900000</td>\n      <td>77.9775</td>\n      <td>77.9775</td>\n      <td>0</td>\n      <td>0.000032</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>77.976113</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>0.003206</td>\n      <td>76.374977</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 06:00:00-04:00</th>\n      <td>77.997500</td>\n      <td>78.270000</td>\n      <td>77.997500</td>\n      <td>78.2450</td>\n      <td>78.2450</td>\n      <td>0</td>\n      <td>0.003430</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.093408</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.343048</td>\n      <td>0.342461</td>\n      <td>0.346265</td>\n      <td>76.417747</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 07:00:00-04:00</th>\n      <td>78.205000</td>\n      <td>78.297500</td>\n      <td>78.017500</td>\n      <td>78.1125</td>\n      <td>78.1125</td>\n      <td>0</td>\n      <td>-0.001693</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.101618</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.169340</td>\n      <td>-0.169483</td>\n      <td>0.176339</td>\n      <td>76.184998</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2020-05-13 08:00:00-04:00</th>\n      <td>78.117500</td>\n      <td>78.500000</td>\n      <td>78.005000</td>\n      <td>78.3225</td>\n      <td>78.3225</td>\n      <td>0</td>\n      <td>0.002688</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>78.194854</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.268843</td>\n      <td>0.268482</td>\n      <td>0.445656</td>\n      <td>76.919998</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 15:30:00-04:00</th>\n      <td>122.474998</td>\n      <td>123.059998</td>\n      <td>122.300003</td>\n      <td>122.8200</td>\n      <td>122.8200</td>\n      <td>14469408</td>\n      <td>0.002858</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.139414</td>\n      <td>...</td>\n      <td>122.794127</td>\n      <td>-1.947949</td>\n      <td>23.803451</td>\n      <td>9.532677</td>\n      <td>14.270774</td>\n      <td>0.285783</td>\n      <td>0.285375</td>\n      <td>57.512023</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 16:00:00-04:00</th>\n      <td>122.820000</td>\n      <td>123.090000</td>\n      <td>122.345000</td>\n      <td>122.4000</td>\n      <td>122.4000</td>\n      <td>0</td>\n      <td>-0.003420</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.227993</td>\n      <td>...</td>\n      <td>122.682449</td>\n      <td>-2.384560</td>\n      <td>13.137307</td>\n      <td>10.253603</td>\n      <td>2.883705</td>\n      <td>-0.341964</td>\n      <td>-0.342550</td>\n      <td>56.973389</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 17:00:00-04:00</th>\n      <td>122.360000</td>\n      <td>124.524100</td>\n      <td>113.934780</td>\n      <td>122.4600</td>\n      <td>122.4600</td>\n      <td>0</td>\n      <td>0.000490</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.272966</td>\n      <td>...</td>\n      <td>122.624161</td>\n      <td>-2.196310</td>\n      <td>3.390093</td>\n      <td>8.880901</td>\n      <td>-5.490808</td>\n      <td>0.049020</td>\n      <td>0.049008</td>\n      <td>57.050337</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 18:00:00-04:00</th>\n      <td>122.460000</td>\n      <td>122.950000</td>\n      <td>122.350000</td>\n      <td>122.7500</td>\n      <td>122.7500</td>\n      <td>0</td>\n      <td>0.002368</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.337262</td>\n      <td>...</td>\n      <td>122.638585</td>\n      <td>-1.870653</td>\n      <td>-5.517361</td>\n      <td>6.001248</td>\n      <td>-11.518609</td>\n      <td>0.236812</td>\n      <td>0.236532</td>\n      <td>57.422251</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-05-12 19:00:00-04:00</th>\n      <td>122.750000</td>\n      <td>123.000000</td>\n      <td>122.640100</td>\n      <td>122.7900</td>\n      <td>122.7900</td>\n      <td>0</td>\n      <td>0.000326</td>\n      <td>6.359432e+08</td>\n      <td>-499096534</td>\n      <td>-0.337262</td>\n      <td>...</td>\n      <td>122.649011</td>\n      <td>-1.799424</td>\n      <td>-13.657404</td>\n      <td>2.069518</td>\n      <td>-15.726922</td>\n      <td>0.032587</td>\n      <td>0.032581</td>\n      <td>57.473549</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4187 rows Ã— 92 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_class(df):\n",
    "    higher_threshold = 1.5\n",
    "    lowest_threshold = -1.5\n",
    "    last_values_higher = []\n",
    "    last_values_lower = []\n",
    "    df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)), axis=1)\n",
    "    while True:\n",
    "        class_counts = df['class_column'].value_counts()\n",
    "        if abs(class_counts[0] - class_counts[1]) < 15 and abs(class_counts[0] - class_counts[-1]) < 15:\n",
    "            break\n",
    "\n",
    "        if len(last_values_higher) == 3:\n",
    "            last_values_higher.pop(0)\n",
    "        if len(last_values_lower) == 3:\n",
    "            last_values_lower.pop(0)\n",
    "\n",
    "        last_values_higher.append(higher_threshold)\n",
    "        last_values_lower.append(lowest_threshold)\n",
    "        if class_counts[0] > class_counts[1]:\n",
    "            higher_threshold -= 0.01\n",
    "        if class_counts[0] > class_counts[-1]:\n",
    "            lowest_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[1]:\n",
    "            higher_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[-1]:\n",
    "            lowest_threshold -= 0.01\n",
    "\n",
    "        if higher_threshold in last_values_higher and lowest_threshold in last_values_lower:\n",
    "            break\n",
    "        df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)),\n",
    "                                      axis=1)\n",
    "    print(df['class_column'].value_counts())\n",
    "    return df\n",
    "\n",
    "\n",
    "data = create_class(data)\n",
    "\n",
    "data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\Desktop\\repo\\magisterka_analiza\\data\\results\\train_test\\AAPL_1y_8_diff_13_05_2021 10_13_15_full.csv\n"
     ]
    }
   ],
   "source": [
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\{symbol}_{INTERVAL}_{WINDOW}_diff_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}_full.csv'\n",
    "data.to_csv(filename_to_export, index=True)\n",
    "print(filename_to_export)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1397\n 0    1395\n-1    1395\nName: class_column, dtype: int64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Class divide\n",
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['close_shift'])\n",
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1397\n-1    1394\n 0    1387\nName: class_column, dtype: int64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size=17):\n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i * chunk_size:(i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def train_model(model, train_x, train_y):\n",
    "    model.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "246"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataframe = split_dataframe(data, 17)\n",
    "len(splited_dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column', 'close_shift']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "# x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "classifiers = dict()\n",
    "\n",
    "classifiers['DecisionTreeClassifier 1'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 2'] = DecisionTreeClassifier(max_depth=20, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 3'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='gini',\n",
    "                                                                 splitter='random')\n",
    "classifiers['DecisionTreeClassifier 4'] = DecisionTreeClassifier(max_depth=10, random_state=0, criterion='entropy',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 5'] = DecisionTreeClassifier(max_depth=15, random_state=0, criterion='entropy',\n",
    "                                                                 splitter='best')\n",
    "classifiers['RandomForestClassifier 4'] = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0,\n",
    "                                                                 criterion='gini', n_jobs=-1)\n",
    "classifiers['RandomForestClassifier 5'] = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0,\n",
    "                                                                 criterion='entropy', n_jobs=-1)\n",
    "classifiers['GradientBoostingClassifier 1'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.1)\n",
    "classifiers['GradientBoostingClassifier 2'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=10,\n",
    "                                                                         learning_rate=0.3)\n",
    "classifiers['GradientBoostingClassifier 3'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.1)\n",
    "classifiers['XGBClassifier 1'] = xgb.XGBClassifier(nthread=-1, max_depth=10, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBClassifier 2'] = xgb.XGBClassifier(nthread=-1, max_depth=14, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBClassifier 3'] = xgb.XGBClassifier(nthread=-1, max_depth=14, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBClassifier 4'] = xgb.XGBClassifier(nthread=-1, max_depth=10, n_estimators=1000, eta=0.5)\n",
    "classifiers['XGBClassifier 5'] = xgb.XGBClassifier(nthread=-1, max_depth=6, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBClassifier 6'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.3)\n",
    "classifiers['XGBRFClassifier 1'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=12, n_estimators=100, eta=0.4)\n",
    "classifiers['XGBRFClassifier 2'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=14, n_estimators=100, eta=0.4)\n",
    "classifiers['XGBRFClassifier 3'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=3, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 4'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=6, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 5'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=10, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBRFClassifier 6'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=10, n_estimators=100, eta=0.4)\n",
    "classifiers_boosted = dict()\n",
    "classifiers_boosted['GradientBoostingClassifier 1S'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=3,\n",
    "                                                                                  learning_rate=0.1)\n",
    "classifiers_boosted['GradientBoostingClassifier 2S'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=3,\n",
    "                                                                                  learning_rate=0.3)\n",
    "classifiers_boosted['GradientBoostingClassifier 3S'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                                  criterion='friedman_mse', max_depth=2,\n",
    "                                                                                  learning_rate=0.5)\n",
    "# classifiers_boosted['GradientBoostingClassifier 4S'] = GradientBoostingClassifier(n_estimators=1000,random_state=0,criterion='friedman_mse',max_depth=2, learning_rate=0.8)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def count_correct(pred_list, original_list):\n",
    "    correct_inc = 0\n",
    "    correct_dec = 0\n",
    "    correct_stag = 0\n",
    "    all = len(pred_list)\n",
    "    all_inc = 0\n",
    "    all_dec = 0\n",
    "    all_stag = 0\n",
    "    for idx, el in enumerate(pred_list):\n",
    "        if original_list[idx] == 1:\n",
    "            all_inc += 1\n",
    "            if el == 1:\n",
    "                correct_inc += 1\n",
    "        if original_list[idx] == -1:\n",
    "            all_dec += 1\n",
    "            if el == -1:\n",
    "                correct_dec += 1\n",
    "        if original_list[idx] == 0:\n",
    "            all_stag += 1\n",
    "            if el == 0:\n",
    "                correct_stag += 1\n",
    "    return correct_inc, correct_dec, correct_stag, all_inc, all_dec, all_stag, all\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    }
   ],
   "source": [
    "print(len(splited_dataframe))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "start\n",
      " 1    582\n",
      "-1    570\n",
      " 0    565\n",
      "Name: class_column, dtype: int64\n",
      " 1    582\n",
      "-1    570\n",
      " 0    565\n",
      "Name: class_column, dtype: int64\n",
      "1717\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:15:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:15:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:15:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:15:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:15:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:15:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:15:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:15:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:15:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:15:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:15:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:16:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_macd_signal', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.35294117647058826\n",
      " 1    585\n",
      "-1    578\n",
      " 0    571\n",
      "Name: class_column, dtype: int64\n",
      " 1    585\n",
      "-1    578\n",
      " 0    571\n",
      "Name: class_column, dtype: int64\n",
      "1734\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:18:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:18:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:18:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:18:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:18:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:18:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:19:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:19:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:19:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:19:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:19:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:19:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volume_nvi', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_trix', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    590\n",
      "-1    581\n",
      " 0    580\n",
      "Name: class_column, dtype: int64\n",
      " 1    590\n",
      "-1    581\n",
      " 0    580\n",
      "Name: class_column, dtype: int64\n",
      "1751\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7647058823529411\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:22:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:22:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:22:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:22:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:22:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:22:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:22:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:22:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_macd', 'trend_trix', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    599\n",
      " 0    585\n",
      "-1    584\n",
      "Name: class_column, dtype: int64\n",
      " 1    599\n",
      " 0    585\n",
      "-1    584\n",
      "Name: class_column, dtype: int64\n",
      "1768\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.9411764705882353\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "1.0\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:25:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:25:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:25:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:25:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:25:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:25:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:25:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:26:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:26:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:26:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:26:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:26:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbl', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.6470588235294118\n",
      " 1    604\n",
      " 0    594\n",
      "-1    587\n",
      "Name: class_column, dtype: int64\n",
      " 1    604\n",
      " 0    594\n",
      "-1    587\n",
      "Name: class_column, dtype: int64\n",
      "1785\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:29:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:29:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:29:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:29:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:29:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:29:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:29:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:29:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:29:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:29:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "1.0\n",
      " 1    612\n",
      " 0    598\n",
      "-1    592\n",
      "Name: class_column, dtype: int64\n",
      " 1    612\n",
      " 0    598\n",
      "-1    592\n",
      "Name: class_column, dtype: int64\n",
      "1802\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.6470588235294118\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:32:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:32:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:32:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:32:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:32:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:32:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:32:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:33:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:33:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:33:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:33:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volume_vwap', 'volatility_atr', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.6470588235294118\n",
      " 1    616\n",
      " 0    604\n",
      "-1    599\n",
      "Name: class_column, dtype: int64\n",
      " 1    616\n",
      " 0    604\n",
      "-1    599\n",
      "Name: class_column, dtype: int64\n",
      "1819\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:36:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:36:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:36:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:36:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:36:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:36:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:36:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:36:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:36:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:36:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:36:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:37:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_trix', 'trend_kst', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.17647058823529413\n",
      " 1    616\n",
      "-1    610\n",
      " 0    610\n",
      "Name: class_column, dtype: int64\n",
      " 1    616\n",
      "-1    610\n",
      " 0    610\n",
      "Name: class_column, dtype: int64\n",
      "1836\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:39:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:39:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:39:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:39:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:39:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:40:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:40:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:40:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:40:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:40:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_trix', 'trend_kst', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 0    623\n",
      " 1    616\n",
      "-1    614\n",
      "Name: class_column, dtype: int64\n",
      " 0    623\n",
      " 1    616\n",
      "-1    614\n",
      "Name: class_column, dtype: int64\n",
      "1853\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:43:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:43:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:43:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:43:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:43:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:43:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:43:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:43:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:43:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:43:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:44:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:44:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_trix', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.11764705882352941\n",
      " 1    631\n",
      " 0    626\n",
      "-1    613\n",
      "Name: class_column, dtype: int64\n",
      " 1    631\n",
      " 0    626\n",
      "-1    613\n",
      "Name: class_column, dtype: int64\n",
      "1870\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:46:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:46:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:47:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:47:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_kcc', 'volatility_dcw', 'trend_macd', 'trend_kst', 'trend_kst_sig', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5294117647058824\n",
      " 1    639\n",
      " 0    630\n",
      "-1    618\n",
      "Name: class_column, dtype: int64\n",
      " 1    639\n",
      " 0    630\n",
      "-1    618\n",
      "Name: class_column, dtype: int64\n",
      "1887\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:50:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:50:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:50:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:50:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:50:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:50:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:50:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:50:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:51:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:51:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_kcl', 'volatility_dcw', 'trend_macd', 'trend_macd_signal', 'trend_trix', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5294117647058824\n",
      " 1    641\n",
      " 0    638\n",
      "-1    625\n",
      "Name: class_column, dtype: int64\n",
      " 1    641\n",
      " 0    638\n",
      "-1    625\n",
      "Name: class_column, dtype: int64\n",
      "1904\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:54:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:54:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:54:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:54:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:54:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:54:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:54:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:54:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:54:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_bbh', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_trix', 'trend_kst_sig', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    651\n",
      " 0    641\n",
      "-1    629\n",
      "Name: class_column, dtype: int64\n",
      " 1    651\n",
      " 0    641\n",
      "-1    629\n",
      "Name: class_column, dtype: int64\n",
      "1921\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 1\n",
      "[10:57:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 2\n",
      "[10:58:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 3\n",
      "[10:58:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 4\n",
      "[10:58:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 5\n",
      "[10:58:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 6\n",
      "[10:58:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[10:58:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[10:58:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[10:58:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[10:58:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[10:58:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[10:58:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_kst_sig', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.35294117647058826\n",
      " 1    655\n",
      " 0    646\n",
      "-1    637\n",
      "Name: class_column, dtype: int64\n",
      " 1    655\n",
      " 0    646\n",
      "-1    637\n",
      "Name: class_column, dtype: int64\n",
      "1938\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.0\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:01:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:01:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:01:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:02:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:02:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:02:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:02:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:02:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:02:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:02:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:02:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_macd', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.47058823529411764\n",
      " 1    662\n",
      "-1    648\n",
      " 0    645\n",
      "Name: class_column, dtype: int64\n",
      " 1    662\n",
      "-1    648\n",
      " 0    645\n",
      "Name: class_column, dtype: int64\n",
      "1955\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:05:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:05:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:06:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:06:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:06:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:06:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:06:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:06:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_sma_fast', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'momentum_tsi', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    666\n",
      " 0    657\n",
      "-1    649\n",
      "Name: class_column, dtype: int64\n",
      " 1    666\n",
      " 0    657\n",
      "-1    649\n",
      "Name: class_column, dtype: int64\n",
      "1972\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:09:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:09:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:09:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:09:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:09:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:09:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:09:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:09:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:09:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:10:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:10:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:10:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_trix', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    674\n",
      " 0    660\n",
      "-1    655\n",
      "Name: class_column, dtype: int64\n",
      " 1    674\n",
      " 0    660\n",
      "-1    655\n",
      "Name: class_column, dtype: int64\n",
      "1989\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.8235294117647058\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:13:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:13:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:13:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:13:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:13:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:13:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:13:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:13:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:14:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:14:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_fi', 'volume_nvi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8823529411764706\n",
      " 1    680\n",
      " 0    666\n",
      "-1    660\n",
      "Name: class_column, dtype: int64\n",
      " 1    680\n",
      " 0    666\n",
      "-1    660\n",
      "Name: class_column, dtype: int64\n",
      "2006\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:17:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:17:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:17:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:17:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:17:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:17:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:17:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:17:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:17:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:17:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:18:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:18:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_adi', 'volatility_bbh', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.6470588235294118\n",
      " 1    687\n",
      " 0    673\n",
      "-1    663\n",
      "Name: class_column, dtype: int64\n",
      " 1    687\n",
      " 0    673\n",
      "-1    663\n",
      "Name: class_column, dtype: int64\n",
      "2023\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.8823529411764706\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:21:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:21:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:21:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:21:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:21:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:21:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:21:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:21:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:21:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:22:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:22:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_dcw', 'trend_ema_fast', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "1.0\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.9411764705882353\n",
      " 1    689\n",
      " 0    679\n",
      "-1    672\n",
      "Name: class_column, dtype: int64\n",
      " 1    689\n",
      " 0    679\n",
      "-1    672\n",
      "Name: class_column, dtype: int64\n",
      "2040\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:25:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:25:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:25:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:26:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_adi', 'volume_nvi', 'volatility_dcw', 'trend_macd_signal', 'trend_kst', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8823529411764706\n",
      " 1    695\n",
      " 0    686\n",
      "-1    676\n",
      "Name: class_column, dtype: int64\n",
      " 1    695\n",
      " 0    686\n",
      "-1    676\n",
      "Name: class_column, dtype: int64\n",
      "2057\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:29:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:29:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:29:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:29:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:29:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:29:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:29:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:29:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:29:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:30:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'Adj Close', 'volume_adi', 'volatility_atr', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_aroon_ind', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 0    694\n",
      " 1    694\n",
      "-1    686\n",
      "Name: class_column, dtype: int64\n",
      " 0    694\n",
      " 1    694\n",
      "-1    686\n",
      "Name: class_column, dtype: int64\n",
      "2074\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:33:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:33:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:33:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:33:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:33:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:33:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:33:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:34:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:34:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_adi', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_sma_fast', 'trend_sma_slow', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    708\n",
      " 0    697\n",
      "-1    686\n",
      "Name: class_column, dtype: int64\n",
      " 1    708\n",
      " 0    697\n",
      "-1    686\n",
      "Name: class_column, dtype: int64\n",
      "2091\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:37:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:37:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:37:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:37:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:37:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:37:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:37:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:37:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:37:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:38:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:38:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'Adj Close', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'trend_sma_slow', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    711\n",
      "-1    701\n",
      " 0    696\n",
      "Name: class_column, dtype: int64\n",
      " 1    711\n",
      "-1    701\n",
      " 0    696\n",
      "Name: class_column, dtype: int64\n",
      "2108\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:41:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:41:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:41:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:41:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:41:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:41:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:41:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:41:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:41:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:42:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_kst', 'trend_visual_ichimoku_b', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8823529411764706\n",
      " 1    713\n",
      "-1    709\n",
      " 0    703\n",
      "Name: class_column, dtype: int64\n",
      " 1    713\n",
      "-1    709\n",
      " 0    703\n",
      "Name: class_column, dtype: int64\n",
      "2125\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "1.0\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:45:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:45:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:45:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:45:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:45:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:46:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:46:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:46:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:46:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_kch', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    723\n",
      "-1    711\n",
      " 0    708\n",
      "Name: class_column, dtype: int64\n",
      " 1    723\n",
      "-1    711\n",
      " 0    708\n",
      "Name: class_column, dtype: int64\n",
      "2142\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:49:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:49:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:49:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:49:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:49:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:49:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:50:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:50:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:50:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:50:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:50:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:50:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['high', 'Adj Close', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'trend_sma_slow', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.9411764705882353\n",
      " 1    725\n",
      " 0    723\n",
      "-1    711\n",
      "Name: class_column, dtype: int64\n",
      " 1    725\n",
      " 0    723\n",
      "-1    711\n",
      "Name: class_column, dtype: int64\n",
      "2159\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:53:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:53:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:53:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:54:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:54:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:54:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:54:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:54:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:54:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:54:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:54:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:54:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    733\n",
      " 0    727\n",
      "-1    716\n",
      "Name: class_column, dtype: int64\n",
      " 1    733\n",
      " 0    727\n",
      "-1    716\n",
      "Name: class_column, dtype: int64\n",
      "2176\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 1\n",
      "[11:57:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[11:58:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[11:58:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[11:58:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[11:58:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[11:58:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[11:58:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[11:58:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[11:58:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[11:58:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[11:58:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[11:59:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['Adj Close', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_ema_slow', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    742\n",
      " 0    728\n",
      "-1    723\n",
      "Name: class_column, dtype: int64\n",
      " 1    742\n",
      " 0    728\n",
      "-1    723\n",
      "Name: class_column, dtype: int64\n",
      "2193\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:02:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:02:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:02:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:02:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:02:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:02:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:02:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:02:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:02:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:03:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:03:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:03:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'Adj Close', 'volatility_atr', 'volatility_dcw', 'volatility_ui', 'trend_kst_sig', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    748\n",
      " 0    738\n",
      "-1    724\n",
      "Name: class_column, dtype: int64\n",
      " 1    748\n",
      " 0    738\n",
      "-1    724\n",
      "Name: class_column, dtype: int64\n",
      "2210\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:06:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:06:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:06:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:06:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:06:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:07:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:07:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:07:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:07:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:07:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:07:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:07:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['high', 'close', 'volatility_atr', 'volatility_dcw', 'volatility_ui', 'trend_kst_sig', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 0    746\n",
      " 1    744\n",
      "-1    737\n",
      "Name: class_column, dtype: int64\n",
      " 0    746\n",
      " 1    744\n",
      "-1    737\n",
      "Name: class_column, dtype: int64\n",
      "2227\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:10:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:11:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:11:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:11:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:11:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:11:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:11:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:11:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:11:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:11:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:12:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:12:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.4117647058823529\n",
      " 1    760\n",
      " 0    748\n",
      "-1    736\n",
      "Name: class_column, dtype: int64\n",
      " 1    760\n",
      " 0    748\n",
      "-1    736\n",
      "Name: class_column, dtype: int64\n",
      "2244\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:15:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:15:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:15:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:15:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:15:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:15:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:15:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:15:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.058823529411764705\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:16:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:16:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.0\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:16:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.0\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:16:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['Adj Close', 'volume_nvi', 'volatility_atr', 'volatility_dcw', 'trend_macd_signal', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 0    759\n",
      " 1    756\n",
      "-1    746\n",
      "Name: class_column, dtype: int64\n",
      " 0    759\n",
      " 1    756\n",
      "-1    746\n",
      "Name: class_column, dtype: int64\n",
      "2261\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.9411764705882353\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.0\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:19:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:19:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:19:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:20:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:20:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:20:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:20:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:20:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:20:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:20:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:20:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    772\n",
      " 0    759\n",
      "-1    747\n",
      "Name: class_column, dtype: int64\n",
      " 1    772\n",
      " 0    759\n",
      "-1    747\n",
      "Name: class_column, dtype: int64\n",
      "2278\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:24:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:24:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:24:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:24:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:24:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:25:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['Adj Close', 'volatility_atr', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_kst', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    772\n",
      " 0    767\n",
      "-1    756\n",
      "Name: class_column, dtype: int64\n",
      " 1    772\n",
      " 0    767\n",
      "-1    756\n",
      "Name: class_column, dtype: int64\n",
      "2295\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7647058823529411\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:29:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:29:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:29:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:29:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:29:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:29:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:29:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:29:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:30:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['Adj Close', 'volatility_atr', 'volatility_dcw', 'trend_sma_slow', 'trend_kst', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 0    779\n",
      " 1    772\n",
      "-1    761\n",
      "Name: class_column, dtype: int64\n",
      " 0    779\n",
      " 1    772\n",
      "-1    761\n",
      "Name: class_column, dtype: int64\n",
      "2312\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:33:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:34:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:34:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:34:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:34:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:34:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:34:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:34:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:34:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_adi', 'volatility_atr', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_ema_slow', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "1.0\n",
      " 1    787\n",
      " 0    774\n",
      "-1    768\n",
      "Name: class_column, dtype: int64\n",
      " 1    787\n",
      " 0    774\n",
      "-1    768\n",
      "Name: class_column, dtype: int64\n",
      "2329\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:38:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:38:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:38:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:38:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:38:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:39:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:39:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:39:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:39:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'volatility_atr', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.4117647058823529\n",
      " 1    784\n",
      " 0    782\n",
      "-1    780\n",
      "Name: class_column, dtype: int64\n",
      " 1    784\n",
      " 0    782\n",
      "-1    780\n",
      "Name: class_column, dtype: int64\n",
      "2346\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:43:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:43:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:43:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:43:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:43:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:44:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:44:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:44:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'trend_aroon_ind', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    794\n",
      " 0    789\n",
      "-1    780\n",
      "Name: class_column, dtype: int64\n",
      " 1    794\n",
      " 0    789\n",
      "-1    780\n",
      "Name: class_column, dtype: int64\n",
      "2363\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:47:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:48:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:48:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:48:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:48:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:48:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:48:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:48:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:48:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:49:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:49:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    797\n",
      " 0    795\n",
      "-1    788\n",
      "Name: class_column, dtype: int64\n",
      " 1    797\n",
      " 0    795\n",
      "-1    788\n",
      "Name: class_column, dtype: int64\n",
      "2380\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:52:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:52:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:53:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:53:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:53:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:53:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:53:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:53:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:53:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:53:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:54:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:54:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'trend_aroon_ind', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5294117647058824\n",
      " 1    803\n",
      " 0    800\n",
      "-1    794\n",
      "Name: class_column, dtype: int64\n",
      " 1    803\n",
      " 0    800\n",
      "-1    794\n",
      "Name: class_column, dtype: int64\n",
      "2397\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.8823529411764706\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[12:57:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[12:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 3\n",
      "[12:57:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 4\n",
      "[12:58:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[12:58:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[12:58:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[12:58:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[12:58:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[12:58:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[12:58:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[12:58:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[12:58:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    815\n",
      "-1    803\n",
      " 0    796\n",
      "Name: class_column, dtype: int64\n",
      " 1    815\n",
      "-1    803\n",
      " 0    796\n",
      "Name: class_column, dtype: int64\n",
      "2414\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.7647058823529411\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:02:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:02:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:02:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:02:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:02:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:03:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:03:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:03:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:03:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:03:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:03:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:03:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_ichimoku_b', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    816\n",
      "-1    808\n",
      " 0    807\n",
      "Name: class_column, dtype: int64\n",
      " 1    816\n",
      "-1    808\n",
      " 0    807\n",
      "Name: class_column, dtype: int64\n",
      "2431\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:07:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:07:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:07:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:07:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:07:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:08:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:08:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:08:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:08:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:08:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:08:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:08:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_adi', 'volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 1    830\n",
      " 0    816\n",
      "-1    802\n",
      "Name: class_column, dtype: int64\n",
      " 1    830\n",
      " 0    816\n",
      "-1    802\n",
      "Name: class_column, dtype: int64\n",
      "2448\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:12:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:12:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:12:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:12:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:12:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:13:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:13:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:13:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:13:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:13:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:13:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:13:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_kch', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_ppo']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    832\n",
      " 0    819\n",
      "-1    814\n",
      "Name: class_column, dtype: int64\n",
      " 1    832\n",
      " 0    819\n",
      "-1    814\n",
      "Name: class_column, dtype: int64\n",
      "2465\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:17:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:17:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:17:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:17:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:17:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:18:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:18:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:18:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:18:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:18:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:18:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:18:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'trend_aroon_ind', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    840\n",
      "-1    822\n",
      " 0    820\n",
      "Name: class_column, dtype: int64\n",
      " 1    840\n",
      "-1    822\n",
      " 0    820\n",
      "Name: class_column, dtype: int64\n",
      "2482\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.9411764705882353\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.9411764705882353\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:22:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:22:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:23:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:23:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:23:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:23:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:23:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:23:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9411764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5294117647058824\n",
      " 1    838\n",
      " 0    836\n",
      "-1    825\n",
      "Name: class_column, dtype: int64\n",
      " 1    838\n",
      " 0    836\n",
      "-1    825\n",
      "Name: class_column, dtype: int64\n",
      "2499\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.17647058823529413\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:27:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:27:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:27:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:28:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:28:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:28:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:28:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:28:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:28:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:28:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:28:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_ichimoku_b', 'trend_visual_ichimoku_b', 'trend_aroon_ind', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.4117647058823529\n",
      " 1    844\n",
      "-1    840\n",
      " 0    832\n",
      "Name: class_column, dtype: int64\n",
      " 1    844\n",
      "-1    840\n",
      " 0    832\n",
      "Name: class_column, dtype: int64\n",
      "2516\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:32:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:32:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:32:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:33:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:33:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:33:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:33:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:33:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:34:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_visual_ichimoku_a', 'momentum_tsi', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 1    850\n",
      " 0    848\n",
      "-1    835\n",
      "Name: class_column, dtype: int64\n",
      " 1    850\n",
      " 0    848\n",
      "-1    835\n",
      "Name: class_column, dtype: int64\n",
      "2533\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:38:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:38:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:38:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:38:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:38:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:38:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:38:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.0\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:39:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:39:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:39:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['Adj Close', 'volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8235294117647058\n",
      " 1    861\n",
      "-1    846\n",
      " 0    843\n",
      "Name: class_column, dtype: int64\n",
      " 1    861\n",
      "-1    846\n",
      " 0    843\n",
      "Name: class_column, dtype: int64\n",
      "2550\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:43:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:43:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:43:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:43:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:43:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:43:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:44:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:44:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:44:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:44:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 1    863\n",
      "-1    854\n",
      " 0    850\n",
      "Name: class_column, dtype: int64\n",
      " 1    863\n",
      "-1    854\n",
      " 0    850\n",
      "Name: class_column, dtype: int64\n",
      "2567\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:48:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:48:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:48:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:48:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:48:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:49:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:49:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:49:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:49:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:49:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:49:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:49:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.35294117647058826\n",
      " 1    875\n",
      " 0    861\n",
      "-1    848\n",
      "Name: class_column, dtype: int64\n",
      " 1    875\n",
      " 0    861\n",
      "-1    848\n",
      "Name: class_column, dtype: int64\n",
      "2584\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.6470588235294118\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:53:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:53:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:54:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:54:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:54:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:54:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[13:54:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[13:54:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[13:54:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[13:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[13:55:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[13:55:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.9411764705882353\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.8823529411764706\n",
      " 1    868\n",
      " 0    867\n",
      "-1    866\n",
      "Name: class_column, dtype: int64\n",
      " 1    868\n",
      " 0    867\n",
      "-1    866\n",
      "Name: class_column, dtype: int64\n",
      "2601\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 1\n",
      "[13:59:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[13:59:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[13:59:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[13:59:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[13:59:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[13:59:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:00:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:00:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:00:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:00:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:00:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    879\n",
      "-1    872\n",
      " 0    867\n",
      "Name: class_column, dtype: int64\n",
      " 1    879\n",
      "-1    872\n",
      " 0    867\n",
      "Name: class_column, dtype: int64\n",
      "2618\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.0\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.058823529411764705\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:04:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:04:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:04:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:05:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:06:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:06:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'trend_sma_slow', 'trend_trix', 'momentum_ppo', 'others_cr', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    888\n",
      "-1    877\n",
      " 0    870\n",
      "Name: class_column, dtype: int64\n",
      " 1    888\n",
      "-1    877\n",
      " 0    870\n",
      "Name: class_column, dtype: int64\n",
      "2635\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:10:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:10:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:10:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:10:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:10:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:10:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:11:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:11:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:11:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:11:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:11:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:11:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_kcl', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.35294117647058826\n",
      " 0    890\n",
      " 1    885\n",
      "-1    877\n",
      "Name: class_column, dtype: int64\n",
      " 0    890\n",
      " 1    885\n",
      "-1    877\n",
      "Name: class_column, dtype: int64\n",
      "2652\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.0\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.8823529411764706\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:15:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:15:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:16:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:16:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:16:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:16:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:16:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:16:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:16:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:16:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:17:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:17:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 1    901\n",
      " 0    890\n",
      "-1    878\n",
      "Name: class_column, dtype: int64\n",
      " 1    901\n",
      " 0    890\n",
      "-1    878\n",
      "Name: class_column, dtype: int64\n",
      "2669\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:21:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:21:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:21:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:21:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:21:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:21:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:22:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:22:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:22:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['close', 'volatility_atr', 'volatility_bbw', 'volatility_kcw', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_ema_slow', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.29411764705882354\n",
      " 0    902\n",
      " 1    896\n",
      "-1    888\n",
      "Name: class_column, dtype: int64\n",
      " 0    902\n",
      " 1    896\n",
      "-1    888\n",
      "Name: class_column, dtype: int64\n",
      "2686\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:26:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:26:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:27:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:27:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:27:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:27:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:27:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:27:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:27:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:27:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:28:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:28:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_ema_fast', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.11764705882352941\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.058823529411764705\n",
      " 1    908\n",
      " 0    898\n",
      "-1    897\n",
      "Name: class_column, dtype: int64\n",
      " 1    908\n",
      " 0    898\n",
      "-1    897\n",
      "Name: class_column, dtype: int64\n",
      "2703\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:32:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:32:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:32:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:32:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:32:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:32:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:33:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:33:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:33:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:33:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:33:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:33:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_ema_slow', 'trend_visual_ichimoku_b', 'trend_aroon_down', 'momentum_ppo_hist', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 1    913\n",
      "-1    907\n",
      " 0    900\n",
      "Name: class_column, dtype: int64\n",
      " 1    913\n",
      "-1    907\n",
      " 0    900\n",
      "Name: class_column, dtype: int64\n",
      "2720\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:37:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:38:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:38:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:38:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:38:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:38:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:38:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:39:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:39:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_kst_sig', 'trend_visual_ichimoku_a', 'momentum_tsi', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 0    919\n",
      " 1    918\n",
      "-1    900\n",
      "Name: class_column, dtype: int64\n",
      " 0    919\n",
      " 1    918\n",
      "-1    900\n",
      "Name: class_column, dtype: int64\n",
      "2737\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:43:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:43:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:43:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:44:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:44:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:44:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:44:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:44:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:44:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:45:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'trend_visual_ichimoku_a', 'momentum_kama', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.47058823529411764\n",
      " 0    922\n",
      " 1    918\n",
      "-1    914\n",
      "Name: class_column, dtype: int64\n",
      " 0    922\n",
      " 1    918\n",
      "-1    914\n",
      "Name: class_column, dtype: int64\n",
      "2754\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:49:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:49:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:49:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:50:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:50:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:50:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:50:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:50:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:51:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'momentum_tsi', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    929\n",
      " 0    926\n",
      "-1    916\n",
      "Name: class_column, dtype: int64\n",
      " 1    929\n",
      " 0    926\n",
      "-1    916\n",
      "Name: class_column, dtype: int64\n",
      "2771\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.8823529411764706\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.8823529411764706\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.35294117647058826\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.7647058823529411\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 1\n",
      "[14:55:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[14:55:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8823529411764706\n",
      "Calculate:  XGBClassifier 3\n",
      "[14:55:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 4\n",
      "[14:55:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 5\n",
      "[14:56:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[14:56:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[14:56:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[14:56:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[14:56:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[14:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[14:56:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[14:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'trend_trix', 'trend_visual_ichimoku_a', 'momentum_tsi', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    938\n",
      " 0    928\n",
      "-1    922\n",
      "Name: class_column, dtype: int64\n",
      " 1    938\n",
      " 0    928\n",
      "-1    922\n",
      "Name: class_column, dtype: int64\n",
      "2788\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7647058823529411\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:01:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:01:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:01:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:01:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8235294117647058\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:02:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:02:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:02:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:02:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:02:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:02:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:02:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_sma_fast', 'momentum_tsi', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.4117647058823529\n",
      " 0    946\n",
      " 1    939\n",
      "-1    920\n",
      "Name: class_column, dtype: int64\n",
      " 0    946\n",
      " 1    939\n",
      "-1    920\n",
      "Name: class_column, dtype: int64\n",
      "2805\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.23529411764705882\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.35294117647058826\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:07:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:07:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:07:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:07:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:07:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:08:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:08:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:08:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:08:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:08:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:08:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_dcw', 'volatility_ui', 'trend_trix', 'trend_visual_ichimoku_a', 'momentum_tsi', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.5882352941176471\n",
      " 1    951\n",
      " 0    939\n",
      "-1    932\n",
      "Name: class_column, dtype: int64\n",
      " 1    951\n",
      " 0    939\n",
      "-1    932\n",
      "Name: class_column, dtype: int64\n",
      "2822\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.8235294117647058\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.5294117647058824\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.0\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.17647058823529413\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:13:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:13:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:13:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:13:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:13:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:13:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:14:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.11764705882352941\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:14:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:14:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:14:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'trend_visual_ichimoku_a', 'momentum_tsi', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 1    952\n",
      " 0    944\n",
      "-1    943\n",
      "Name: class_column, dtype: int64\n",
      " 1    952\n",
      " 0    944\n",
      "-1    943\n",
      "Name: class_column, dtype: int64\n",
      "2839\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.17647058823529413\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.058823529411764705\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.5882352941176471\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:19:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:19:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:19:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:19:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:19:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:19:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:19:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:19:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7647058823529411\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:19:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:20:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:20:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:20:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbm', 'volatility_bbh', 'volatility_dch', 'volatility_dcw', 'volatility_ui', 'trend_sma_slow', 'momentum_tsi', 'momentum_kama', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.8235294117647058\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7647058823529411\n",
      " 1    958\n",
      " 0    955\n",
      "-1    943\n",
      "Name: class_column, dtype: int64\n",
      " 1    958\n",
      " 0    955\n",
      "-1    943\n",
      "Name: class_column, dtype: int64\n",
      "2856\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.7647058823529411\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.5882352941176471\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.47058823529411764\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.47058823529411764\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.7058823529411765\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:25:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:25:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:25:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:25:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:26:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5294117647058824\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:26:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:26:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volatility_atr', 'volatility_bbh', 'volatility_bbl', 'volatility_dch', 'volatility_dcw', 'trend_trix', 'trend_visual_ichimoku_a', 'trend_aroon_down', 'momentum_tsi', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n",
      " 0    966\n",
      " 1    959\n",
      "-1    948\n",
      "Name: class_column, dtype: int64\n",
      " 0    966\n",
      " 1    959\n",
      "-1    948\n",
      "Name: class_column, dtype: int64\n",
      "2873\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.23529411764705882\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.4117647058823529\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.5294117647058824\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:30:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:31:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:31:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:31:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:31:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5882352941176471\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:31:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.47058823529411764\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:31:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:31:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:31:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.4117647058823529\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:32:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:32:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.29411764705882354\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:32:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_dch', 'volatility_dcw', 'trend_sma_slow', 'trend_visual_ichimoku_a', 'momentum_tsi', 'momentum_ppo', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.5294117647058824\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.29411764705882354\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.23529411764705882\n",
      " 1    971\n",
      "-1    960\n",
      " 0    959\n",
      "Name: class_column, dtype: int64\n",
      " 1    971\n",
      "-1    960\n",
      " 0    959\n",
      "Name: class_column, dtype: int64\n",
      "2890\n",
      "Calculate:  DecisionTreeClassifier 1\n",
      "0.29411764705882354\n",
      "Calculate:  DecisionTreeClassifier 2\n",
      "0.35294117647058826\n",
      "Calculate:  DecisionTreeClassifier 3\n",
      "0.058823529411764705\n",
      "Calculate:  DecisionTreeClassifier 4\n",
      "0.4117647058823529\n",
      "Calculate:  DecisionTreeClassifier 5\n",
      "0.29411764705882354\n",
      "Calculate:  RandomForestClassifier 4\n",
      "0.11764705882352941\n",
      "Calculate:  RandomForestClassifier 5\n",
      "0.0\n",
      "Calculate:  GradientBoostingClassifier 1\n",
      "0.7058823529411765\n",
      "Calculate:  GradientBoostingClassifier 2\n",
      "0.47058823529411764\n",
      "Calculate:  GradientBoostingClassifier 3\n",
      "0.7647058823529411\n",
      "Calculate:  XGBClassifier 1\n",
      "[15:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 2\n",
      "[15:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 3\n",
      "[15:37:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 4\n",
      "[15:37:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6470588235294118\n",
      "Calculate:  XGBClassifier 5\n",
      "[15:37:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBClassifier 6\n",
      "[15:37:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7058823529411765\n",
      "Calculate:  XGBRFClassifier 1\n",
      "[15:37:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 2\n",
      "[15:37:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 3\n",
      "[15:37:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.23529411764705882\n",
      "Calculate:  XGBRFClassifier 4\n",
      "[15:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 5\n",
      "[15:38:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n",
      "Calculate:  XGBRFClassifier 6\n",
      "[15:38:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.17647058823529413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_features_to_select=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with predictive power: ['volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_bbw', 'volatility_dch', 'volatility_dcw', 'trend_trix', 'trend_visual_ichimoku_a', 'trend_psar_down', 'close_shift']\n",
      "Calculate:  GradientBoostingClassifier 1S\n",
      "0.6470588235294118\n",
      "Calculate:  GradientBoostingClassifier 2S\n",
      "0.5882352941176471\n",
      "Calculate:  GradientBoostingClassifier 3S\n",
      "0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "start_data = splited_dataframe[:100]\n",
    "next_data = splited_dataframe[100:170]\n",
    "print(len(next_data))\n",
    "score = defaultdict(list)\n",
    "points = defaultdict(list)\n",
    "points_train = defaultdict(list)\n",
    "score_train = defaultdict(list)\n",
    "step_headers = []\n",
    "i = 0\n",
    "print(\"start\")\n",
    "for idx, day in enumerate(next_data):\n",
    "    start_data.append(day)\n",
    "    data_set = pd.concat(start_data)\n",
    "    data_set = create_class(data_set)\n",
    "    print(data_set['class_column'].value_counts())\n",
    "    y = data_set['class_column']\n",
    "    features = [x for x in data_set.columns if x not in ['class_column']]\n",
    "    x = data_set[features]\n",
    "    x_train = x.iloc[:-17]\n",
    "    y_train = y.iloc[:-17]\n",
    "    x_test = x.iloc[-17:]\n",
    "    y_test = y.iloc[-17:]\n",
    "\n",
    "    print(len(data_set))\n",
    "\n",
    "    step_headers.append(f'<{i}>')\n",
    "    i = i + 1\n",
    "    predictions_train = dict()\n",
    "    predictions = dict()\n",
    "\n",
    "    for k, v in classifiers.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v, x_train, y_train)\n",
    "        predictions_train[k] = v.predict(x_train)\n",
    "        score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k], y_train.values))\n",
    "        points[k].append(count_correct(predictions[k], y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "    rfe = RFE(classifiers['RandomForestClassifier 5'], 10)\n",
    "    fited = rfe.fit(x_train, y_train)\n",
    "    names = x.columns\n",
    "    columns = []\n",
    "    for i in range(len(fited.support_)):\n",
    "        if fited.support_[i]:\n",
    "            columns.append(names[i])\n",
    "\n",
    "    print(\"Columns with predictive power:\", columns)\n",
    "    columns = columns + ['high', 'low', 'volume', 'open']\n",
    "    x_test_cropped = x_test[columns]\n",
    "    x_train_cropped = x_train[columns]\n",
    "    for k, v in classifiers_boosted.items():\n",
    "        print(\"Calculate: \", k)\n",
    "        train_model(v, x_train_cropped, y_train)\n",
    "        predictions_train[k] = v.predict(x_train_cropped)\n",
    "        score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "        predictions[k] = v.predict(x_test_cropped)\n",
    "        score[k].append(accuracy_score(y_test.values, predictions[k]))\n",
    "        points_train[k].append(count_correct(predictions_train[k], y_train.values))\n",
    "        points[k].append(count_correct(predictions[k], y_test.values))\n",
    "        print(accuracy_score(y_test.values, predictions[k]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+----------+----------+-----------+----------+\n",
      "|    | Classifier type               |      <0> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |      <90> |      <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |      <90> |     <90> |     <90> |      <90> |      <90> |     <90> |      <90> |     <90> |      <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |      <90> |      <90> |     <90> |     <90> |      <90> |     mean |\n",
      "|----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+----------+----------+-----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.411765 | 0.176471 | 0.235294 | 0.470588 | 0.176471 | 0.352941 | 0.470588 | 0.294118 | 0.647059  | 0.647059 | 0.470588 | 0.529412 | 0.352941  | 0.470588 | 0.470588 | 0.411765 | 0.235294 | 0.117647  | 0.411765 | 0.352941 | 0.352941 | 0.411765 | 0.470588 | 0.117647 | 0.352941 | 0.411765 | 0.588235  | 0.352941 | 0.764706 | 0.588235 | 0.529412 | 0.294118  | 0         | 0.411765 | 0.411765  | 0.588235 | 0.294118 | 0.705882 | 0.588235 | 0.117647  | 0.294118 | 0.705882 | 0.117647 | 0.411765 | 0.117647 | 0.294118 | 0.294118 | 0.411765 | 0.529412 | 0.0588235 | 0.235294 | 0.176471 | 0.705882  | 0.117647  | 0.235294 | 0.352941  | 0.294118 | 0.235294  | 0.529412 | 0.470588 | 0.294118 | 0.117647 | 0.647059 | 0.352941 | 0.647059 | 0.823529  | 0.0588235 | 0.764706 | 0.235294 | 0.294118  | 0.384034 |\n",
      "|  1 | DecisionTreeClassifier 2      | 0.529412 | 0.117647 | 0.352941 | 0.411765 | 0.176471 | 0.352941 | 0.352941 | 0.294118 | 0.588235  | 0.588235 | 0.235294 | 0.294118 | 0.352941  | 0.470588 | 0.294118 | 0.352941 | 0.176471 | 0.176471  | 0.294118 | 0.352941 | 0.352941 | 0.529412 | 0.588235 | 0.176471 | 0.647059 | 0.529412 | 0.235294  | 0.411765 | 0.764706 | 0.352941 | 0.529412 | 0.235294  | 0.0588235 | 0.352941 | 0.411765  | 0.764706 | 0.294118 | 0.647059 | 0.588235 | 0.117647  | 0.411765 | 0.647059 | 0.117647 | 0.294118 | 0.117647 | 0.176471 | 0.294118 | 0.470588 | 0.176471 | 0.235294  | 0.352941 | 0.294118 | 0.705882  | 0         | 0.235294 | 0.235294  | 0.235294 | 0.235294  | 0.529412 | 0.352941 | 0.294118 | 0.117647 | 0.647059 | 0.411765 | 0.588235 | 0.647059  | 0.0588235 | 0.470588 | 0.235294 | 0.352941  | 0.361345 |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.470588 | 0.411765 | 0.294118 | 0.941176 | 0.176471 | 0.647059 | 0.352941 | 0.176471 | 0.411765  | 0.352941 | 0.235294 | 0.294118 | 0.411765  | 0.352941 | 0.235294 | 0.529412 | 0.117647 | 0.470588  | 0.882353 | 0.176471 | 0.588235 | 0.352941 | 0.235294 | 0.470588 | 0.117647 | 0.352941 | 0.705882  | 0.176471 | 0.470588 | 0.764706 | 0.411765 | 0.352941  | 0.941176  | 0.352941 | 0.352941  | 0.529412 | 0.117647 | 0        | 0        | 0.352941  | 0        | 0.529412 | 0.176471 | 0.352941 | 0.411765 | 0        | 0.352941 | 0.235294 | 0.235294 | 0.411765  | 0.529412 | 0.294118 | 0.176471  | 0.411765  | 0.117647 | 0.117647  | 0.705882 | 0.0588235 | 0.529412 | 0.294118 | 0.529412 | 0.411765 | 0.588235 | 0.294118 | 0.235294 | 0.294118  | 0.176471  | 0.588235 | 0.294118 | 0.0588235 | 0.357143 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.529412 | 0.352941 | 0.352941 | 0.411765 | 0.235294 | 0.411765 | 0.294118 | 0.411765 | 0.294118  | 0.529412 | 0.529412 | 0.470588 | 0.352941  | 0.529412 | 0.470588 | 0.117647 | 0.117647 | 0.117647  | 0.235294 | 0.647059 | 0.235294 | 0.529412 | 0.176471 | 0.117647 | 0.647059 | 0.647059 | 0.117647  | 0.529412 | 0.411765 | 0.588235 | 0.529412 | 0.705882  | 0.411765  | 0.470588 | 0.764706  | 0.529412 | 0.235294 | 0.352941 | 0.588235 | 0.294118  | 0.882353 | 0.764706 | 0.411765 | 0.294118 | 0.352941 | 0.941176 | 0.117647 | 0.294118 | 0.294118 | 0.0588235 | 0.470588 | 0.470588 | 0.352941  | 0.117647  | 0.588235 | 0.0588235 | 0.235294 | 0.0588235 | 0.529412 | 0.411765 | 0.529412 | 0.117647 | 0.882353 | 0.117647 | 0.235294 | 0.705882  | 0.235294  | 0.470588 | 0.294118 | 0.411765  | 0.4      |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.352941 | 0.470588 | 0.529412 | 0.411765 | 0.352941 | 0.352941 | 0.294118 | 0.705882 | 0.470588  | 0.470588 | 0.470588 | 0.470588 | 0.352941  | 0.705882 | 0.235294 | 0.176471 | 0.117647 | 0.0588235 | 0.411765 | 0.470588 | 0.294118 | 0.529412 | 0.294118 | 0.117647 | 0.411765 | 0.705882 | 0.0588235 | 0.470588 | 0.411765 | 0.647059 | 0.529412 | 0.705882  | 0.294118  | 0.705882 | 0.764706  | 0.529412 | 0.235294 | 0.352941 | 0.647059 | 0.294118  | 0.470588 | 0.764706 | 0.117647 | 0.235294 | 0.352941 | 0.941176 | 0.117647 | 0.411765 | 0.529412 | 0.0588235 | 0.470588 | 0.470588 | 0.470588  | 0.117647  | 0.588235 | 0.352941  | 0.411765 | 0.0588235 | 0.529412 | 0.352941 | 0.411765 | 0.117647 | 0.882353 | 0.117647 | 0.588235 | 0.529412  | 0.0588235 | 0.470588 | 0.294118 | 0.294118  | 0.406723 |\n",
      "|  5 | RandomForestClassifier 4      | 0.529412 | 0.470588 | 0.764706 | 0.470588 | 0.294118 | 0.647059 | 0.470588 | 0.294118 | 0.0588235 | 0.352941 | 0.235294 | 0.117647 | 0.0588235 | 0        | 0.352941 | 0.352941 | 0.823529 | 0.588235  | 0.176471 | 0.352941 | 0.705882 | 0.294118 | 0.411765 | 0.529412 | 0.470588 | 0.529412 | 0.0588235 | 0.294118 | 0.411765 | 0.411765 | 0.411765 | 0.529412  | 0         | 0.705882 | 0.529412  | 0.411765 | 0.235294 | 0.176471 | 0.470588 | 0.235294  | 0.352941 | 0.705882 | 0.411765 | 0.235294 | 0.705882 | 0.470588 | 0.176471 | 0.470588 | 0.411765 | 0.0588235 | 0.411765 | 0.647059 | 0.0588235 | 0.294118  | 0.294118 | 0         | 0.352941 | 0.588235  | 0.529412 | 0.705882 | 0.529412 | 0.411765 | 0.352941 | 0.764706 | 0.470588 | 0         | 0.588235  | 0.705882 | 0.294118 | 0.117647  | 0.390756 |\n",
      "|  6 | RandomForestClassifier 5      | 0.529412 | 0.647059 | 0.764706 | 0.470588 | 0.117647 | 0.588235 | 0.294118 | 0.235294 | 0.0588235 | 0.411765 | 0.235294 | 0.117647 | 0.0588235 | 0        | 0.294118 | 0.117647 | 0.882353 | 0.588235  | 0.176471 | 0.352941 | 0.352941 | 0.235294 | 0.117647 | 0.235294 | 0.470588 | 0.529412 | 0         | 0.294118 | 0.411765 | 0.647059 | 0.352941 | 0.117647  | 0         | 0.470588 | 0.0588235 | 0.411765 | 0.176471 | 0.176471 | 0        | 0.294118  | 0.352941 | 0.352941 | 0.411765 | 0.294118 | 0.470588 | 0.235294 | 0.411765 | 0.352941 | 0.294118 | 0.0588235 | 0.470588 | 0.294118 | 0.0588235 | 0.294118  | 0.294118 | 0         | 0.117647 | 0.823529  | 0.529412 | 0.588235 | 0.647059 | 0.529412 | 0.235294 | 0.647059 | 0.235294 | 0.0588235 | 0.588235  | 0.705882 | 0.294118 | 0         | 0.327731 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.764706 | 0.705882 | 0.529412 | 0.882353 | 0.529412 | 0.705882 | 0.588235 | 0.176471 | 0.352941  | 0.647059 | 0.411765 | 0.411765 | 0.352941  | 0.588235 | 0.411765 | 0.411765 | 0.647059 | 0.647059  | 0.529412 | 0.470588 | 0.470588 | 0.764706 | 0.705882 | 0.411765 | 0.764706 | 0.647059 | 0.705882  | 0.470588 | 0.588235 | 0.529412 | 0.294118 | 0.176471  | 0.352941  | 0.588235 | 0.588235  | 0.529412 | 0.411765 | 0.823529 | 0.882353 | 0.294118  | 0.647059 | 0.529412 | 0.352941 | 0.470588 | 0.705882 | 0.823529 | 0.294118 | 0.588235 | 0.588235 | 0.294118  | 0.411765 | 0.823529 | 0.705882  | 0.235294  | 0.588235 | 0.529412  | 0.117647 | 0.411765  | 0.647059 | 0.588235 | 0.352941 | 0.705882 | 0.705882 | 0.823529 | 0.352941 | 0.176471  | 0.470588  | 0.588235 | 0.411765 | 0.705882  | 0.534454 |\n",
      "|  8 | GradientBoostingClassifier 2  | 0.647059 | 0.705882 | 0.529412 | 0.823529 | 0.588235 | 0.764706 | 0.529412 | 0.294118 | 0.411765  | 0.764706 | 0.588235 | 0.117647 | 0.352941  | 0.823529 | 0.764706 | 0.647059 | 0.705882 | 0.647059  | 0.705882 | 0.705882 | 0.823529 | 0.764706 | 0.705882 | 0.647059 | 1        | 0.764706 | 0.823529  | 0.529412 | 0.411765 | 0.470588 | 0.588235 | 0.470588  | 0.352941  | 0.705882 | 0.529412  | 0.529412 | 0.411765 | 0.764706 | 0.588235 | 0.0588235 | 0.882353 | 0.705882 | 0.411765 | 0.352941 | 0.588235 | 0.941176 | 0.176471 | 0.588235 | 0.470588 | 0.352941  | 0.352941 | 0.764706 | 0.705882  | 0.117647  | 0.411765 | 0.882353  | 0.117647 | 0.470588  | 0.588235 | 0.764706 | 0.411765 | 0.647059 | 0.764706 | 0.470588 | 0.588235 | 0.294118  | 0.294118  | 0.647059 | 0.294118 | 0.470588  | 0.565546 |\n",
      "|  9 | GradientBoostingClassifier 3  | 0.588235 | 0.705882 | 0.588235 | 1        | 0.823529 | 0.823529 | 0.411765 | 0.294118 | 0.529412  | 0.647059 | 0.470588 | 0.235294 | 0.411765  | 0.647059 | 0.941176 | 0.470588 | 0.823529 | 0.647059  | 0.764706 | 0.588235 | 0.588235 | 0.882353 | 0.588235 | 0.823529 | 0.823529 | 0.529412 | 0.705882  | 0.411765 | 0.529412 | 0.470588 | 0.588235 | 0.588235  | 0.294118  | 0.705882 | 0.588235  | 0.529412 | 0.470588 | 0.764706 | 0.882353 | 0.294118  | 0.529412 | 0.705882 | 0.588235 | 0.235294 | 0.764706 | 0.823529 | 0.294118 | 0.764706 | 0.647059 | 0.294118  | 0.470588 | 0.764706 | 0.705882  | 0.0588235 | 0.411765 | 0.529412  | 0.117647 | 0.235294  | 0.588235 | 0.588235 | 0.352941 | 0.647059 | 0.882353 | 0.764706 | 0.352941 | 0.470588  | 0.411765  | 0.529412 | 0.529412 | 0.764706  | 0.57563  |\n",
      "| 10 | XGBClassifier 1               | 0.529412 | 0.647059 | 0.470588 | 0.941176 | 0.647059 | 0.705882 | 0.588235 | 0.294118 | 0.411765  | 0.705882 | 0.529412 | 0.294118 | 0.352941  | 0.529412 | 0.529412 | 0.705882 | 0.764706 | 0.647059  | 0.823529 | 0.588235 | 0.764706 | 0.529412 | 0.764706 | 0.588235 | 0.823529 | 0.588235 | 0.529412  | 0.647059 | 0.588235 | 0.647059 | 0.647059 | 0.352941  | 0.882353  | 0.764706 | 0.705882  | 0.529412 | 0.470588 | 0.882353 | 0.882353 | 0.294118  | 0.705882 | 0.647059 | 0.294118 | 0.235294 | 0.588235 | 0.882353 | 0.294118 | 0.647059 | 0.470588 | 0.294118  | 0.588235 | 0.647059 | 0.647059  | 0.235294  | 0.352941 | 0.470588  | 0.117647 | 0.529412  | 0.647059 | 0.647059 | 0.647059 | 0.705882 | 0.705882 | 0.647059 | 0.647059 | 0.647059  | 0.647059  | 0.529412 | 0.411765 | 0.705882  | 0.583193 |\n",
      "| 11 | XGBClassifier 2               | 0.588235 | 0.705882 | 0.470588 | 0.882353 | 0.647059 | 0.705882 | 0.529412 | 0.235294 | 0.352941  | 0.705882 | 0.411765 | 0.352941 | 0.294118  | 0.588235 | 0.411765 | 0.823529 | 0.764706 | 0.647059  | 0.764706 | 0.529412 | 0.529412 | 0.705882 | 0.647059 | 0.529412 | 0.529412 | 0.529412 | 0.588235  | 0.647059 | 0.529412 | 0.647059 | 0.823529 | 0.294118  | 0.470588  | 0.705882 | 0.823529  | 0.647059 | 0.411765 | 0.823529 | 0.882353 | 0.235294  | 0.764706 | 0.647059 | 0.235294 | 0.352941 | 0.529412 | 0.823529 | 0.294118 | 0.588235 | 0.588235 | 0.411765  | 0.529412 | 0.529412 | 0.705882  | 0.176471  | 0.352941 | 0.470588  | 0.117647 | 0.588235  | 0.705882 | 0.647059 | 0.411765 | 0.470588 | 0.882353 | 0.529412 | 0.470588 | 0.411765  | 0.352941  | 0.529412 | 0.411765 | 0.705882  | 0.552101 |\n",
      "| 12 | XGBClassifier 3               | 0.588235 | 0.588235 | 0.529412 | 0.823529 | 0.647059 | 0.705882 | 0.470588 | 0.235294 | 0.470588  | 0.764706 | 0.411765 | 0.352941 | 0.294118  | 0.647059 | 0.352941 | 0.647059 | 0.823529 | 0.647059  | 0.764706 | 0.529412 | 0.647059 | 0.470588 | 0.705882 | 0.529412 | 0.470588 | 0.647059 | 0.529412  | 0.588235 | 0.647059 | 0.647059 | 0.705882 | 0.176471  | 0.705882  | 0.764706 | 0.647059  | 0.588235 | 0.411765 | 0.764706 | 0.882353 | 0.235294  | 0.823529 | 0.647059 | 0.176471 | 0.235294 | 0.470588 | 0.941176 | 0.294118 | 0.647059 | 0.588235 | 0.294118  | 0.529412 | 0.411765 | 0.705882  | 0.235294  | 0.352941 | 0.470588  | 0.117647 | 0.411765  | 0.529412 | 0.647059 | 0.352941 | 0.705882 | 0.764706 | 0.588235 | 0.588235 | 0.588235  | 0.470588  | 0.647059 | 0.411765 | 0.705882  | 0.548739 |\n",
      "| 13 | XGBClassifier 4               | 0.529412 | 0.705882 | 0.588235 | 0.823529 | 0.764706 | 0.647059 | 0.529412 | 0.235294 | 0.352941  | 0.647059 | 0.411765 | 0.470588 | 0.352941  | 0.529412 | 0.294118 | 0.764706 | 0.705882 | 0.647059  | 0.764706 | 0.588235 | 0.764706 | 0.411765 | 0.823529 | 0.647059 | 0.588235 | 0.823529 | 0.529412  | 0.705882 | 0.588235 | 0.647059 | 0.529412 | 0.235294  | 0.411765  | 0.764706 | 0.823529  | 0.529412 | 0.235294 | 0.823529 | 0.882353 | 0.352941  | 0.647059 | 0.705882 | 0.235294 | 0.235294 | 0.588235 | 0.705882 | 0.294118 | 0.529412 | 0.647059 | 0.529412  | 0.588235 | 0.411765 | 0.705882  | 0.294118  | 0.411765 | 0.470588  | 0.117647 | 0.411765  | 0.411765 | 0.705882 | 0.411765 | 0.647059 | 0.764706 | 0.705882 | 0.352941 | 0.470588  | 0.176471  | 0.529412 | 0.470588 | 0.647059  | 0.547059 |\n",
      "| 14 | XGBClassifier 5               | 0.705882 | 0.705882 | 0.470588 | 0.882353 | 0.647059 | 0.647059 | 0.470588 | 0.352941 | 0.235294  | 0.823529 | 0.470588 | 0.352941 | 0.352941  | 0.764706 | 0.470588 | 0.764706 | 0.823529 | 0.647059  | 0.764706 | 0.588235 | 0.705882 | 0.705882 | 0.764706 | 0.705882 | 0.764706 | 0.764706 | 0.588235  | 0.588235 | 0.588235 | 0.647059 | 0.764706 | 0.470588  | 0.705882  | 0.823529 | 0.882353  | 0.705882 | 0.411765 | 0.882353 | 0.882353 | 0.294118  | 0.588235 | 0.529412 | 0.294118 | 0.294118 | 0.647059 | 0.823529 | 0.352941 | 0.588235 | 0.647059 | 0.294118  | 0.764706 | 0.705882 | 0.705882  | 0.235294  | 0.529412 | 0.470588  | 0.117647 | 0.647059  | 0.588235 | 0.588235 | 0.647059 | 0.470588 | 0.705882 | 0.823529 | 0.352941 | 0.529412  | 0.588235  | 0.529412 | 0.588235 | 0.705882  | 0.59916  |\n",
      "| 15 | XGBClassifier 6               | 0.529412 | 0.823529 | 0.470588 | 0.941176 | 0.882353 | 0.647059 | 0.470588 | 0.352941 | 0.411765  | 0.764706 | 0.470588 | 0.352941 | 0.294118  | 0.647059 | 0.529412 | 0.588235 | 0.882353 | 0.647059  | 0.882353 | 0.705882 | 0.764706 | 0.764706 | 0.823529 | 0.588235 | 0.764706 | 0.764706 | 0.647059  | 0.470588 | 0.764706 | 0.588235 | 0.764706 | 0.352941  | 0.705882  | 0.647059 | 0.764706  | 0.647059 | 0.470588 | 0.823529 | 0.882353 | 0.294118  | 0.529412 | 0.647059 | 0.411765 | 0.352941 | 0.647059 | 0.764706 | 0.294118 | 0.705882 | 0.411765 | 0.411765  | 0.647059 | 0.647059 | 0.647059  | 0.235294  | 0.647059 | 0.529412  | 0.117647 | 0.470588  | 0.529412 | 0.705882 | 0.529412 | 0.705882 | 0.705882 | 0.529412 | 0.647059 | 0.647059  | 0.411765  | 0.647059 | 0.470588 | 0.705882  | 0.59916  |\n",
      "| 16 | XGBRFClassifier 1             | 0.529412 | 0.411765 | 0.411765 | 0.470588 | 0.588235 | 0.705882 | 0.470588 | 0.529412 | 0.294118  | 0.647059 | 0.588235 | 0.176471 | 0.294118  | 0.588235 | 0.411765 | 0.294118 | 0.588235 | 0.529412  | 0.294118 | 0.529412 | 0.235294 | 0.294118 | 0.470588 | 0.411765 | 0.529412 | 0.705882 | 0.294118  | 0.588235 | 0.470588 | 0.588235 | 0.588235 | 0.117647  | 0.470588  | 0.588235 | 0.470588  | 0.470588 | 0.294118 | 0.588235 | 0.529412 | 0.294118  | 0.823529 | 0.764706 | 0.352941 | 0.352941 | 0.411765 | 0.941176 | 0.235294 | 0.529412 | 0.588235 | 0.411765  | 0.470588 | 0.764706 | 0.705882  | 0.176471  | 0.294118 | 0.529412  | 0.117647 | 0.588235  | 0.647059 | 0.235294 | 0.588235 | 0.294118 | 0.823529 | 0.470588 | 0.294118 | 0.176471  | 0.647059  | 0.705882 | 0.294118 | 0.176471  | 0.468067 |\n",
      "| 17 | XGBRFClassifier 2             | 0.529412 | 0.411765 | 0.411765 | 0.411765 | 0.588235 | 0.705882 | 0.470588 | 0.588235 | 0.294118  | 0.647059 | 0.588235 | 0.176471 | 0.294118  | 0.588235 | 0.411765 | 0.294118 | 0.588235 | 0.529412  | 0.294118 | 0.529412 | 0.235294 | 0.294118 | 0.470588 | 0.470588 | 0.529412 | 0.705882 | 0.352941  | 0.647059 | 0.470588 | 0.588235 | 0.588235 | 0.0588235 | 0.470588  | 0.588235 | 0.470588  | 0.470588 | 0.294118 | 0.529412 | 0.529412 | 0.294118  | 0.823529 | 0.823529 | 0.352941 | 0.235294 | 0.411765 | 0.941176 | 0.235294 | 0.588235 | 0.588235 | 0.411765  | 0.470588 | 0.764706 | 0.705882  | 0.176471  | 0.294118 | 0.470588  | 0.117647 | 0.588235  | 0.647059 | 0.235294 | 0.588235 | 0.352941 | 0.764706 | 0.529412 | 0.294118 | 0.176471  | 0.764706  | 0.705882 | 0.294118 | 0.176471  | 0.470588 |\n",
      "| 18 | XGBRFClassifier 3             | 0.529412 | 0.176471 | 0.294118 | 0.176471 | 0.235294 | 0.647059 | 0.588235 | 0.294118 | 0.647059  | 0.470588 | 0.411765 | 0.294118 | 0.294118  | 0.411765 | 0.411765 | 0.470588 | 0.588235 | 0.588235  | 0.176471 | 0.529412 | 0.294118 | 0.235294 | 0.705882 | 0.352941 | 0.294118 | 0.529412 | 0.176471  | 0.411765 | 0.411765 | 0.352941 | 0.294118 | 0.176471  | 0.588235  | 0.470588 | 0.529412  | 0.470588 | 0.411765 | 0.411765 | 0.529412 | 0.411765  | 0.941176 | 0.764706 | 0.352941 | 0.352941 | 0.588235 | 0.352941 | 0.411765 | 0.529412 | 0        | 0.352941  | 0.294118 | 0.647059 | 0.470588  | 0.294118  | 0.294118 | 0.411765  | 0.117647 | 0.117647  | 0.647059 | 0.235294 | 0.294118 | 0.176471 | 0.647059 | 0.529412 | 0.470588 | 0.117647  | 0.588235  | 0.470588 | 0.411765 | 0.235294  | 0.405882 |\n",
      "| 19 | XGBRFClassifier 4             | 0.588235 | 0.352941 | 0.352941 | 0.411765 | 0.647059 | 0.705882 | 0.588235 | 0.294118 | 0.294118  | 0.647059 | 0.647059 | 0.176471 | 0.294118  | 0.588235 | 0.529412 | 0.235294 | 0.588235 | 0.588235  | 0.176471 | 0.411765 | 0.352941 | 0.294118 | 0.411765 | 0.352941 | 0.352941 | 0.705882 | 0.411765  | 0.470588 | 0.529412 | 0.529412 | 0.352941 | 0         | 0.529412  | 0.764706 | 0.588235  | 0.470588 | 0.411765 | 0.588235 | 0.529412 | 0.294118  | 0.941176 | 0.764706 | 0.352941 | 0.235294 | 0.588235 | 0.823529 | 0.411765 | 0.529412 | 0.470588 | 0.352941  | 0.352941 | 0.647059 | 0.705882  | 0.294118  | 0.294118 | 0.529412  | 0.117647 | 0.470588  | 0.647059 | 0.176471 | 0.411765 | 0.176471 | 0.705882 | 0.411765 | 0.294118 | 0.411765  | 0.588235  | 0.529412 | 0.352941 | 0.176471  | 0.454622 |\n",
      "| 20 | XGBRFClassifier 5             | 0.470588 | 0.352941 | 0.411765 | 0.411765 | 0.588235 | 0.705882 | 0.588235 | 0.294118 | 0.294118  | 0.647059 | 0.647059 | 0.176471 | 0.294118  | 0.588235 | 0.470588 | 0.235294 | 0.647059 | 0.588235  | 0.294118 | 0.411765 | 0.294118 | 0.235294 | 0.470588 | 0.176471 | 0.529412 | 0.705882 | 0.352941  | 0.588235 | 0.529412 | 0.588235 | 0.411765 | 0         | 0.470588  | 0.705882 | 0.470588  | 0.529412 | 0.294118 | 0.588235 | 0.529412 | 0.294118  | 0.882353 | 0.764706 | 0.352941 | 0.352941 | 0.352941 | 0.941176 | 0.294118 | 0.529412 | 0.588235 | 0.411765  | 0.411765 | 0.764706 | 0.705882  | 0.294118  | 0.294118 | 0.529412  | 0.117647 | 0.588235  | 0.647059 | 0.294118 | 0.470588 | 0.235294 | 0.823529 | 0.647059 | 0.294118 | 0.235294  | 0.588235  | 0.705882 | 0.294118 | 0.176471  | 0.463866 |\n",
      "| 21 | XGBRFClassifier 6             | 0.588235 | 0.352941 | 0.411765 | 0.411765 | 0.588235 | 0.705882 | 0.470588 | 0.529412 | 0.294118  | 0.588235 | 0.588235 | 0.117647 | 0.294118  | 0.588235 | 0.470588 | 0.176471 | 0.588235 | 0.529412  | 0.294118 | 0.529412 | 0.235294 | 0.294118 | 0.470588 | 0.411765 | 0.529412 | 0.705882 | 0.352941  | 0.588235 | 0.470588 | 0.588235 | 0.529412 | 0.117647  | 0.352941  | 0.588235 | 0.470588  | 0.529412 | 0.294118 | 0.470588 | 0.529412 | 0.294118  | 0.823529 | 0.764706 | 0.352941 | 0.352941 | 0.352941 | 0.941176 | 0.176471 | 0.529412 | 0.588235 | 0.411765  | 0.470588 | 0.764706 | 0.705882  | 0.176471  | 0.294118 | 0.529412  | 0.117647 | 0.588235  | 0.647059 | 0.235294 | 0.588235 | 0.235294 | 0.823529 | 0.470588 | 0.294118 | 0.294118  | 0.705882  | 0.705882 | 0.352941 | 0.176471  | 0.463025 |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.588235 | 0.941176 | 0.470588 | 0.470588 | 0.823529 | 0.647059 | 0.235294 | 0.647059 | 0.176471  | 0.411765 | 0.411765 | 0.176471 | 0.352941  | 0.470588 | 0.823529 | 0.647059 | 0.823529 | 0.588235  | 0.882353 | 0.705882 | 0.764706 | 0.823529 | 0.823529 | 0.588235 | 0.882353 | 0.882353 | 0.823529  | 0.529412 | 0.529412 | 0.705882 | 0.588235 | 0.411765  | 0.941176  | 0.882353 | 0.705882  | 0.647059 | 0.470588 | 0.823529 | 0.588235 | 0.235294  | 0.294118 | 0.411765 | 0.588235 | 0.235294 | 0.764706 | 0.941176 | 0.352941 | 0.588235 | 0.588235 | 0.470588  | 0.411765 | 0.764706 | 0.705882  | 0.352941  | 0.411765 | 0.529412  | 0.117647 | 0.117647  | 0.588235 | 0.588235 | 0.352941 | 0.764706 | 0.705882 | 0.352941 | 0.529412 | 0.470588  | 0.823529  | 0.588235 | 0.529412 | 0.647059  | 0.578992 |\n",
      "| 23 | GradientBoostingClassifier 2S | 0.352941 | 0.764706 | 0.411765 | 0.705882 | 0.882353 | 0.647059 | 0.352941 | 0.529412 | 0.352941  | 0.529412 | 0.470588 | 0.294118 | 0.411765  | 0.588235 | 0.882353 | 0.705882 | 0.941176 | 0.588235  | 1        | 0.823529 | 0.588235 | 0.823529 | 0.588235 | 0.705882 | 0.882353 | 0.882353 | 0.764706  | 0.705882 | 0.823529 | 0.764706 | 0.352941 | 0.823529  | 0.882353  | 0.823529 | 0.882353  | 0.882353 | 0.529412 | 0.823529 | 0.823529 | 0.117647  | 0.176471 | 0.705882 | 0.941176 | 0.235294 | 0.705882 | 0.411765 | 0.470588 | 0.588235 | 0.647059 | 0.352941  | 0.176471 | 0.941176 | 0.705882  | 0.411765  | 0.352941 | 0.352941  | 0.176471 | 0.235294  | 0.588235 | 0.764706 | 0.588235 | 0.823529 | 0.705882 | 0.470588 | 0.705882 | 0.588235  | 0.588235  | 0.529412 | 0.294118 | 0.588235  | 0.607563 |\n",
      "| 24 | GradientBoostingClassifier 3S | 0.352941 | 0.764706 | 0.294118 | 0.647059 | 1        | 0.647059 | 0.176471 | 0.588235 | 0.117647  | 0.529412 | 0.529412 | 0.294118 | 0.352941  | 0.470588 | 0.823529 | 0.764706 | 0.882353 | 0.647059  | 0.941176 | 0.882353 | 0.588235 | 0.823529 | 0.764706 | 0.882353 | 0.823529 | 0.941176 | 0.823529  | 0.705882 | 0.823529 | 0.764706 | 0.411765 | 0.823529  | 0.764706  | 0.823529 | 0.764706  | 1        | 0.411765 | 0.823529 | 0.764706 | 0.529412  | 0.294118 | 0.764706 | 0.588235 | 0.294118 | 0.764706 | 0.529412 | 0.411765 | 0.588235 | 0.823529 | 0.294118  | 0.352941 | 0.882353 | 0.705882  | 0.705882  | 0.352941 | 0.588235  | 0.294118 | 0.0588235 | 0.588235 | 0.705882 | 0.470588 | 0.764706 | 0.705882 | 0.411765 | 0.588235 | 0.705882  | 0.764706  | 0.705882 | 0.235294 | 0.705882  | 0.620168 |\n",
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+-----------+-----------+----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+----------+----------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers2 = [\"Classifier type\", ] + step_headers\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "accuracy_df = pd.DataFrame(score_df['Accuracy'].tolist(), index=score_df.index, columns=step_headers)\n",
    "score_df = score_df.drop('Accuracy', 1)\n",
    "f_out = pd.merge(score_df, accuracy_df, how='left', left_index=True, right_index=True)\n",
    "f_out['mean'] = f_out.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out, headers2, tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "f_out.to_csv(filename_to_export, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'DecisionTreeClassifier 1': [0.92,\n              0.9295282469423413,\n              0.933679354094579,\n              0.9189034837235865,\n              0.9298642533936652,\n              0.9165266106442577,\n              0.9278579356270811,\n              0.9213853765805388,\n              0.9286492374727668,\n              0.9336211548839719,\n              0.9411764705882353,\n              0.9321674615792263,\n              0.9296218487394958,\n              0.9172306090577824,\n              0.9055727554179567,\n              0.9166240409207161,\n              0.9259634888438134,\n              0.9270990447461036,\n              0.9252243270189432,\n              0.921403855659911,\n              0.9279411764705883,\n              0.9431210500729217,\n              0.929122468659595,\n              0.9005260640841702,\n              0.9198292220113852,\n              0.9336470588235294,\n              0.9206349206349206,\n              0.9407132931912923,\n              0.9356617647058824,\n              0.9420884632922937,\n              0.9402714932126697,\n              0.9443197126178716,\n              0.9322638146167558,\n              0.9332153914197258,\n              0.9297629499561019,\n              0.9324618736383442,\n              0.9303633217993079,\n              0.9291541434091884,\n              0.8913043478260869,\n              0.8963182395260262,\n              0.8886554621848739,\n              0.8848560700876095,\n              0.9299917149958575,\n              0.8897573015220074,\n              0.8876633986928104,\n              0.886815415821501,\n              0.8851732473811442,\n              0.881952781112445,\n              0.8819554848966613,\n              0.9206474536123174,\n              0.9196078431372549,\n              0.8784573432021815,\n              0.9125386996904025,\n              0.908881199538639,\n              0.9117647058823529,\n              0.8827324478178368,\n              0.8996983408748115,\n              0.8527538403896591,\n              0.8782576321667908,\n              0.8261191268960414,\n              0.8338235294117647,\n              0.9006211180124224,\n              0.9052287581699346,\n              0.8513172140021653,\n              0.8472022955523673,\n              0.8848484848484849,\n              0.8508150248051027,\n              0.8594575554772808,\n              0.8494397759103641,\n              0.8430212321615037],\n             'DecisionTreeClassifier 2': [1.0,\n              1.0,\n              0.9994232987312572,\n              0.9988577955454027,\n              0.998868778280543,\n              1.0,\n              1.0,\n              1.0,\n              0.9983660130718954,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              0.999484004127967,\n              1.0,\n              0.9969574036511156,\n              0.9969834087481146,\n              0.9985044865403788,\n              0.9985170538803757,\n              0.9985294117647059,\n              0.9980554205153136,\n              0.9980713596914176,\n              1.0,\n              0.9990512333965844,\n              0.9990588235294118,\n              0.9981325863678805,\n              0.999073645206114,\n              0.9990808823529411,\n              0.9995440036479708,\n              1.0,\n              1.0,\n              0.9986631016042781,\n              0.9986731534719151,\n              1.0,\n              1.0,\n              1.0,\n              0.9952769428939459,\n              0.9995737425404945,\n              0.9995768091409225,\n              0.9995798319327731,\n              0.999165623696287,\n              0.9987572493786246,\n              0.9979432332373509,\n              0.9995915032679739,\n              0.9995943204868154,\n              0.9987912973408541,\n              0.9987995198079231,\n              1.0,\n              1.0,\n              1.0,\n              0.9953252824308532,\n              1.0,\n              1.0,\n              1.0,\n              0.9992409867172676,\n              1.0,\n              0.997002622705133,\n              0.9992553983618764,\n              0.9992600813910469,\n              0.993014705882353,\n              1.0,\n              1.0,\n              0.9931432695777698,\n              1.0,\n              0.9932263814616756,\n              0.9932671863926293,\n              0.9996477632969355,\n              0.9814425770308123,\n              0.9989557953358859],\n             'DecisionTreeClassifier 3': [0.8323529411764706,\n              0.8124635993011066,\n              0.8137254901960784,\n              0.7966876070816676,\n              0.7997737556561086,\n              0.8156862745098039,\n              0.7863485016648168,\n              0.8581638262781748,\n              0.8001089324618736,\n              0.8650836481381543,\n              0.8390374331550802,\n              0.7774244833068362,\n              0.8004201680672269,\n              0.7928162415408642,\n              0.8095975232198143,\n              0.8271099744245525,\n              0.8245436105476673,\n              0.836098541980895,\n              0.7831505483549352,\n              0.7874443895205141,\n              0.8088235294117647,\n              0.7744287797763734,\n              0.7463837994214079,\n              0.8589191774270684,\n              0.7855787476280834,\n              0.7807058823529411,\n              0.8632119514472456,\n              0.7545159796201946,\n              0.7863051470588235,\n              0.7268581851345189,\n              0.7986425339366516,\n              0.740907049842838,\n              0.820855614973262,\n              0.7598407784166298,\n              0.7699736611062336,\n              0.7969498910675381,\n              0.8057958477508651,\n              0.802490339201374,\n              0.6901108269394715,\n              0.705459162082099,\n              0.7352941176470589,\n              0.7313308302044222,\n              0.7054681027340514,\n              0.8074866310160428,\n              0.7724673202614379,\n              0.7663286004056795,\n              0.7074939564867043,\n              0.7863145258103241,\n              0.7293322734499205,\n              0.7220686932491117,\n              0.7282352941176471,\n              0.7460070120763537,\n              0.7240712074303406,\n              0.707035755478662,\n              0.7223071046600459,\n              0.6478178368121442,\n              0.6904223227752639,\n              0.7122517796927689,\n              0.7520476545048399,\n              0.7343692193858675,\n              0.7345588235294118,\n              0.7186700767263428,\n              0.7022512708787219,\n              0.7289787080476362,\n              0.7883787661406025,\n              0.7985739750445633,\n              0.7834868887313962,\n              0.7083480098626277,\n              0.7689075630252101,\n              0.7796728158719108],\n             'DecisionTreeClassifier 4': [0.9129411764705883,\n              0.8980780430984275,\n              0.9002306805074971,\n              0.9120502569960023,\n              0.8981900452488688,\n              0.9103641456582633,\n              0.9317425083240843,\n              0.9285321605277626,\n              0.923202614379085,\n              0.9217485159201295,\n              0.9245989304812834,\n              0.9242183359830418,\n              0.9122899159663865,\n              0.9135866736074961,\n              0.9035087719298246,\n              0.9069053708439898,\n              0.9183569979716024,\n              0.9180492709904474,\n              0.9177467597208375,\n              0.9144834404349975,\n              0.8980392156862745,\n              0.9163830821584832,\n              0.9170684667309547,\n              0.8952654232424677,\n              0.9122390891840607,\n              0.9341176470588235,\n              0.9197012138188608,\n              0.9263547938860583,\n              0.9301470588235294,\n              0.9316005471956225,\n              0.9298642533936652,\n              0.9317467444993265,\n              0.9135472370766489,\n              0.9243697478991597,\n              0.9209833187006146,\n              0.9137254901960784,\n              0.9147923875432526,\n              0.9149849720910261,\n              0.8712702472293266,\n              0.8679644519678374,\n              0.846218487394958,\n              0.8410513141426783,\n              0.8956089478044739,\n              0.8745372274784039,\n              0.8758169934640523,\n              0.8746450304259635,\n              0.9008863819500403,\n              0.907563025210084,\n              0.8863275039745628,\n              0.8981444926964074,\n              0.9019607843137255,\n              0.8776782236073237,\n              0.8773219814241486,\n              0.8858131487889274,\n              0.8747135217723453,\n              0.8724857685009487,\n              0.9030920060331825,\n              0.8340202322967404,\n              0.887192851824274,\n              0.8930817610062893,\n              0.881985294117647,\n              0.8786993058092802,\n              0.8801742919389978,\n              0.8783832551425478,\n              0.819583931133429,\n              0.8106951871657754,\n              0.8621545003543586,\n              0.8150757308911588,\n              0.8095238095238095,\n              0.8002088409328229],\n             'DecisionTreeClassifier 5': [0.9988235294117647,\n              0.9924286546301689,\n              0.9930795847750865,\n              1.0,\n              0.9932126696832579,\n              0.9938375350140056,\n              0.9983351831298557,\n              0.9983507421660253,\n              0.9945533769063181,\n              0.9962223421478683,\n              0.9983957219251337,\n              0.9984101748807631,\n              0.993172268907563,\n              0.9927121290994274,\n              0.9860681114551083,\n              0.989769820971867,\n              0.9903651115618661,\n              0.9904474610356964,\n              0.9930209371884346,\n              0.9930795847750865,\n              0.9926470588235294,\n              0.9970831307729704,\n              0.9971070395371263,\n              0.9985652797704447,\n              0.9943074003795066,\n              0.9976470588235294,\n              0.9920634920634921,\n              0.9967577582213988,\n              0.9981617647058824,\n              0.9990880072959416,\n              0.997737556561086,\n              0.9982038616973506,\n              0.9955436720142602,\n              0.994250331711632,\n              0.995171202809482,\n              0.9938997821350762,\n              0.995242214532872,\n              0.9939888364104766,\n              0.9833759590792839,\n              0.9843419382141346,\n              0.9882352941176471,\n              0.9670421360033375,\n              0.9780447390223695,\n              0.9864253393665159,\n              0.9938725490196079,\n              0.9939148073022313,\n              0.984689766317486,\n              0.9899959983993597,\n              0.9972178060413355,\n              0.9818397157520726,\n              0.9890196078431372,\n              0.9906505648617062,\n              0.9907120743034056,\n              0.9769319492502884,\n              0.9778456837280367,\n              0.9912713472485768,\n              0.9913273001508296,\n              0.985013113525665,\n              0.9929262844378257,\n              0.9948205697373289,\n              0.9915441176470589,\n              0.9923273657289002,\n              0.9967320261437909,\n              0.9913388668350775,\n              0.9802725968436155,\n              0.9828877005347594,\n              0.9844082211197732,\n              0.9852060584712927,\n              0.9845938375350141,\n              0.9912982944657153],\n             'RandomForestClassifier 4': [0.6276470588235294,\n              0.6208503203261503,\n              0.6141868512110726,\n              0.6213592233009708,\n              0.6119909502262444,\n              0.6252100840336134,\n              0.6315205327413984,\n              0.6311159978009896,\n              0.6345315904139434,\n              0.631948192120885,\n              0.6347593582887701,\n              0.6369899311075782,\n              0.6355042016806722,\n              0.6283185840707964,\n              0.6331269349845201,\n              0.6240409207161125,\n              0.6272819472616633,\n              0.6078431372549019,\n              0.6096709870388833,\n              0.6124567474048442,\n              0.621078431372549,\n              0.6198347107438017,\n              0.6152362584378014,\n              0.615973218555715,\n              0.611954459203036,\n              0.6112941176470589,\n              0.6050420168067226,\n              0.6164891153311718,\n              0.6135110294117647,\n              0.6041951664386684,\n              0.6004524886877828,\n              0.6012572968118545,\n              0.6002673796791443,\n              0.6147722246793454,\n              0.6136962247585601,\n              0.5943355119825708,\n              0.5938581314878892,\n              0.5916702447402319,\n              0.592924126172208,\n              0.6115107913669064,\n              0.6063025210084033,\n              0.5949103045473508,\n              0.5940347970173985,\n              0.6001645413410119,\n              0.6004901960784313,\n              0.5935091277890466,\n              0.5922643029814666,\n              0.5930372148859544,\n              0.5878378378378378,\n              0.5945519147256217,\n              0.5976470588235294,\n              0.596805609661083,\n              0.5870743034055728,\n              0.5978469819300269,\n              0.5897631779984721,\n              0.5901328273244781,\n              0.6029411764705882,\n              0.6039715249156987,\n              0.5994043186895012,\n              0.5956344802071772,\n              0.5970588235294118,\n              0.5831202046035806,\n              0.5820624546114742,\n              0.588957055214724,\n              0.5846484935437589,\n              0.5832442067736185,\n              0.5843373493975904,\n              0.5861218738992603,\n              0.5833333333333334,\n              0.5795335885833623],\n             'RandomForestClassifier 5': [0.5488235294117647,\n              0.5270821199767035,\n              0.5288350634371396,\n              0.5316961736150772,\n              0.5192307692307693,\n              0.5428571428571428,\n              0.5577136514983352,\n              0.5415063221550303,\n              0.5430283224400871,\n              0.5472207231516459,\n              0.5631016042780749,\n              0.5506094329623742,\n              0.5593487394957983,\n              0.5523165018219677,\n              0.567079463364293,\n              0.5483375959079284,\n              0.5679513184584178,\n              0.5575666163901458,\n              0.5603190428713859,\n              0.5516559565002471,\n              0.5632352941176471,\n              0.5556635877491493,\n              0.5424300867888139,\n              0.5518890483022477,\n              0.5384250474383302,\n              0.5581176470588235,\n              0.5443510737628384,\n              0.5585919407132932,\n              0.5556066176470589,\n              0.5389876880984952,\n              0.5411764705882353,\n              0.5383924562191289,\n              0.5311942959001783,\n              0.5532950022114109,\n              0.5364354697102721,\n              0.5281045751633987,\n              0.5553633217993079,\n              0.5328467153284672,\n              0.5383631713554987,\n              0.5531104528142192,\n              0.5436974789915966,\n              0.5469336670838548,\n              0.5389395194697597,\n              0.5475113122171946,\n              0.551062091503268,\n              0.5419878296146045,\n              0.5273972602739726,\n              0.531812725090036,\n              0.5286168521462639,\n              0.5294117647058824,\n              0.5458823529411765,\n              0.5383716400467472,\n              0.5414086687306502,\n              0.540561322568243,\n              0.5320855614973262,\n              0.5335863377609108,\n              0.5339366515837104,\n              0.560134881978269,\n              0.5487714072970961,\n              0.5334813170551239,\n              0.5448529411764705,\n              0.5407380343441724,\n              0.5337690631808278,\n              0.5326596896427283,\n              0.5355093256814921,\n              0.5176470588235295,\n              0.5290574060949681,\n              0.5336386051426558,\n              0.521358543417367,\n              0.5019143752175427],\n             'GradientBoostingClassifier 1': [0.9394117647058824,\n              0.9341875364006988,\n              0.9388696655132641,\n              0.9354654483152485,\n              0.9326923076923077,\n              0.9411764705882353,\n              0.935072142064373,\n              0.9334799340296867,\n              0.9340958605664488,\n              0.9336211548839719,\n              0.9363636363636364,\n              0.9401165871754107,\n              0.9327731092436975,\n              0.9349297241020302,\n              0.93343653250774,\n              0.9345268542199489,\n              0.9300202839756593,\n              0.930115635997989,\n              0.9312063808574277,\n              0.9362333168561542,\n              0.9343137254901961,\n              0.932912007778318,\n              0.9329797492767599,\n              0.9282639885222381,\n              0.9297912713472486,\n              0.9317647058823529,\n              0.9299719887955182,\n              0.9319129226493748,\n              0.9301470588235294,\n              0.9316005471956225,\n              0.9334841628959276,\n              0.9357880556802873,\n              0.9313725490196079,\n              0.9252543122512162,\n              0.9253731343283582,\n              0.9281045751633987,\n              0.9320934256055363,\n              0.9231429798196651,\n              0.9262574595055414,\n              0.9217096910706729,\n              0.9105042016806723,\n              0.9211514392991239,\n              0.9179784589892295,\n              0.9193747429041547,\n              0.9142156862745098,\n              0.9188640973630832,\n              0.9125705076551168,\n              0.907563025210084,\n              0.9093799682034976,\n              0.9072246348203711,\n              0.9145098039215687,\n              0.9084534476042072,\n              0.9109907120743034,\n              0.9065743944636678,\n              0.9079449961802903,\n              0.9104364326375711,\n              0.9095022624434389,\n              0.9067066316972648,\n              0.8983618763961281,\n              0.9056603773584906,\n              0.9036764705882353,\n              0.9017172086225794,\n              0.8986928104575164,\n              0.8978708047636232,\n              0.8999282639885222,\n              0.899108734402852,\n              0.8968816442239547,\n              0.8989080662205001,\n              0.8995098039215687,\n              0.8931430560389837],\n             'GradientBoostingClassifier 2': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'GradientBoostingClassifier 3': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 1': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 2': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 3': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 4': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 5': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBClassifier 6': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'XGBRFClassifier 1': [0.9935294117647059,\n              0.9947582993593477,\n              0.9919261822376009,\n              0.9942889777270132,\n              0.9920814479638009,\n              0.9904761904761905,\n              0.9916759156492786,\n              0.9928532160527762,\n              0.9912854030501089,\n              0.9913653534808419,\n              0.9919786096256684,\n              0.9931107578166402,\n              0.9910714285714286,\n              0.9921915668922436,\n              0.9922600619195047,\n              0.9907928388746803,\n              0.9939148073022313,\n              0.9929612870789342,\n              0.9920239282153539,\n              0.9920909540286703,\n              0.990686274509804,\n              0.9912493923189111,\n              0.991321118611379,\n              0.991869918699187,\n              0.99146110056926,\n              0.9924705882352941,\n              0.9911297852474323,\n              0.9916628068550255,\n              0.9931066176470589,\n              0.9936160510715915,\n              0.9923076923076923,\n              0.9932644813650651,\n              0.9906417112299465,\n              0.9911543564794338,\n              0.9916593503072871,\n              0.9908496732026144,\n              0.9883217993079585,\n              0.9901245169600686,\n              0.989769820971867,\n              0.988573846804909,\n              0.9873949579831933,\n              0.9891531080517313,\n              0.9871582435791217,\n              0.9868366927190456,\n              0.9873366013071896,\n              0.9890466531440162,\n              0.9858984689766318,\n              0.990796318527411,\n              0.9900635930047694,\n              0.9901302803000395,\n              0.9886274509803922,\n              0.9894818854694195,\n              0.9926470588235294,\n              0.9915417147251058,\n              0.9912146676852559,\n              0.9901328273244782,\n              0.9905731523378583,\n              0.991007868115399,\n              0.9918093819806404,\n              0.9922308546059934,\n              0.9933823529411765,\n              0.9923273657289002,\n              0.9912854030501089,\n              0.991699747383616,\n              0.990674318507891,\n              0.9907308377896613,\n              0.9922041105598866,\n              0.9897851356111307,\n              0.9898459383753502,\n              0.9923424991298294],\n             'XGBRFClassifier 2': [0.9970588235294118,\n              0.9982527664531159,\n              0.9965397923875432,\n              0.9977155910908052,\n              0.9971719457013575,\n              0.9955182072829132,\n              0.9966703662597114,\n              0.9967014843320505,\n              0.9950980392156863,\n              0.9978413383702105,\n              0.9983957219251337,\n              0.9978802331743508,\n              0.9994747899159664,\n              0.9973971889640812,\n              0.9953560371517027,\n              0.9974424552429667,\n              0.9979716024340771,\n              0.9984917043740573,\n              0.9980059820538385,\n              0.9970341077607514,\n              0.9985294117647059,\n              0.997569275644142,\n              0.9951783992285439,\n              0.9976087996174079,\n              0.9971537001897534,\n              0.9967058823529412,\n              0.9957983193277311,\n              0.9962945808244558,\n              0.9967830882352942,\n              0.9972640218878249,\n              0.9959276018099548,\n              0.9968567579703638,\n              0.9964349376114082,\n              0.9969040247678018,\n              0.9978050921861282,\n              0.9969498910675382,\n              0.9961072664359861,\n              0.9974237870330614,\n              0.9961636828644501,\n              0.996191282268303,\n              0.9957983193277311,\n              0.9949937421777222,\n              0.9946147473073736,\n              0.9950637597696421,\n              0.994281045751634,\n              0.9963488843813387,\n              0.9943593875906527,\n              0.9959983993597439,\n              0.996422893481717,\n              0.9960521121200158,\n              0.9964705882352941,\n              0.9961044020257109,\n              0.9976780185758514,\n              0.9957708573625529,\n              0.9969442322383499,\n              0.996584440227704,\n              0.996606334841629,\n              0.9966279505432747,\n              0.996276991809382,\n              0.9959304476507584,\n              0.9963235294117647,\n              0.9967117281695287,\n              0.9963689179375453,\n              0.9967520750631541,\n              0.9949784791965567,\n              0.9967914438502674,\n              0.9961020552799433,\n              0.9950686861570975,\n              0.9964985994397759,\n              0.9968673860076575],\n             'XGBRFClassifier 3': [0.658235294117647,\n              0.6598718695398952,\n              0.6464821222606689,\n              0.6493432324386065,\n              0.6515837104072398,\n              0.66890756302521,\n              0.6703662597114317,\n              0.6619021440351842,\n              0.6595860566448801,\n              0.6540744738262277,\n              0.6625668449197861,\n              0.6640169581346052,\n              0.664390756302521,\n              0.6678813118167621,\n              0.6615067079463365,\n              0.659846547314578,\n              0.6612576064908722,\n              0.6711915535444947,\n              0.6799601196410767,\n              0.6786950074147305,\n              0.6681372549019607,\n              0.667963052989791,\n              0.6663452266152362,\n              0.6580583452893353,\n              0.6522770398481973,\n              0.6677647058823529,\n              0.6563958916900093,\n              0.6632700324224178,\n              0.6645220588235294,\n              0.6712266301869585,\n              0.6610859728506787,\n              0.6722047597665021,\n              0.6631016042780749,\n              0.6585581601061478,\n              0.6628621597892889,\n              0.661437908496732,\n              0.6617647058823529,\n              0.6526406182911121,\n              0.6521739130434783,\n              0.6542530681337283,\n              0.6495798319327731,\n              0.637880684188569,\n              0.644159072079536,\n              0.6363636363636364,\n              0.6315359477124183,\n              0.6413793103448275,\n              0.6410153102336825,\n              0.636654661864746,\n              0.6498410174880763,\n              0.6435057244374259,\n              0.6423529411764706,\n              0.6497857421114142,\n              0.6389318885448917,\n              0.6485966935793925,\n              0.6432391138273491,\n              0.6497153700189753,\n              0.6417797888386124,\n              0.6508055451479955,\n              0.6433358153387937,\n              0.634850166481687,\n              0.6275735294117647,\n              0.6357325538911217,\n              0.6285403050108932,\n              0.6178274990977987,\n              0.6176470588235294,\n              0.6242424242424243,\n              0.6162296243798724,\n              0.6061993659739345,\n              0.6109943977591037,\n              0.6143404107205013],\n             'XGBRFClassifier 4': [0.8676470588235294,\n              0.8707047175305765,\n              0.8748558246828143,\n              0.8686464877213022,\n              0.871606334841629,\n              0.8717086834733894,\n              0.8740288568257492,\n              0.8647608576140736,\n              0.8687363834422658,\n              0.8715596330275229,\n              0.8705882352941177,\n              0.8691043985161632,\n              0.8644957983193278,\n              0.8604893284747527,\n              0.8622291021671826,\n              0.8618925831202046,\n              0.8635902636916836,\n              0.8687782805429864,\n              0.8673978065802592,\n              0.8690064260998517,\n              0.8666666666666667,\n              0.8653378706854643,\n              0.8702989392478303,\n              0.8641798182687709,\n              0.8638519924098672,\n              0.8649411764705882,\n              0.8613445378151261,\n              0.8633626679018064,\n              0.8653492647058824,\n              0.8677610579115367,\n              0.8647058823529412,\n              0.8702290076335878,\n              0.8587344028520499,\n              0.8589119858469704,\n              0.8555750658472344,\n              0.8583877995642701,\n              0.8546712802768166,\n              0.8484328037784457,\n              0.8444160272804774,\n              0.8345323741007195,\n              0.8361344537815126,\n              0.8356278681685441,\n              0.835956917978459,\n              0.8408062525709584,\n              0.8349673202614379,\n              0.8365111561866125,\n              0.83883964544722,\n              0.8415366146458584,\n              0.8449920508744038,\n              0.8456375838926175,\n              0.8458823529411764,\n              0.8472925594078691,\n              0.8401702786377709,\n              0.843521722414456,\n              0.838044308632544,\n              0.8398481973434535,\n              0.8299396681749623,\n              0.839640314724616,\n              0.8466120625465376,\n              0.8412874583795783,\n              0.8430147058823529,\n              0.8432590427475338,\n              0.840958605664488,\n              0.8390472753518585,\n              0.8400286944045912,\n              0.8445632798573975,\n              0.8394755492558469,\n              0.8400845368087355,\n              0.8378851540616247,\n              0.8395405499477898],\n             'XGBRFClassifier 5': [0.9794117647058823,\n              0.9813628421665695,\n              0.9803921568627451,\n              0.9771559109080525,\n              0.9768099547511312,\n              0.9764705882352941,\n              0.9778024417314095,\n              0.9780098955470038,\n              0.9754901960784313,\n              0.9757150566648678,\n              0.9796791443850268,\n              0.9814520402755696,\n              0.9784663865546218,\n              0.9786569495054659,\n              0.978328173374613,\n              0.9749360613810741,\n              0.9761663286004056,\n              0.9793866264454499,\n              0.9810568295114656,\n              0.9817103311913,\n              0.9823529411764705,\n              0.9795819154107924,\n              0.9792671166827387,\n              0.9794356767097083,\n              0.9743833017077799,\n              0.9811764705882353,\n              0.9757236227824463,\n              0.9768411301528486,\n              0.9811580882352942,\n              0.9817601459188326,\n              0.9773755656108597,\n              0.9775482712168837,\n              0.9754901960784313,\n              0.9787704555506413,\n              0.9762949956101844,\n              0.9755991285403051,\n              0.9779411764705882,\n              0.9708029197080292,\n              0.9714407502131287,\n              0.9707998307236564,\n              0.9697478991596639,\n              0.9737171464330413,\n              0.9705882352941176,\n              0.9728506787330317,\n              0.9734477124183006,\n              0.9740365111561866,\n              0.9734085414987913,\n              0.9747899159663865,\n              0.9773449920508744,\n              0.9751283063560995,\n              0.9752941176470589,\n              0.9774055317491235,\n              0.9756191950464397,\n              0.9773164167627836,\n              0.975553857906799,\n              0.9768500948766603,\n              0.9739819004524887,\n              0.9763956538029225,\n              0.9810126582278481,\n              0.9800221975582686,\n              0.9786764705882353,\n              0.978808914870296,\n              0.9771241830065359,\n              0.9769036448935402,\n              0.973816355810617,\n              0.9771836007130125,\n              0.9776754075124026,\n              0.9732300105671011,\n              0.9737394957983193,\n              0.9752871562826314],\n             'XGBRFClassifier 6': [0.9758823529411764,\n              0.9842748980780431,\n              0.9803921568627451,\n              0.981724728726442,\n              0.9773755656108597,\n              0.9781512605042016,\n              0.9750277469478358,\n              0.977460142935679,\n              0.9771241830065359,\n              0.9740960604425256,\n              0.9770053475935829,\n              0.9819819819819819,\n              0.9789915966386554,\n              0.9786569495054659,\n              0.9767801857585139,\n              0.9728900255754476,\n              0.9751521298174443,\n              0.9798893916540975,\n              0.9805583250249252,\n              0.9826989619377162,\n              0.9794117647058823,\n              0.9795819154107924,\n              0.9773384763741563,\n              0.9770444763271162,\n              0.9748576850094877,\n              0.9783529411764705,\n              0.9761904761904762,\n              0.9749884205650764,\n              0.9779411764705882,\n              0.9808481532147743,\n              0.9751131221719457,\n              0.9811405478221823,\n              0.9754901960784313,\n              0.9761167624944714,\n              0.9727831431079894,\n              0.9742919389978214,\n              0.971885813148789,\n              0.9708029197080292,\n              0.9701619778346121,\n              0.9712230215827338,\n              0.9689075630252101,\n              0.9678765123070505,\n              0.9701739850869926,\n              0.9687371452077335,\n              0.9714052287581699,\n              0.9744421906693712,\n              0.9746172441579372,\n              0.9735894357743097,\n              0.9745627980922098,\n              0.9735491512041058,\n              0.9749019607843137,\n              0.9762368523568368,\n              0.9756191950464397,\n              0.9734717416378316,\n              0.9751718869365928,\n              0.9726755218216319,\n              0.9717194570135747,\n              0.9745222929936306,\n              0.9798957557706627,\n              0.9763226045135035,\n              0.9772058823529411,\n              0.9769820971867008,\n              0.9778503994190269,\n              0.9776254059906171,\n              0.9730989956958394,\n              0.9754010695187165,\n              0.9751948972360028,\n              0.9732300105671011,\n              0.9733893557422969,\n              0.9756352245040028],\n             'GradientBoostingClassifier 1S': [0.8605882352941177,\n              0.9132207338380897,\n              0.9146482122260668,\n              0.8532267275842376,\n              0.8467194570135747,\n              0.853781512605042,\n              0.9012208657047724,\n              0.8471687740516768,\n              0.8420479302832244,\n              0.8974635725849973,\n              0.9053475935828877,\n              0.9019607843137255,\n              0.8975840336134454,\n              0.8906819364914107,\n              0.9050567595459237,\n              0.9033248081841432,\n              0.8990872210953347,\n              0.8969331322272499,\n              0.9212362911266201,\n              0.9036085022244191,\n              0.9215686274509803,\n              0.8891589693728731,\n              0.9238187078109933,\n              0.9076996652319465,\n              0.8918406072106262,\n              0.9157647058823529,\n              0.9145658263305322,\n              0.9092172301991662,\n              0.9053308823529411,\n              0.9106247150022799,\n              0.8923076923076924,\n              0.9110911540188594,\n              0.9126559714795008,\n              0.9150818222025653,\n              0.9069359086918349,\n              0.9145969498910675,\n              0.9147923875432526,\n              0.9063975955345642,\n              0.8891730605285593,\n              0.8975878121032586,\n              0.8857142857142857,\n              0.8881935753024615,\n              0.8835956917978459,\n              0.786096256684492,\n              0.8790849673202614,\n              0.7959432048681542,\n              0.8843674456083803,\n              0.8823529411764706,\n              0.8970588235294118,\n              0.9009080142123964,\n              0.876078431372549,\n              0.872613946240748,\n              0.871517027863777,\n              0.8942714340638216,\n              0.8777692895339955,\n              0.869449715370019,\n              0.8910256410256411,\n              0.8714874484825778,\n              0.8644825018615041,\n              0.8568257491675916,\n              0.8650735294117647,\n              0.8739495798319328,\n              0.8685548293391431,\n              0.8729700469144713,\n              0.8733859397417504,\n              0.875222816399287,\n              0.8664068036853295,\n              0.8700246565692145,\n              0.863795518207283,\n              0.8691263487643578],\n             'GradientBoostingClassifier 2S': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0],\n             'GradientBoostingClassifier 3S': [1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0,\n              1.0]})"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "|    | Classifier type               |      <0> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     <90> |     mean |\n",
      "|----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|\n",
      "|  0 | DecisionTreeClassifier 1      | 0.92     | 0.929528 | 0.933679 | 0.918903 | 0.929864 | 0.916527 | 0.927858 | 0.921385 | 0.928649 | 0.933621 | 0.941176 | 0.932167 | 0.929622 | 0.917231 | 0.905573 | 0.916624 | 0.925963 | 0.927099 | 0.925224 | 0.921404 | 0.927941 | 0.943121 | 0.929122 | 0.900526 | 0.919829 | 0.933647 | 0.920635 | 0.940713 | 0.935662 | 0.942088 | 0.940271 | 0.94432  | 0.932264 | 0.933215 | 0.929763 | 0.932462 | 0.930363 | 0.929154 | 0.891304 | 0.896318 | 0.888655 | 0.884856 | 0.929992 | 0.889757 | 0.887663 | 0.886815 | 0.885173 | 0.881953 | 0.881955 | 0.920647 | 0.919608 | 0.878457 | 0.912539 | 0.908881 | 0.911765 | 0.882732 | 0.899698 | 0.852754 | 0.878258 | 0.826119 | 0.833824 | 0.900621 | 0.905229 | 0.851317 | 0.847202 | 0.884848 | 0.850815 | 0.859458 | 0.84944  | 0.843021 | 0.906984 |\n",
      "|  1 | DecisionTreeClassifier 2      | 1        | 1        | 0.999423 | 0.998858 | 0.998869 | 1        | 1        | 1        | 0.998366 | 1        | 1        | 1        | 1        | 1        | 0.999484 | 1        | 0.996957 | 0.996983 | 0.998504 | 0.998517 | 0.998529 | 0.998055 | 0.998071 | 1        | 0.999051 | 0.999059 | 0.998133 | 0.999074 | 0.999081 | 0.999544 | 1        | 1        | 0.998663 | 0.998673 | 1        | 1        | 1        | 0.995277 | 0.999574 | 0.999577 | 0.99958  | 0.999166 | 0.998757 | 0.997943 | 0.999592 | 0.999594 | 0.998791 | 0.9988   | 1        | 1        | 1        | 0.995325 | 1        | 1        | 1        | 0.999241 | 1        | 0.997003 | 0.999255 | 0.99926  | 0.993015 | 1        | 1        | 0.993143 | 1        | 0.993226 | 0.993267 | 0.999648 | 0.981443 | 0.998956 | 0.99859  |\n",
      "|  2 | DecisionTreeClassifier 3      | 0.832353 | 0.812464 | 0.813725 | 0.796688 | 0.799774 | 0.815686 | 0.786349 | 0.858164 | 0.800109 | 0.865084 | 0.839037 | 0.777424 | 0.80042  | 0.792816 | 0.809598 | 0.82711  | 0.824544 | 0.836099 | 0.783151 | 0.787444 | 0.808824 | 0.774429 | 0.746384 | 0.858919 | 0.785579 | 0.780706 | 0.863212 | 0.754516 | 0.786305 | 0.726858 | 0.798643 | 0.740907 | 0.820856 | 0.759841 | 0.769974 | 0.79695  | 0.805796 | 0.80249  | 0.690111 | 0.705459 | 0.735294 | 0.731331 | 0.705468 | 0.807487 | 0.772467 | 0.766329 | 0.707494 | 0.786315 | 0.729332 | 0.722069 | 0.728235 | 0.746007 | 0.724071 | 0.707036 | 0.722307 | 0.647818 | 0.690422 | 0.712252 | 0.752048 | 0.734369 | 0.734559 | 0.71867  | 0.702251 | 0.728979 | 0.788379 | 0.798574 | 0.783487 | 0.708348 | 0.768908 | 0.779673 | 0.771068 |\n",
      "|  3 | DecisionTreeClassifier 4      | 0.912941 | 0.898078 | 0.900231 | 0.91205  | 0.89819  | 0.910364 | 0.931743 | 0.928532 | 0.923203 | 0.921749 | 0.924599 | 0.924218 | 0.91229  | 0.913587 | 0.903509 | 0.906905 | 0.918357 | 0.918049 | 0.917747 | 0.914483 | 0.898039 | 0.916383 | 0.917068 | 0.895265 | 0.912239 | 0.934118 | 0.919701 | 0.926355 | 0.930147 | 0.931601 | 0.929864 | 0.931747 | 0.913547 | 0.92437  | 0.920983 | 0.913725 | 0.914792 | 0.914985 | 0.87127  | 0.867964 | 0.846218 | 0.841051 | 0.895609 | 0.874537 | 0.875817 | 0.874645 | 0.900886 | 0.907563 | 0.886328 | 0.898144 | 0.901961 | 0.877678 | 0.877322 | 0.885813 | 0.874714 | 0.872486 | 0.903092 | 0.83402  | 0.887193 | 0.893082 | 0.881985 | 0.878699 | 0.880174 | 0.878383 | 0.819584 | 0.810695 | 0.862155 | 0.815076 | 0.809524 | 0.800209 | 0.894566 |\n",
      "|  4 | DecisionTreeClassifier 5      | 0.998824 | 0.992429 | 0.99308  | 1        | 0.993213 | 0.993838 | 0.998335 | 0.998351 | 0.994553 | 0.996222 | 0.998396 | 0.99841  | 0.993172 | 0.992712 | 0.986068 | 0.98977  | 0.990365 | 0.990447 | 0.993021 | 0.99308  | 0.992647 | 0.997083 | 0.997107 | 0.998565 | 0.994307 | 0.997647 | 0.992063 | 0.996758 | 0.998162 | 0.999088 | 0.997738 | 0.998204 | 0.995544 | 0.99425  | 0.995171 | 0.9939   | 0.995242 | 0.993989 | 0.983376 | 0.984342 | 0.988235 | 0.967042 | 0.978045 | 0.986425 | 0.993873 | 0.993915 | 0.98469  | 0.989996 | 0.997218 | 0.98184  | 0.98902  | 0.990651 | 0.990712 | 0.976932 | 0.977846 | 0.991271 | 0.991327 | 0.985013 | 0.992926 | 0.994821 | 0.991544 | 0.992327 | 0.996732 | 0.991339 | 0.980273 | 0.982888 | 0.984408 | 0.985206 | 0.984594 | 0.991298 | 0.991455 |\n",
      "|  5 | RandomForestClassifier 4      | 0.627647 | 0.62085  | 0.614187 | 0.621359 | 0.611991 | 0.62521  | 0.631521 | 0.631116 | 0.634532 | 0.631948 | 0.634759 | 0.63699  | 0.635504 | 0.628319 | 0.633127 | 0.624041 | 0.627282 | 0.607843 | 0.609671 | 0.612457 | 0.621078 | 0.619835 | 0.615236 | 0.615973 | 0.611954 | 0.611294 | 0.605042 | 0.616489 | 0.613511 | 0.604195 | 0.600452 | 0.601257 | 0.600267 | 0.614772 | 0.613696 | 0.594336 | 0.593858 | 0.59167  | 0.592924 | 0.611511 | 0.606303 | 0.59491  | 0.594035 | 0.600165 | 0.60049  | 0.593509 | 0.592264 | 0.593037 | 0.587838 | 0.594552 | 0.597647 | 0.596806 | 0.587074 | 0.597847 | 0.589763 | 0.590133 | 0.602941 | 0.603972 | 0.599404 | 0.595634 | 0.597059 | 0.58312  | 0.582062 | 0.588957 | 0.584648 | 0.583244 | 0.584337 | 0.586122 | 0.583333 | 0.579534 | 0.606006 |\n",
      "|  6 | RandomForestClassifier 5      | 0.548824 | 0.527082 | 0.528835 | 0.531696 | 0.519231 | 0.542857 | 0.557714 | 0.541506 | 0.543028 | 0.547221 | 0.563102 | 0.550609 | 0.559349 | 0.552317 | 0.567079 | 0.548338 | 0.567951 | 0.557567 | 0.560319 | 0.551656 | 0.563235 | 0.555664 | 0.54243  | 0.551889 | 0.538425 | 0.558118 | 0.544351 | 0.558592 | 0.555607 | 0.538988 | 0.541176 | 0.538392 | 0.531194 | 0.553295 | 0.536435 | 0.528105 | 0.555363 | 0.532847 | 0.538363 | 0.55311  | 0.543697 | 0.546934 | 0.53894  | 0.547511 | 0.551062 | 0.541988 | 0.527397 | 0.531813 | 0.528617 | 0.529412 | 0.545882 | 0.538372 | 0.541409 | 0.540561 | 0.532086 | 0.533586 | 0.533937 | 0.560135 | 0.548771 | 0.533481 | 0.544853 | 0.540738 | 0.533769 | 0.53266  | 0.535509 | 0.517647 | 0.529057 | 0.533639 | 0.521359 | 0.501914 | 0.542409 |\n",
      "|  7 | GradientBoostingClassifier 1  | 0.939412 | 0.934188 | 0.93887  | 0.935465 | 0.932692 | 0.941176 | 0.935072 | 0.93348  | 0.934096 | 0.933621 | 0.936364 | 0.940117 | 0.932773 | 0.93493  | 0.933437 | 0.934527 | 0.93002  | 0.930116 | 0.931206 | 0.936233 | 0.934314 | 0.932912 | 0.93298  | 0.928264 | 0.929791 | 0.931765 | 0.929972 | 0.931913 | 0.930147 | 0.931601 | 0.933484 | 0.935788 | 0.931373 | 0.925254 | 0.925373 | 0.928105 | 0.932093 | 0.923143 | 0.926257 | 0.92171  | 0.910504 | 0.921151 | 0.917978 | 0.919375 | 0.914216 | 0.918864 | 0.912571 | 0.907563 | 0.90938  | 0.907225 | 0.91451  | 0.908453 | 0.910991 | 0.906574 | 0.907945 | 0.910436 | 0.909502 | 0.906707 | 0.898362 | 0.90566  | 0.903676 | 0.901717 | 0.898693 | 0.897871 | 0.899928 | 0.899109 | 0.896882 | 0.898908 | 0.89951  | 0.893143 | 0.921449 |\n",
      "|  8 | GradientBoostingClassifier 2  | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "|  9 | GradientBoostingClassifier 3  | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 10 | XGBClassifier 1               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 11 | XGBClassifier 2               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 12 | XGBClassifier 3               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 13 | XGBClassifier 4               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 14 | XGBClassifier 5               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 15 | XGBClassifier 6               | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 16 | XGBRFClassifier 1             | 0.993529 | 0.994758 | 0.991926 | 0.994289 | 0.992081 | 0.990476 | 0.991676 | 0.992853 | 0.991285 | 0.991365 | 0.991979 | 0.993111 | 0.991071 | 0.992192 | 0.99226  | 0.990793 | 0.993915 | 0.992961 | 0.992024 | 0.992091 | 0.990686 | 0.991249 | 0.991321 | 0.99187  | 0.991461 | 0.992471 | 0.99113  | 0.991663 | 0.993107 | 0.993616 | 0.992308 | 0.993264 | 0.990642 | 0.991154 | 0.991659 | 0.99085  | 0.988322 | 0.990125 | 0.98977  | 0.988574 | 0.987395 | 0.989153 | 0.987158 | 0.986837 | 0.987337 | 0.989047 | 0.985898 | 0.990796 | 0.990064 | 0.99013  | 0.988627 | 0.989482 | 0.992647 | 0.991542 | 0.991215 | 0.990133 | 0.990573 | 0.991008 | 0.991809 | 0.992231 | 0.993382 | 0.992327 | 0.991285 | 0.9917   | 0.990674 | 0.990731 | 0.992204 | 0.989785 | 0.989846 | 0.992342 | 0.991132 |\n",
      "| 17 | XGBRFClassifier 2             | 0.997059 | 0.998253 | 0.99654  | 0.997716 | 0.997172 | 0.995518 | 0.99667  | 0.996701 | 0.995098 | 0.997841 | 0.998396 | 0.99788  | 0.999475 | 0.997397 | 0.995356 | 0.997442 | 0.997972 | 0.998492 | 0.998006 | 0.997034 | 0.998529 | 0.997569 | 0.995178 | 0.997609 | 0.997154 | 0.996706 | 0.995798 | 0.996295 | 0.996783 | 0.997264 | 0.995928 | 0.996857 | 0.996435 | 0.996904 | 0.997805 | 0.99695  | 0.996107 | 0.997424 | 0.996164 | 0.996191 | 0.995798 | 0.994994 | 0.994615 | 0.995064 | 0.994281 | 0.996349 | 0.994359 | 0.995998 | 0.996423 | 0.996052 | 0.996471 | 0.996104 | 0.997678 | 0.995771 | 0.996944 | 0.996584 | 0.996606 | 0.996628 | 0.996277 | 0.99593  | 0.996324 | 0.996712 | 0.996369 | 0.996752 | 0.994978 | 0.996791 | 0.996102 | 0.995069 | 0.996499 | 0.996867 | 0.996615 |\n",
      "| 18 | XGBRFClassifier 3             | 0.658235 | 0.659872 | 0.646482 | 0.649343 | 0.651584 | 0.668908 | 0.670366 | 0.661902 | 0.659586 | 0.654074 | 0.662567 | 0.664017 | 0.664391 | 0.667881 | 0.661507 | 0.659847 | 0.661258 | 0.671192 | 0.67996  | 0.678695 | 0.668137 | 0.667963 | 0.666345 | 0.658058 | 0.652277 | 0.667765 | 0.656396 | 0.66327  | 0.664522 | 0.671227 | 0.661086 | 0.672205 | 0.663102 | 0.658558 | 0.662862 | 0.661438 | 0.661765 | 0.652641 | 0.652174 | 0.654253 | 0.64958  | 0.637881 | 0.644159 | 0.636364 | 0.631536 | 0.641379 | 0.641015 | 0.636655 | 0.649841 | 0.643506 | 0.642353 | 0.649786 | 0.638932 | 0.648597 | 0.643239 | 0.649715 | 0.64178  | 0.650806 | 0.643336 | 0.63485  | 0.627574 | 0.635733 | 0.62854  | 0.617827 | 0.617647 | 0.624242 | 0.61623  | 0.606199 | 0.610994 | 0.61434  | 0.650605 |\n",
      "| 19 | XGBRFClassifier 4             | 0.867647 | 0.870705 | 0.874856 | 0.868646 | 0.871606 | 0.871709 | 0.874029 | 0.864761 | 0.868736 | 0.87156  | 0.870588 | 0.869104 | 0.864496 | 0.860489 | 0.862229 | 0.861893 | 0.86359  | 0.868778 | 0.867398 | 0.869006 | 0.866667 | 0.865338 | 0.870299 | 0.86418  | 0.863852 | 0.864941 | 0.861345 | 0.863363 | 0.865349 | 0.867761 | 0.864706 | 0.870229 | 0.858734 | 0.858912 | 0.855575 | 0.858388 | 0.854671 | 0.848433 | 0.844416 | 0.834532 | 0.836134 | 0.835628 | 0.835957 | 0.840806 | 0.834967 | 0.836511 | 0.83884  | 0.841537 | 0.844992 | 0.845638 | 0.845882 | 0.847293 | 0.84017  | 0.843522 | 0.838044 | 0.839848 | 0.82994  | 0.83964  | 0.846612 | 0.841287 | 0.843015 | 0.843259 | 0.840959 | 0.839047 | 0.840029 | 0.844563 | 0.839476 | 0.840085 | 0.837885 | 0.839541 | 0.853923 |\n",
      "| 20 | XGBRFClassifier 5             | 0.979412 | 0.981363 | 0.980392 | 0.977156 | 0.97681  | 0.976471 | 0.977802 | 0.97801  | 0.97549  | 0.975715 | 0.979679 | 0.981452 | 0.978466 | 0.978657 | 0.978328 | 0.974936 | 0.976166 | 0.979387 | 0.981057 | 0.98171  | 0.982353 | 0.979582 | 0.979267 | 0.979436 | 0.974383 | 0.981176 | 0.975724 | 0.976841 | 0.981158 | 0.98176  | 0.977376 | 0.977548 | 0.97549  | 0.97877  | 0.976295 | 0.975599 | 0.977941 | 0.970803 | 0.971441 | 0.9708   | 0.969748 | 0.973717 | 0.970588 | 0.972851 | 0.973448 | 0.974037 | 0.973409 | 0.97479  | 0.977345 | 0.975128 | 0.975294 | 0.977406 | 0.975619 | 0.977316 | 0.975554 | 0.97685  | 0.973982 | 0.976396 | 0.981013 | 0.980022 | 0.978676 | 0.978809 | 0.977124 | 0.976904 | 0.973816 | 0.977184 | 0.977675 | 0.97323  | 0.973739 | 0.975287 | 0.976845 |\n",
      "| 21 | XGBRFClassifier 6             | 0.975882 | 0.984275 | 0.980392 | 0.981725 | 0.977376 | 0.978151 | 0.975028 | 0.97746  | 0.977124 | 0.974096 | 0.977005 | 0.981982 | 0.978992 | 0.978657 | 0.97678  | 0.97289  | 0.975152 | 0.979889 | 0.980558 | 0.982699 | 0.979412 | 0.979582 | 0.977338 | 0.977044 | 0.974858 | 0.978353 | 0.97619  | 0.974988 | 0.977941 | 0.980848 | 0.975113 | 0.981141 | 0.97549  | 0.976117 | 0.972783 | 0.974292 | 0.971886 | 0.970803 | 0.970162 | 0.971223 | 0.968908 | 0.967877 | 0.970174 | 0.968737 | 0.971405 | 0.974442 | 0.974617 | 0.973589 | 0.974563 | 0.973549 | 0.974902 | 0.976237 | 0.975619 | 0.973472 | 0.975172 | 0.972676 | 0.971719 | 0.974522 | 0.979896 | 0.976323 | 0.977206 | 0.976982 | 0.97785  | 0.977625 | 0.973099 | 0.975401 | 0.975195 | 0.97323  | 0.973389 | 0.975635 | 0.975796 |\n",
      "| 22 | GradientBoostingClassifier 1S | 0.860588 | 0.913221 | 0.914648 | 0.853227 | 0.846719 | 0.853782 | 0.901221 | 0.847169 | 0.842048 | 0.897464 | 0.905348 | 0.901961 | 0.897584 | 0.890682 | 0.905057 | 0.903325 | 0.899087 | 0.896933 | 0.921236 | 0.903609 | 0.921569 | 0.889159 | 0.923819 | 0.9077   | 0.891841 | 0.915765 | 0.914566 | 0.909217 | 0.905331 | 0.910625 | 0.892308 | 0.911091 | 0.912656 | 0.915082 | 0.906936 | 0.914597 | 0.914792 | 0.906398 | 0.889173 | 0.897588 | 0.885714 | 0.888194 | 0.883596 | 0.786096 | 0.879085 | 0.795943 | 0.884367 | 0.882353 | 0.897059 | 0.900908 | 0.876078 | 0.872614 | 0.871517 | 0.894271 | 0.877769 | 0.86945  | 0.891026 | 0.871487 | 0.864483 | 0.856826 | 0.865074 | 0.87395  | 0.868555 | 0.87297  | 0.873386 | 0.875223 | 0.866407 | 0.870025 | 0.863796 | 0.869126 | 0.886178 |\n",
      "| 23 | GradientBoostingClassifier 2S | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "| 24 | GradientBoostingClassifier 3S | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        | 1        |\n",
      "+----+-------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers = [\"Classifier type\", \"Accuracy\"]\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "headers2 = [\"Classifier type\", ] + step_headers\n",
    "score_df_train = pd.DataFrame(score_train.items(), columns=headers)\n",
    "accuracy_df_train = pd.DataFrame(score_df_train['Accuracy'].tolist(), index=score_df_train.index, columns=step_headers)\n",
    "score_df_train = score_df_train.drop('Accuracy', 1)\n",
    "f_out_train = pd.merge(score_df_train, accuracy_df_train, how='left', left_index=True, right_index=True)\n",
    "f_out_train['mean'] = f_out_train.mean(axis=1)\n",
    "headers2 = headers2 + ['mean']\n",
    "print(tabulate(f_out_train, headers2, tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_test_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "f_out_train.to_csv(filename_to_export_train, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "                              0_x                     0_y  \\\n0        DecisionTreeClassifier 1  (4, 3, 0, 5, 8, 4, 17)   \n1        DecisionTreeClassifier 2  (4, 3, 2, 5, 8, 4, 17)   \n2        DecisionTreeClassifier 3  (5, 2, 1, 5, 8, 4, 17)   \n3        DecisionTreeClassifier 4  (3, 6, 0, 5, 8, 4, 17)   \n4        DecisionTreeClassifier 5  (3, 3, 0, 5, 8, 4, 17)   \n5        RandomForestClassifier 4  (5, 4, 0, 5, 8, 4, 17)   \n6        RandomForestClassifier 5  (5, 4, 0, 5, 8, 4, 17)   \n7    GradientBoostingClassifier 1  (4, 5, 4, 5, 8, 4, 17)   \n8    GradientBoostingClassifier 2  (4, 4, 3, 5, 8, 4, 17)   \n9    GradientBoostingClassifier 3  (5, 1, 4, 5, 8, 4, 17)   \n10                XGBClassifier 1  (4, 1, 4, 5, 8, 4, 17)   \n11                XGBClassifier 2  (4, 2, 4, 5, 8, 4, 17)   \n12                XGBClassifier 3  (4, 2, 4, 5, 8, 4, 17)   \n13                XGBClassifier 4  (4, 1, 4, 5, 8, 4, 17)   \n14                XGBClassifier 5  (4, 4, 4, 5, 8, 4, 17)   \n15                XGBClassifier 6  (4, 1, 4, 5, 8, 4, 17)   \n16              XGBRFClassifier 1  (4, 2, 3, 5, 8, 4, 17)   \n17              XGBRFClassifier 2  (4, 2, 3, 5, 8, 4, 17)   \n18              XGBRFClassifier 3  (4, 3, 2, 5, 8, 4, 17)   \n19              XGBRFClassifier 4  (4, 3, 3, 5, 8, 4, 17)   \n20              XGBRFClassifier 5  (4, 1, 3, 5, 8, 4, 17)   \n21              XGBRFClassifier 6  (4, 3, 3, 5, 8, 4, 17)   \n22  GradientBoostingClassifier 1S  (4, 2, 4, 5, 8, 4, 17)   \n23  GradientBoostingClassifier 2S  (3, 1, 2, 5, 8, 4, 17)   \n24  GradientBoostingClassifier 3S  (3, 2, 1, 5, 8, 4, 17)   \n\n                         1                       2                       3  \\\n0   (2, 0, 1, 7, 8, 2, 17)  (1, 3, 0, 9, 3, 5, 17)  (5, 3, 0, 5, 3, 9, 17)   \n1   (2, 0, 0, 7, 8, 2, 17)  (2, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   \n2   (1, 5, 1, 7, 8, 2, 17)  (2, 3, 0, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)   \n3   (0, 6, 0, 7, 8, 2, 17)  (4, 0, 2, 9, 3, 5, 17)  (0, 3, 4, 5, 3, 9, 17)   \n4   (0, 7, 1, 7, 8, 2, 17)  (4, 3, 2, 9, 3, 5, 17)  (0, 3, 4, 5, 3, 9, 17)   \n5   (3, 5, 0, 7, 8, 2, 17)  (8, 3, 2, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)   \n6   (7, 4, 0, 7, 8, 2, 17)  (8, 3, 2, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)   \n7   (3, 8, 1, 7, 8, 2, 17)  (3, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)   \n8   (4, 8, 0, 7, 8, 2, 17)  (3, 3, 3, 9, 3, 5, 17)  (3, 3, 8, 5, 3, 9, 17)   \n9   (3, 8, 1, 7, 8, 2, 17)  (5, 3, 2, 9, 3, 5, 17)  (5, 3, 9, 5, 3, 9, 17)   \n10  (2, 8, 1, 7, 8, 2, 17)  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)   \n11  (3, 8, 1, 7, 8, 2, 17)  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)   \n12  (1, 8, 1, 7, 8, 2, 17)  (3, 3, 3, 9, 3, 5, 17)  (2, 3, 9, 5, 3, 9, 17)   \n13  (3, 8, 1, 7, 8, 2, 17)  (4, 3, 3, 9, 3, 5, 17)  (3, 3, 8, 5, 3, 9, 17)   \n14  (3, 8, 1, 7, 8, 2, 17)  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)   \n15  (5, 8, 1, 7, 8, 2, 17)  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)   \n16  (0, 7, 0, 7, 8, 2, 17)  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)   \n17  (0, 7, 0, 7, 8, 2, 17)  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   \n18  (0, 3, 0, 7, 8, 2, 17)  (0, 3, 2, 9, 3, 5, 17)  (0, 3, 0, 5, 3, 9, 17)   \n19  (0, 6, 0, 7, 8, 2, 17)  (2, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   \n20  (0, 6, 0, 7, 8, 2, 17)  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   \n21  (0, 6, 0, 7, 8, 2, 17)  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   \n22  (7, 8, 1, 7, 8, 2, 17)  (2, 3, 3, 9, 3, 5, 17)  (5, 3, 0, 5, 3, 9, 17)   \n23  (4, 8, 1, 7, 8, 2, 17)  (4, 0, 3, 9, 3, 5, 17)  (5, 3, 4, 5, 3, 9, 17)   \n24  (4, 8, 1, 7, 8, 2, 17)  (2, 0, 3, 9, 3, 5, 17)  (5, 2, 4, 5, 3, 9, 17)   \n\n                           4                         5  \\\n0    (3, 0, 0, 17, 0, 0, 17)   (4, 2, 0, 11, 5, 1, 17)   \n1    (3, 0, 0, 17, 0, 0, 17)   (4, 2, 0, 11, 5, 1, 17)   \n2    (3, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n3    (4, 0, 0, 17, 0, 0, 17)   (6, 1, 0, 11, 5, 1, 17)   \n4    (6, 0, 0, 17, 0, 0, 17)   (5, 1, 0, 11, 5, 1, 17)   \n5    (5, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n6    (2, 0, 0, 17, 0, 0, 17)   (5, 5, 0, 11, 5, 1, 17)   \n7    (9, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n8   (10, 0, 0, 17, 0, 0, 17)  (11, 2, 0, 11, 5, 1, 17)   \n9   (14, 0, 0, 17, 0, 0, 17)  (11, 3, 0, 11, 5, 1, 17)   \n10  (11, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n11  (11, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n12  (11, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n13  (13, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n14  (11, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n15  (15, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n16  (10, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n17  (10, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n18   (4, 0, 0, 17, 0, 0, 17)  (10, 1, 0, 11, 5, 1, 17)   \n19  (11, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n20  (10, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n21  (10, 0, 0, 17, 0, 0, 17)  (11, 1, 0, 11, 5, 1, 17)   \n22  (14, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n23  (15, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n24  (17, 0, 0, 17, 0, 0, 17)  (11, 0, 0, 11, 5, 1, 17)   \n\n                           6                         7  \\\n0    (0, 5, 3, 1, 10, 6, 17)   (2, 0, 3, 3, 11, 3, 17)   \n1    (0, 6, 0, 1, 10, 6, 17)   (2, 0, 3, 3, 11, 3, 17)   \n2    (1, 5, 0, 1, 10, 6, 17)   (0, 0, 3, 3, 11, 3, 17)   \n3    (1, 4, 0, 1, 10, 6, 17)   (3, 1, 3, 3, 11, 3, 17)   \n4    (1, 4, 0, 1, 10, 6, 17)   (2, 7, 3, 3, 11, 3, 17)   \n5    (0, 8, 0, 1, 10, 6, 17)   (2, 3, 0, 3, 11, 3, 17)   \n6    (0, 5, 0, 1, 10, 6, 17)   (3, 1, 0, 3, 11, 3, 17)   \n7   (0, 10, 0, 1, 10, 6, 17)   (3, 0, 0, 3, 11, 3, 17)   \n8    (0, 9, 0, 1, 10, 6, 17)   (3, 0, 2, 3, 11, 3, 17)   \n9    (1, 6, 0, 1, 10, 6, 17)   (3, 1, 1, 3, 11, 3, 17)   \n10   (0, 8, 2, 1, 10, 6, 17)   (3, 1, 1, 3, 11, 3, 17)   \n11   (0, 9, 0, 1, 10, 6, 17)   (3, 0, 1, 3, 11, 3, 17)   \n12   (1, 7, 0, 1, 10, 6, 17)   (3, 1, 0, 3, 11, 3, 17)   \n13   (0, 8, 1, 1, 10, 6, 17)   (3, 1, 0, 3, 11, 3, 17)   \n14   (0, 8, 0, 1, 10, 6, 17)   (3, 2, 1, 3, 11, 3, 17)   \n15   (1, 7, 0, 1, 10, 6, 17)   (3, 2, 1, 3, 11, 3, 17)   \n16   (0, 8, 0, 1, 10, 6, 17)   (3, 5, 1, 3, 11, 3, 17)   \n17   (0, 8, 0, 1, 10, 6, 17)   (3, 5, 2, 3, 11, 3, 17)   \n18  (0, 10, 0, 1, 10, 6, 17)   (3, 2, 0, 3, 11, 3, 17)   \n19  (0, 10, 0, 1, 10, 6, 17)   (3, 2, 0, 3, 11, 3, 17)   \n20  (0, 10, 0, 1, 10, 6, 17)   (3, 1, 1, 3, 11, 3, 17)   \n21   (0, 8, 0, 1, 10, 6, 17)   (3, 5, 1, 3, 11, 3, 17)   \n22   (1, 3, 0, 1, 10, 6, 17)  (1, 10, 0, 3, 11, 3, 17)   \n23   (0, 3, 3, 1, 10, 6, 17)   (2, 7, 0, 3, 11, 3, 17)   \n24   (0, 3, 0, 1, 10, 6, 17)   (1, 8, 1, 3, 11, 3, 17)   \n\n                         8  ...                      60  \\\n0   (8, 3, 0, 8, 4, 5, 17)  ...  (4, 1, 0, 5, 7, 5, 17)   \n1   (6, 3, 1, 8, 4, 5, 17)  ...  (4, 1, 0, 5, 7, 5, 17)   \n2   (3, 4, 0, 8, 4, 5, 17)  ...  (5, 4, 0, 5, 7, 5, 17)   \n3   (4, 0, 1, 8, 4, 5, 17)  ...  (4, 5, 0, 5, 7, 5, 17)   \n4   (7, 1, 0, 8, 4, 5, 17)  ...  (0, 5, 2, 5, 7, 5, 17)   \n5   (1, 0, 0, 8, 4, 5, 17)  ...  (4, 5, 0, 5, 7, 5, 17)   \n6   (0, 1, 0, 8, 4, 5, 17)  ...  (4, 7, 0, 5, 7, 5, 17)   \n7   (4, 2, 0, 8, 4, 5, 17)  ...  (5, 1, 0, 5, 7, 5, 17)   \n8   (4, 3, 0, 8, 4, 5, 17)  ...  (5, 2, 0, 5, 7, 5, 17)   \n9   (5, 4, 0, 8, 4, 5, 17)  ...  (5, 1, 0, 5, 7, 5, 17)   \n10  (4, 3, 0, 8, 4, 5, 17)  ...  (5, 6, 0, 5, 7, 5, 17)   \n11  (3, 3, 0, 8, 4, 5, 17)  ...  (5, 2, 0, 5, 7, 5, 17)   \n12  (4, 4, 0, 8, 4, 5, 17)  ...  (5, 1, 0, 5, 7, 5, 17)   \n13  (4, 0, 2, 8, 4, 5, 17)  ...  (5, 2, 0, 5, 7, 5, 17)   \n14  (4, 0, 0, 8, 4, 5, 17)  ...  (5, 6, 0, 5, 7, 5, 17)   \n15  (4, 2, 1, 8, 4, 5, 17)  ...  (5, 3, 1, 5, 7, 5, 17)   \n16  (1, 4, 0, 8, 4, 5, 17)  ...  (5, 4, 1, 5, 7, 5, 17)   \n17  (1, 4, 0, 8, 4, 5, 17)  ...  (5, 4, 1, 5, 7, 5, 17)   \n18  (8, 3, 0, 8, 4, 5, 17)  ...  (4, 0, 1, 5, 7, 5, 17)   \n19  (1, 4, 0, 8, 4, 5, 17)  ...  (5, 2, 0, 5, 7, 5, 17)   \n20  (1, 4, 0, 8, 4, 5, 17)  ...  (5, 2, 1, 5, 7, 5, 17)   \n21  (1, 4, 0, 8, 4, 5, 17)  ...  (5, 4, 1, 5, 7, 5, 17)   \n22  (0, 3, 0, 8, 4, 5, 17)  ...  (5, 1, 0, 5, 7, 5, 17)   \n23  (4, 2, 0, 8, 4, 5, 17)  ...  (3, 7, 0, 5, 7, 5, 17)   \n24  (2, 0, 0, 8, 4, 5, 17)  ...  (1, 7, 0, 5, 7, 5, 17)   \n\n                        61                        62  \\\n0   (2, 0, 0, 6, 7, 4, 17)  (11, 0, 0, 11, 2, 4, 17)   \n1   (2, 0, 0, 6, 7, 4, 17)  (11, 0, 0, 11, 2, 4, 17)   \n2   (4, 2, 1, 6, 7, 4, 17)   (9, 1, 0, 11, 2, 4, 17)   \n3   (2, 0, 0, 6, 7, 4, 17)  (11, 2, 2, 11, 2, 4, 17)   \n4   (2, 0, 0, 6, 7, 4, 17)  (11, 2, 2, 11, 2, 4, 17)   \n5   (0, 7, 0, 6, 7, 4, 17)   (5, 1, 0, 11, 2, 4, 17)   \n6   (2, 7, 0, 6, 7, 4, 17)   (3, 1, 0, 11, 2, 4, 17)   \n7   (5, 7, 0, 6, 7, 4, 17)  (10, 1, 1, 11, 2, 4, 17)   \n8   (6, 5, 0, 6, 7, 4, 17)  (11, 0, 2, 11, 2, 4, 17)   \n9   (6, 5, 0, 6, 7, 4, 17)  (11, 1, 3, 11, 2, 4, 17)   \n10  (6, 6, 0, 6, 7, 4, 17)  (10, 1, 1, 11, 2, 4, 17)   \n11  (6, 2, 0, 6, 7, 4, 17)  (11, 2, 2, 11, 2, 4, 17)   \n12  (6, 6, 0, 6, 7, 4, 17)  (11, 1, 1, 11, 2, 4, 17)   \n13  (5, 6, 0, 6, 7, 4, 17)  (10, 1, 2, 11, 2, 4, 17)   \n14  (5, 3, 0, 6, 7, 4, 17)  (11, 1, 0, 11, 2, 4, 17)   \n15  (6, 6, 0, 6, 7, 4, 17)   (9, 1, 2, 11, 2, 4, 17)   \n16  (4, 1, 0, 6, 7, 4, 17)  (11, 1, 2, 11, 2, 4, 17)   \n17  (4, 2, 0, 6, 7, 4, 17)  (10, 1, 2, 11, 2, 4, 17)   \n18  (0, 3, 0, 6, 7, 4, 17)  (10, 1, 0, 11, 2, 4, 17)   \n19  (3, 0, 0, 6, 7, 4, 17)  (11, 1, 0, 11, 2, 4, 17)   \n20  (4, 0, 0, 6, 7, 4, 17)  (11, 1, 2, 11, 2, 4, 17)   \n21  (4, 0, 0, 6, 7, 4, 17)  (11, 1, 2, 11, 2, 4, 17)   \n22  (6, 7, 0, 6, 7, 4, 17)  (11, 1, 0, 11, 2, 4, 17)   \n23  (6, 7, 1, 6, 7, 4, 17)  (10, 0, 2, 11, 2, 4, 17)   \n24  (6, 7, 0, 6, 7, 4, 17)  (11, 0, 1, 11, 2, 4, 17)   \n\n                          63                       64  \\\n0    (0, 6, 0, 2, 13, 2, 17)  (0, 6, 5, 1, 10, 6, 17)   \n1    (0, 6, 1, 2, 13, 2, 17)  (0, 6, 4, 1, 10, 6, 17)   \n2    (0, 5, 0, 2, 13, 2, 17)  (1, 3, 0, 1, 10, 6, 17)   \n3    (0, 1, 1, 2, 13, 2, 17)  (1, 2, 1, 1, 10, 6, 17)   \n4    (0, 1, 1, 2, 13, 2, 17)  (1, 8, 1, 1, 10, 6, 17)   \n5   (0, 13, 0, 2, 13, 2, 17)  (0, 8, 0, 1, 10, 6, 17)   \n6   (0, 11, 0, 2, 13, 2, 17)  (0, 4, 0, 1, 10, 6, 17)   \n7   (2, 11, 1, 2, 13, 2, 17)  (0, 1, 5, 1, 10, 6, 17)   \n8    (0, 7, 1, 2, 13, 2, 17)  (0, 8, 2, 1, 10, 6, 17)   \n9   (1, 10, 2, 2, 13, 2, 17)  (0, 1, 5, 1, 10, 6, 17)   \n10  (0, 10, 1, 2, 13, 2, 17)  (0, 8, 3, 1, 10, 6, 17)   \n11   (0, 8, 1, 2, 13, 2, 17)  (1, 5, 2, 1, 10, 6, 17)   \n12   (0, 9, 1, 2, 13, 2, 17)  (1, 7, 2, 1, 10, 6, 17)   \n13  (0, 11, 1, 2, 13, 2, 17)  (0, 3, 3, 1, 10, 6, 17)   \n14  (1, 12, 1, 2, 13, 2, 17)  (0, 4, 2, 1, 10, 6, 17)   \n15   (0, 7, 2, 2, 13, 2, 17)  (0, 6, 5, 1, 10, 6, 17)   \n16   (0, 7, 1, 2, 13, 2, 17)  (0, 4, 1, 1, 10, 6, 17)   \n17   (0, 8, 1, 2, 13, 2, 17)  (0, 4, 1, 1, 10, 6, 17)   \n18   (0, 8, 1, 2, 13, 2, 17)  (0, 4, 4, 1, 10, 6, 17)   \n19   (0, 6, 1, 2, 13, 2, 17)  (0, 4, 1, 1, 10, 6, 17)   \n20  (0, 10, 1, 2, 13, 2, 17)  (0, 4, 1, 1, 10, 6, 17)   \n21   (0, 7, 1, 2, 13, 2, 17)  (0, 4, 1, 1, 10, 6, 17)   \n22   (2, 2, 2, 2, 13, 2, 17)  (1, 6, 2, 1, 10, 6, 17)   \n23   (1, 5, 2, 2, 13, 2, 17)  (1, 9, 2, 1, 10, 6, 17)   \n24   (1, 4, 2, 2, 13, 2, 17)  (1, 7, 2, 1, 10, 6, 17)   \n\n                          65                       66  \\\n0   (11, 0, 3, 12, 0, 5, 17)  (1, 0, 0, 1, 11, 5, 17)   \n1   (11, 0, 0, 12, 0, 5, 17)  (1, 0, 0, 1, 11, 5, 17)   \n2    (5, 0, 0, 12, 0, 5, 17)  (1, 2, 0, 1, 11, 5, 17)   \n3   (12, 0, 0, 12, 0, 5, 17)  (1, 3, 0, 1, 11, 5, 17)   \n4    (9, 0, 0, 12, 0, 5, 17)  (1, 0, 0, 1, 11, 5, 17)   \n5    (0, 0, 0, 12, 0, 5, 17)  (1, 9, 0, 1, 11, 5, 17)   \n6    (1, 0, 0, 12, 0, 5, 17)  (1, 9, 0, 1, 11, 5, 17)   \n7    (3, 0, 0, 12, 0, 5, 17)  (1, 2, 5, 1, 11, 5, 17)   \n8    (5, 0, 0, 12, 0, 5, 17)  (1, 0, 4, 1, 11, 5, 17)   \n9    (8, 0, 0, 12, 0, 5, 17)  (1, 1, 5, 1, 11, 5, 17)   \n10   (9, 0, 2, 12, 0, 5, 17)  (1, 5, 5, 1, 11, 5, 17)   \n11   (7, 0, 0, 12, 0, 5, 17)  (1, 4, 1, 1, 11, 5, 17)   \n12   (9, 0, 1, 12, 0, 5, 17)  (1, 3, 4, 1, 11, 5, 17)   \n13   (7, 0, 1, 12, 0, 5, 17)  (1, 1, 1, 1, 11, 5, 17)   \n14   (7, 0, 2, 12, 0, 5, 17)  (1, 4, 5, 1, 11, 5, 17)   \n15   (9, 0, 2, 12, 0, 5, 17)  (1, 1, 5, 1, 11, 5, 17)   \n16   (3, 0, 0, 12, 0, 5, 17)  (1, 9, 1, 1, 11, 5, 17)   \n17   (3, 0, 0, 12, 0, 5, 17)  (1, 9, 3, 1, 11, 5, 17)   \n18   (2, 0, 0, 12, 0, 5, 17)  (1, 9, 0, 1, 11, 5, 17)   \n19   (7, 0, 0, 12, 0, 5, 17)  (1, 9, 0, 1, 11, 5, 17)   \n20   (4, 0, 0, 12, 0, 5, 17)  (1, 9, 0, 1, 11, 5, 17)   \n21   (5, 0, 0, 12, 0, 5, 17)  (1, 9, 2, 1, 11, 5, 17)   \n22   (8, 0, 0, 12, 0, 5, 17)  (1, 8, 5, 1, 11, 5, 17)   \n23   (9, 0, 1, 12, 0, 5, 17)  (1, 4, 5, 1, 11, 5, 17)   \n24  (10, 0, 2, 12, 0, 5, 17)  (1, 8, 4, 1, 11, 5, 17)   \n\n                          67                      68                        69  \n0   (0, 12, 1, 0, 12, 5, 17)  (1, 3, 0, 7, 5, 5, 17)   (2, 0, 3, 12, 0, 5, 17)  \n1    (0, 8, 0, 0, 12, 5, 17)  (1, 3, 0, 7, 5, 5, 17)   (3, 0, 3, 12, 0, 5, 17)  \n2    (0, 6, 4, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (1, 0, 0, 12, 0, 5, 17)  \n3    (0, 8, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (7, 0, 0, 12, 0, 5, 17)  \n4    (0, 8, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (5, 0, 0, 12, 0, 5, 17)  \n5   (0, 12, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (2, 0, 0, 12, 0, 5, 17)  \n6   (0, 12, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (0, 0, 0, 12, 0, 5, 17)  \n7   (0, 10, 0, 0, 12, 5, 17)  (2, 5, 0, 7, 5, 5, 17)  (12, 0, 0, 12, 0, 5, 17)  \n8   (0, 11, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (8, 0, 0, 12, 0, 5, 17)  \n9    (0, 7, 2, 0, 12, 5, 17)  (3, 5, 1, 7, 5, 5, 17)  (12, 0, 1, 12, 0, 5, 17)  \n10   (0, 9, 0, 0, 12, 5, 17)  (2, 5, 0, 7, 5, 5, 17)  (12, 0, 0, 12, 0, 5, 17)  \n11   (0, 8, 1, 0, 12, 5, 17)  (2, 5, 0, 7, 5, 5, 17)  (12, 0, 0, 12, 0, 5, 17)  \n12  (0, 11, 0, 0, 12, 5, 17)  (2, 5, 0, 7, 5, 5, 17)  (12, 0, 0, 12, 0, 5, 17)  \n13   (0, 9, 0, 0, 12, 5, 17)  (3, 5, 0, 7, 5, 5, 17)  (10, 0, 1, 12, 0, 5, 17)  \n14   (0, 9, 0, 0, 12, 5, 17)  (5, 5, 0, 7, 5, 5, 17)  (11, 0, 1, 12, 0, 5, 17)  \n15   (0, 9, 2, 0, 12, 5, 17)  (3, 5, 0, 7, 5, 5, 17)  (11, 0, 1, 12, 0, 5, 17)  \n16  (0, 12, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (3, 0, 0, 12, 0, 5, 17)  \n17  (0, 12, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (3, 0, 0, 12, 0, 5, 17)  \n18   (0, 8, 0, 0, 12, 5, 17)  (0, 4, 3, 7, 5, 5, 17)   (0, 0, 4, 12, 0, 5, 17)  \n19   (0, 9, 0, 0, 12, 5, 17)  (0, 5, 1, 7, 5, 5, 17)   (2, 0, 1, 12, 0, 5, 17)  \n20  (0, 12, 0, 0, 12, 5, 17)  (0, 5, 0, 7, 5, 5, 17)   (3, 0, 0, 12, 0, 5, 17)  \n21  (0, 12, 0, 0, 12, 5, 17)  (1, 5, 0, 7, 5, 5, 17)   (3, 0, 0, 12, 0, 5, 17)  \n22  (0, 10, 0, 0, 12, 5, 17)  (1, 5, 3, 7, 5, 5, 17)  (10, 0, 1, 12, 0, 5, 17)  \n23   (0, 9, 0, 0, 12, 5, 17)  (1, 1, 3, 7, 5, 5, 17)   (9, 0, 1, 12, 0, 5, 17)  \n24  (0, 10, 2, 0, 12, 5, 17)  (1, 1, 2, 7, 5, 5, 17)  (10, 0, 2, 12, 0, 5, 17)  \n\n[25 rows x 71 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0_x</th>\n      <th>0_y</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>63</th>\n      <th>64</th>\n      <th>65</th>\n      <th>66</th>\n      <th>67</th>\n      <th>68</th>\n      <th>69</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 1</td>\n      <td>(4, 3, 0, 5, 8, 4, 17)</td>\n      <td>(2, 0, 1, 7, 8, 2, 17)</td>\n      <td>(1, 3, 0, 9, 3, 5, 17)</td>\n      <td>(5, 3, 0, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(4, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 5, 3, 1, 10, 6, 17)</td>\n      <td>(2, 0, 3, 3, 11, 3, 17)</td>\n      <td>(8, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 1, 0, 5, 7, 5, 17)</td>\n      <td>(2, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 0, 0, 11, 2, 4, 17)</td>\n      <td>(0, 6, 0, 2, 13, 2, 17)</td>\n      <td>(0, 6, 5, 1, 10, 6, 17)</td>\n      <td>(11, 0, 3, 12, 0, 5, 17)</td>\n      <td>(1, 0, 0, 1, 11, 5, 17)</td>\n      <td>(0, 12, 1, 0, 12, 5, 17)</td>\n      <td>(1, 3, 0, 7, 5, 5, 17)</td>\n      <td>(2, 0, 3, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeClassifier 2</td>\n      <td>(4, 3, 2, 5, 8, 4, 17)</td>\n      <td>(2, 0, 0, 7, 8, 2, 17)</td>\n      <td>(2, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(4, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 6, 0, 1, 10, 6, 17)</td>\n      <td>(2, 0, 3, 3, 11, 3, 17)</td>\n      <td>(6, 3, 1, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 1, 0, 5, 7, 5, 17)</td>\n      <td>(2, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 0, 0, 11, 2, 4, 17)</td>\n      <td>(0, 6, 1, 2, 13, 2, 17)</td>\n      <td>(0, 6, 4, 1, 10, 6, 17)</td>\n      <td>(11, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 0, 0, 1, 11, 5, 17)</td>\n      <td>(0, 8, 0, 0, 12, 5, 17)</td>\n      <td>(1, 3, 0, 7, 5, 5, 17)</td>\n      <td>(3, 0, 3, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DecisionTreeClassifier 3</td>\n      <td>(5, 2, 1, 5, 8, 4, 17)</td>\n      <td>(1, 5, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 0, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 5, 0, 1, 10, 6, 17)</td>\n      <td>(0, 0, 3, 3, 11, 3, 17)</td>\n      <td>(3, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 4, 0, 5, 7, 5, 17)</td>\n      <td>(4, 2, 1, 6, 7, 4, 17)</td>\n      <td>(9, 1, 0, 11, 2, 4, 17)</td>\n      <td>(0, 5, 0, 2, 13, 2, 17)</td>\n      <td>(1, 3, 0, 1, 10, 6, 17)</td>\n      <td>(5, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 2, 0, 1, 11, 5, 17)</td>\n      <td>(0, 6, 4, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(1, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier 4</td>\n      <td>(3, 6, 0, 5, 8, 4, 17)</td>\n      <td>(0, 6, 0, 7, 8, 2, 17)</td>\n      <td>(4, 0, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 4, 5, 3, 9, 17)</td>\n      <td>(4, 0, 0, 17, 0, 0, 17)</td>\n      <td>(6, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 4, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 3, 3, 11, 3, 17)</td>\n      <td>(4, 0, 1, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 5, 0, 5, 7, 5, 17)</td>\n      <td>(2, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 2, 2, 11, 2, 4, 17)</td>\n      <td>(0, 1, 1, 2, 13, 2, 17)</td>\n      <td>(1, 2, 1, 1, 10, 6, 17)</td>\n      <td>(12, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 3, 0, 1, 11, 5, 17)</td>\n      <td>(0, 8, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(7, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DecisionTreeClassifier 5</td>\n      <td>(3, 3, 0, 5, 8, 4, 17)</td>\n      <td>(0, 7, 1, 7, 8, 2, 17)</td>\n      <td>(4, 3, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 4, 5, 3, 9, 17)</td>\n      <td>(6, 0, 0, 17, 0, 0, 17)</td>\n      <td>(5, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 4, 0, 1, 10, 6, 17)</td>\n      <td>(2, 7, 3, 3, 11, 3, 17)</td>\n      <td>(7, 1, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(0, 5, 2, 5, 7, 5, 17)</td>\n      <td>(2, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 2, 2, 11, 2, 4, 17)</td>\n      <td>(0, 1, 1, 2, 13, 2, 17)</td>\n      <td>(1, 8, 1, 1, 10, 6, 17)</td>\n      <td>(9, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 0, 0, 1, 11, 5, 17)</td>\n      <td>(0, 8, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(5, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RandomForestClassifier 4</td>\n      <td>(5, 4, 0, 5, 8, 4, 17)</td>\n      <td>(3, 5, 0, 7, 8, 2, 17)</td>\n      <td>(8, 3, 2, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(5, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(2, 3, 0, 3, 11, 3, 17)</td>\n      <td>(1, 0, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 5, 0, 5, 7, 5, 17)</td>\n      <td>(0, 7, 0, 6, 7, 4, 17)</td>\n      <td>(5, 1, 0, 11, 2, 4, 17)</td>\n      <td>(0, 13, 0, 2, 13, 2, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(0, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 0, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(2, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RandomForestClassifier 5</td>\n      <td>(5, 4, 0, 5, 8, 4, 17)</td>\n      <td>(7, 4, 0, 7, 8, 2, 17)</td>\n      <td>(8, 3, 2, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(2, 0, 0, 17, 0, 0, 17)</td>\n      <td>(5, 5, 0, 11, 5, 1, 17)</td>\n      <td>(0, 5, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(0, 1, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 7, 0, 5, 7, 5, 17)</td>\n      <td>(2, 7, 0, 6, 7, 4, 17)</td>\n      <td>(3, 1, 0, 11, 2, 4, 17)</td>\n      <td>(0, 11, 0, 2, 13, 2, 17)</td>\n      <td>(0, 4, 0, 1, 10, 6, 17)</td>\n      <td>(1, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 0, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(0, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GradientBoostingClassifier 1</td>\n      <td>(4, 5, 4, 5, 8, 4, 17)</td>\n      <td>(3, 8, 1, 7, 8, 2, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(9, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 0, 3, 11, 3, 17)</td>\n      <td>(4, 2, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 1, 0, 5, 7, 5, 17)</td>\n      <td>(5, 7, 0, 6, 7, 4, 17)</td>\n      <td>(10, 1, 1, 11, 2, 4, 17)</td>\n      <td>(2, 11, 1, 2, 13, 2, 17)</td>\n      <td>(0, 1, 5, 1, 10, 6, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 2, 5, 1, 11, 5, 17)</td>\n      <td>(0, 10, 0, 0, 12, 5, 17)</td>\n      <td>(2, 5, 0, 7, 5, 5, 17)</td>\n      <td>(12, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GradientBoostingClassifier 2</td>\n      <td>(4, 4, 3, 5, 8, 4, 17)</td>\n      <td>(4, 8, 0, 7, 8, 2, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(3, 3, 8, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 9, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 2, 3, 11, 3, 17)</td>\n      <td>(4, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 2, 0, 5, 7, 5, 17)</td>\n      <td>(6, 5, 0, 6, 7, 4, 17)</td>\n      <td>(11, 0, 2, 11, 2, 4, 17)</td>\n      <td>(0, 7, 1, 2, 13, 2, 17)</td>\n      <td>(0, 8, 2, 1, 10, 6, 17)</td>\n      <td>(5, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 0, 4, 1, 11, 5, 17)</td>\n      <td>(0, 11, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(8, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GradientBoostingClassifier 3</td>\n      <td>(5, 1, 4, 5, 8, 4, 17)</td>\n      <td>(3, 8, 1, 7, 8, 2, 17)</td>\n      <td>(5, 3, 2, 9, 3, 5, 17)</td>\n      <td>(5, 3, 9, 5, 3, 9, 17)</td>\n      <td>(14, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 3, 0, 11, 5, 1, 17)</td>\n      <td>(1, 6, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(5, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 1, 0, 5, 7, 5, 17)</td>\n      <td>(6, 5, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 3, 11, 2, 4, 17)</td>\n      <td>(1, 10, 2, 2, 13, 2, 17)</td>\n      <td>(0, 1, 5, 1, 10, 6, 17)</td>\n      <td>(8, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 1, 5, 1, 11, 5, 17)</td>\n      <td>(0, 7, 2, 0, 12, 5, 17)</td>\n      <td>(3, 5, 1, 7, 5, 5, 17)</td>\n      <td>(12, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBClassifier 1</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(2, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 2, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(4, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 6, 0, 5, 7, 5, 17)</td>\n      <td>(6, 6, 0, 6, 7, 4, 17)</td>\n      <td>(10, 1, 1, 11, 2, 4, 17)</td>\n      <td>(0, 10, 1, 2, 13, 2, 17)</td>\n      <td>(0, 8, 3, 1, 10, 6, 17)</td>\n      <td>(9, 0, 2, 12, 0, 5, 17)</td>\n      <td>(1, 5, 5, 1, 11, 5, 17)</td>\n      <td>(0, 9, 0, 0, 12, 5, 17)</td>\n      <td>(2, 5, 0, 7, 5, 5, 17)</td>\n      <td>(12, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGBClassifier 2</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(3, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 9, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 1, 3, 11, 3, 17)</td>\n      <td>(3, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 2, 0, 5, 7, 5, 17)</td>\n      <td>(6, 2, 0, 6, 7, 4, 17)</td>\n      <td>(11, 2, 2, 11, 2, 4, 17)</td>\n      <td>(0, 8, 1, 2, 13, 2, 17)</td>\n      <td>(1, 5, 2, 1, 10, 6, 17)</td>\n      <td>(7, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 4, 1, 1, 11, 5, 17)</td>\n      <td>(0, 8, 1, 0, 12, 5, 17)</td>\n      <td>(2, 5, 0, 7, 5, 5, 17)</td>\n      <td>(12, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGBClassifier 3</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(1, 8, 1, 7, 8, 2, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(2, 3, 9, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 7, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(4, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 1, 0, 5, 7, 5, 17)</td>\n      <td>(6, 6, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 1, 11, 2, 4, 17)</td>\n      <td>(0, 9, 1, 2, 13, 2, 17)</td>\n      <td>(1, 7, 2, 1, 10, 6, 17)</td>\n      <td>(9, 0, 1, 12, 0, 5, 17)</td>\n      <td>(1, 3, 4, 1, 11, 5, 17)</td>\n      <td>(0, 11, 0, 0, 12, 5, 17)</td>\n      <td>(2, 5, 0, 7, 5, 5, 17)</td>\n      <td>(12, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGBClassifier 4</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(3, 8, 1, 7, 8, 2, 17)</td>\n      <td>(4, 3, 3, 9, 3, 5, 17)</td>\n      <td>(3, 3, 8, 5, 3, 9, 17)</td>\n      <td>(13, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 1, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(4, 0, 2, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 2, 0, 5, 7, 5, 17)</td>\n      <td>(5, 6, 0, 6, 7, 4, 17)</td>\n      <td>(10, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 11, 1, 2, 13, 2, 17)</td>\n      <td>(0, 3, 3, 1, 10, 6, 17)</td>\n      <td>(7, 0, 1, 12, 0, 5, 17)</td>\n      <td>(1, 1, 1, 1, 11, 5, 17)</td>\n      <td>(0, 9, 0, 0, 12, 5, 17)</td>\n      <td>(3, 5, 0, 7, 5, 5, 17)</td>\n      <td>(10, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGBClassifier 5</td>\n      <td>(4, 4, 4, 5, 8, 4, 17)</td>\n      <td>(3, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 1, 3, 11, 3, 17)</td>\n      <td>(4, 0, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 6, 0, 5, 7, 5, 17)</td>\n      <td>(5, 3, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 0, 11, 2, 4, 17)</td>\n      <td>(1, 12, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 2, 1, 10, 6, 17)</td>\n      <td>(7, 0, 2, 12, 0, 5, 17)</td>\n      <td>(1, 4, 5, 1, 11, 5, 17)</td>\n      <td>(0, 9, 0, 0, 12, 5, 17)</td>\n      <td>(5, 5, 0, 7, 5, 5, 17)</td>\n      <td>(11, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGBClassifier 6</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(5, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(15, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 7, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 1, 3, 11, 3, 17)</td>\n      <td>(4, 2, 1, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 3, 1, 5, 7, 5, 17)</td>\n      <td>(6, 6, 0, 6, 7, 4, 17)</td>\n      <td>(9, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 7, 2, 2, 13, 2, 17)</td>\n      <td>(0, 6, 5, 1, 10, 6, 17)</td>\n      <td>(9, 0, 2, 12, 0, 5, 17)</td>\n      <td>(1, 1, 5, 1, 11, 5, 17)</td>\n      <td>(0, 9, 2, 0, 12, 5, 17)</td>\n      <td>(3, 5, 0, 7, 5, 5, 17)</td>\n      <td>(11, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>XGBRFClassifier 1</td>\n      <td>(4, 2, 3, 5, 8, 4, 17)</td>\n      <td>(0, 7, 0, 7, 8, 2, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 4, 1, 5, 7, 5, 17)</td>\n      <td>(4, 1, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 7, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 1, 1, 10, 6, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 1, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>XGBRFClassifier 2</td>\n      <td>(4, 2, 3, 5, 8, 4, 17)</td>\n      <td>(0, 7, 0, 7, 8, 2, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 2, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 4, 1, 5, 7, 5, 17)</td>\n      <td>(4, 2, 0, 6, 7, 4, 17)</td>\n      <td>(10, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 8, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 1, 1, 10, 6, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 3, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>XGBRFClassifier 3</td>\n      <td>(4, 3, 2, 5, 8, 4, 17)</td>\n      <td>(0, 3, 0, 7, 8, 2, 17)</td>\n      <td>(0, 3, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 0, 5, 3, 9, 17)</td>\n      <td>(4, 0, 0, 17, 0, 0, 17)</td>\n      <td>(10, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 0, 3, 11, 3, 17)</td>\n      <td>(8, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(4, 0, 1, 5, 7, 5, 17)</td>\n      <td>(0, 3, 0, 6, 7, 4, 17)</td>\n      <td>(10, 1, 0, 11, 2, 4, 17)</td>\n      <td>(0, 8, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 4, 1, 10, 6, 17)</td>\n      <td>(2, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 0, 1, 11, 5, 17)</td>\n      <td>(0, 8, 0, 0, 12, 5, 17)</td>\n      <td>(0, 4, 3, 7, 5, 5, 17)</td>\n      <td>(0, 0, 4, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>XGBRFClassifier 4</td>\n      <td>(4, 3, 3, 5, 8, 4, 17)</td>\n      <td>(0, 6, 0, 7, 8, 2, 17)</td>\n      <td>(2, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 0, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 2, 0, 5, 7, 5, 17)</td>\n      <td>(3, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 0, 11, 2, 4, 17)</td>\n      <td>(0, 6, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 1, 1, 10, 6, 17)</td>\n      <td>(7, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 0, 1, 11, 5, 17)</td>\n      <td>(0, 9, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 1, 7, 5, 5, 17)</td>\n      <td>(2, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>XGBRFClassifier 5</td>\n      <td>(4, 1, 3, 5, 8, 4, 17)</td>\n      <td>(0, 6, 0, 7, 8, 2, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 2, 1, 5, 7, 5, 17)</td>\n      <td>(4, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 10, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 1, 1, 10, 6, 17)</td>\n      <td>(4, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 0, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(0, 5, 0, 7, 5, 5, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>XGBRFClassifier 6</td>\n      <td>(4, 3, 3, 5, 8, 4, 17)</td>\n      <td>(0, 6, 0, 7, 8, 2, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 4, 1, 5, 7, 5, 17)</td>\n      <td>(4, 0, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 2, 11, 2, 4, 17)</td>\n      <td>(0, 7, 1, 2, 13, 2, 17)</td>\n      <td>(0, 4, 1, 1, 10, 6, 17)</td>\n      <td>(5, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 9, 2, 1, 11, 5, 17)</td>\n      <td>(0, 12, 0, 0, 12, 5, 17)</td>\n      <td>(1, 5, 0, 7, 5, 5, 17)</td>\n      <td>(3, 0, 0, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>GradientBoostingClassifier 1S</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(7, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(5, 3, 0, 5, 3, 9, 17)</td>\n      <td>(14, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 3, 0, 1, 10, 6, 17)</td>\n      <td>(1, 10, 0, 3, 11, 3, 17)</td>\n      <td>(0, 3, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(5, 1, 0, 5, 7, 5, 17)</td>\n      <td>(6, 7, 0, 6, 7, 4, 17)</td>\n      <td>(11, 1, 0, 11, 2, 4, 17)</td>\n      <td>(2, 2, 2, 2, 13, 2, 17)</td>\n      <td>(1, 6, 2, 1, 10, 6, 17)</td>\n      <td>(8, 0, 0, 12, 0, 5, 17)</td>\n      <td>(1, 8, 5, 1, 11, 5, 17)</td>\n      <td>(0, 10, 0, 0, 12, 5, 17)</td>\n      <td>(1, 5, 3, 7, 5, 5, 17)</td>\n      <td>(10, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>GradientBoostingClassifier 2S</td>\n      <td>(3, 1, 2, 5, 8, 4, 17)</td>\n      <td>(4, 8, 1, 7, 8, 2, 17)</td>\n      <td>(4, 0, 3, 9, 3, 5, 17)</td>\n      <td>(5, 3, 4, 5, 3, 9, 17)</td>\n      <td>(15, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 3, 3, 1, 10, 6, 17)</td>\n      <td>(2, 7, 0, 3, 11, 3, 17)</td>\n      <td>(4, 2, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(3, 7, 0, 5, 7, 5, 17)</td>\n      <td>(6, 7, 1, 6, 7, 4, 17)</td>\n      <td>(10, 0, 2, 11, 2, 4, 17)</td>\n      <td>(1, 5, 2, 2, 13, 2, 17)</td>\n      <td>(1, 9, 2, 1, 10, 6, 17)</td>\n      <td>(9, 0, 1, 12, 0, 5, 17)</td>\n      <td>(1, 4, 5, 1, 11, 5, 17)</td>\n      <td>(0, 9, 0, 0, 12, 5, 17)</td>\n      <td>(1, 1, 3, 7, 5, 5, 17)</td>\n      <td>(9, 0, 1, 12, 0, 5, 17)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>GradientBoostingClassifier 3S</td>\n      <td>(3, 2, 1, 5, 8, 4, 17)</td>\n      <td>(4, 8, 1, 7, 8, 2, 17)</td>\n      <td>(2, 0, 3, 9, 3, 5, 17)</td>\n      <td>(5, 2, 4, 5, 3, 9, 17)</td>\n      <td>(17, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 3, 0, 1, 10, 6, 17)</td>\n      <td>(1, 8, 1, 3, 11, 3, 17)</td>\n      <td>(2, 0, 0, 8, 4, 5, 17)</td>\n      <td>...</td>\n      <td>(1, 7, 0, 5, 7, 5, 17)</td>\n      <td>(6, 7, 0, 6, 7, 4, 17)</td>\n      <td>(11, 0, 1, 11, 2, 4, 17)</td>\n      <td>(1, 4, 2, 2, 13, 2, 17)</td>\n      <td>(1, 7, 2, 1, 10, 6, 17)</td>\n      <td>(10, 0, 2, 12, 0, 5, 17)</td>\n      <td>(1, 8, 4, 1, 11, 5, 17)</td>\n      <td>(0, 10, 2, 0, 12, 5, 17)</td>\n      <td>(1, 1, 2, 7, 5, 5, 17)</td>\n      <td>(10, 0, 2, 12, 0, 5, 17)</td>\n    </tr>\n  </tbody>\n</table>\n<p>25 rows Ã— 71 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_to_export_train = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result_train_points_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\nfold\\\\result__points_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "\n",
    "points\n",
    "df_points = pd.DataFrame(points.items())\n",
    "df_temp = pd.DataFrame(df_points[1].tolist(), index=score_df_train.index)\n",
    "df_points = df_points.drop(df_points.columns[1], axis=1)\n",
    "df_points = pd.merge(df_points, df_temp, how='left', left_index=True, right_index=True)\n",
    "df_points.to_csv(filename_to_export)\n",
    "df_points\n",
    "\n",
    "# df_points_train = pd(points_train.items())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "                              0_x                     0_y  \\\n0        DecisionTreeClassifier 1  (4, 3, 0, 5, 8, 4, 17)   \n1        DecisionTreeClassifier 2  (4, 3, 2, 5, 8, 4, 17)   \n2        DecisionTreeClassifier 3  (5, 2, 1, 5, 8, 4, 17)   \n3        DecisionTreeClassifier 4  (3, 6, 0, 5, 8, 4, 17)   \n4        DecisionTreeClassifier 5  (3, 3, 0, 5, 8, 4, 17)   \n5        RandomForestClassifier 4  (5, 4, 0, 5, 8, 4, 17)   \n6        RandomForestClassifier 5  (5, 4, 0, 5, 8, 4, 17)   \n7    GradientBoostingClassifier 1  (4, 5, 4, 5, 8, 4, 17)   \n8    GradientBoostingClassifier 2  (4, 4, 3, 5, 8, 4, 17)   \n9    GradientBoostingClassifier 3  (5, 1, 4, 5, 8, 4, 17)   \n10                XGBClassifier 1  (4, 1, 4, 5, 8, 4, 17)   \n11                XGBClassifier 2  (4, 2, 4, 5, 8, 4, 17)   \n12                XGBClassifier 3  (4, 2, 4, 5, 8, 4, 17)   \n13                XGBClassifier 4  (4, 1, 4, 5, 8, 4, 17)   \n14                XGBClassifier 5  (4, 4, 4, 5, 8, 4, 17)   \n15                XGBClassifier 6  (4, 1, 4, 5, 8, 4, 17)   \n16              XGBRFClassifier 1  (4, 2, 3, 5, 8, 4, 17)   \n17              XGBRFClassifier 2  (4, 2, 3, 5, 8, 4, 17)   \n18              XGBRFClassifier 3  (4, 3, 2, 5, 8, 4, 17)   \n19              XGBRFClassifier 4  (4, 3, 3, 5, 8, 4, 17)   \n20              XGBRFClassifier 5  (4, 1, 3, 5, 8, 4, 17)   \n21              XGBRFClassifier 6  (4, 3, 3, 5, 8, 4, 17)   \n22  GradientBoostingClassifier 1S  (4, 2, 4, 5, 8, 4, 17)   \n23  GradientBoostingClassifier 2S  (3, 1, 2, 5, 8, 4, 17)   \n24  GradientBoostingClassifier 3S  (3, 2, 1, 5, 8, 4, 17)   \n\n                       2_x                     3_x                       4_x  \\\n0   (1, 3, 0, 9, 3, 5, 17)  (5, 3, 0, 5, 3, 9, 17)   (3, 0, 0, 17, 0, 0, 17)   \n1   (2, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)   (3, 0, 0, 17, 0, 0, 17)   \n2   (2, 3, 0, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)   (3, 0, 0, 17, 0, 0, 17)   \n3   (4, 0, 2, 9, 3, 5, 17)  (0, 3, 4, 5, 3, 9, 17)   (4, 0, 0, 17, 0, 0, 17)   \n4   (4, 3, 2, 9, 3, 5, 17)  (0, 3, 4, 5, 3, 9, 17)   (6, 0, 0, 17, 0, 0, 17)   \n5   (8, 3, 2, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)   (5, 0, 0, 17, 0, 0, 17)   \n6   (8, 3, 2, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)   (2, 0, 0, 17, 0, 0, 17)   \n7   (3, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)   (9, 0, 0, 17, 0, 0, 17)   \n8   (3, 3, 3, 9, 3, 5, 17)  (3, 3, 8, 5, 3, 9, 17)  (10, 0, 0, 17, 0, 0, 17)   \n9   (5, 3, 2, 9, 3, 5, 17)  (5, 3, 9, 5, 3, 9, 17)  (14, 0, 0, 17, 0, 0, 17)   \n10  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)  (11, 0, 0, 17, 0, 0, 17)   \n11  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)  (11, 0, 0, 17, 0, 0, 17)   \n12  (3, 3, 3, 9, 3, 5, 17)  (2, 3, 9, 5, 3, 9, 17)  (11, 0, 0, 17, 0, 0, 17)   \n13  (4, 3, 3, 9, 3, 5, 17)  (3, 3, 8, 5, 3, 9, 17)  (13, 0, 0, 17, 0, 0, 17)   \n14  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 8, 5, 3, 9, 17)  (11, 0, 0, 17, 0, 0, 17)   \n15  (2, 3, 3, 9, 3, 5, 17)  (4, 3, 9, 5, 3, 9, 17)  (15, 0, 0, 17, 0, 0, 17)   \n16  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 1, 5, 3, 9, 17)  (10, 0, 0, 17, 0, 0, 17)   \n17  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)  (10, 0, 0, 17, 0, 0, 17)   \n18  (0, 3, 2, 9, 3, 5, 17)  (0, 3, 0, 5, 3, 9, 17)   (4, 0, 0, 17, 0, 0, 17)   \n19  (2, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)  (11, 0, 0, 17, 0, 0, 17)   \n20  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)  (10, 0, 0, 17, 0, 0, 17)   \n21  (3, 3, 1, 9, 3, 5, 17)  (4, 3, 0, 5, 3, 9, 17)  (10, 0, 0, 17, 0, 0, 17)   \n22  (2, 3, 3, 9, 3, 5, 17)  (5, 3, 0, 5, 3, 9, 17)  (14, 0, 0, 17, 0, 0, 17)   \n23  (4, 0, 3, 9, 3, 5, 17)  (5, 3, 4, 5, 3, 9, 17)  (15, 0, 0, 17, 0, 0, 17)   \n24  (2, 0, 3, 9, 3, 5, 17)  (5, 2, 4, 5, 3, 9, 17)  (17, 0, 0, 17, 0, 0, 17)   \n\n                         5_x                       6_x  \\\n0    (4, 2, 0, 11, 5, 1, 17)   (0, 5, 3, 1, 10, 6, 17)   \n1    (4, 2, 0, 11, 5, 1, 17)   (0, 6, 0, 1, 10, 6, 17)   \n2   (11, 0, 0, 11, 5, 1, 17)   (1, 5, 0, 1, 10, 6, 17)   \n3    (6, 1, 0, 11, 5, 1, 17)   (1, 4, 0, 1, 10, 6, 17)   \n4    (5, 1, 0, 11, 5, 1, 17)   (1, 4, 0, 1, 10, 6, 17)   \n5   (11, 0, 0, 11, 5, 1, 17)   (0, 8, 0, 1, 10, 6, 17)   \n6    (5, 5, 0, 11, 5, 1, 17)   (0, 5, 0, 1, 10, 6, 17)   \n7   (11, 1, 0, 11, 5, 1, 17)  (0, 10, 0, 1, 10, 6, 17)   \n8   (11, 2, 0, 11, 5, 1, 17)   (0, 9, 0, 1, 10, 6, 17)   \n9   (11, 3, 0, 11, 5, 1, 17)   (1, 6, 0, 1, 10, 6, 17)   \n10  (11, 1, 0, 11, 5, 1, 17)   (0, 8, 2, 1, 10, 6, 17)   \n11  (11, 1, 0, 11, 5, 1, 17)   (0, 9, 0, 1, 10, 6, 17)   \n12  (11, 1, 0, 11, 5, 1, 17)   (1, 7, 0, 1, 10, 6, 17)   \n13  (11, 0, 0, 11, 5, 1, 17)   (0, 8, 1, 1, 10, 6, 17)   \n14  (11, 0, 0, 11, 5, 1, 17)   (0, 8, 0, 1, 10, 6, 17)   \n15  (11, 0, 0, 11, 5, 1, 17)   (1, 7, 0, 1, 10, 6, 17)   \n16  (11, 1, 0, 11, 5, 1, 17)   (0, 8, 0, 1, 10, 6, 17)   \n17  (11, 1, 0, 11, 5, 1, 17)   (0, 8, 0, 1, 10, 6, 17)   \n18  (10, 1, 0, 11, 5, 1, 17)  (0, 10, 0, 1, 10, 6, 17)   \n19  (11, 1, 0, 11, 5, 1, 17)  (0, 10, 0, 1, 10, 6, 17)   \n20  (11, 1, 0, 11, 5, 1, 17)  (0, 10, 0, 1, 10, 6, 17)   \n21  (11, 1, 0, 11, 5, 1, 17)   (0, 8, 0, 1, 10, 6, 17)   \n22  (11, 0, 0, 11, 5, 1, 17)   (1, 3, 0, 1, 10, 6, 17)   \n23  (11, 0, 0, 11, 5, 1, 17)   (0, 3, 3, 1, 10, 6, 17)   \n24  (11, 0, 0, 11, 5, 1, 17)   (0, 3, 0, 1, 10, 6, 17)   \n\n                         7_x                     8_x                     9_x  \\\n0    (2, 0, 3, 3, 11, 3, 17)  (8, 3, 0, 8, 4, 5, 17)  (2, 7, 2, 3, 9, 5, 17)   \n1    (2, 0, 3, 3, 11, 3, 17)  (6, 3, 1, 8, 4, 5, 17)  (1, 7, 2, 3, 9, 5, 17)   \n2    (0, 0, 3, 3, 11, 3, 17)  (3, 4, 0, 8, 4, 5, 17)  (0, 3, 3, 3, 9, 5, 17)   \n3    (3, 1, 3, 3, 11, 3, 17)  (4, 0, 1, 8, 4, 5, 17)  (2, 7, 0, 3, 9, 5, 17)   \n4    (2, 7, 3, 3, 11, 3, 17)  (7, 1, 0, 8, 4, 5, 17)  (1, 7, 0, 3, 9, 5, 17)   \n5    (2, 3, 0, 3, 11, 3, 17)  (1, 0, 0, 8, 4, 5, 17)  (1, 5, 0, 3, 9, 5, 17)   \n6    (3, 1, 0, 3, 11, 3, 17)  (0, 1, 0, 8, 4, 5, 17)  (2, 5, 0, 3, 9, 5, 17)   \n7    (3, 0, 0, 3, 11, 3, 17)  (4, 2, 0, 8, 4, 5, 17)  (2, 6, 3, 3, 9, 5, 17)   \n8    (3, 0, 2, 3, 11, 3, 17)  (4, 3, 0, 8, 4, 5, 17)  (1, 9, 3, 3, 9, 5, 17)   \n9    (3, 1, 1, 3, 11, 3, 17)  (5, 4, 0, 8, 4, 5, 17)  (1, 7, 3, 3, 9, 5, 17)   \n10   (3, 1, 1, 3, 11, 3, 17)  (4, 3, 0, 8, 4, 5, 17)  (1, 9, 2, 3, 9, 5, 17)   \n11   (3, 0, 1, 3, 11, 3, 17)  (3, 3, 0, 8, 4, 5, 17)  (1, 8, 3, 3, 9, 5, 17)   \n12   (3, 1, 0, 3, 11, 3, 17)  (4, 4, 0, 8, 4, 5, 17)  (2, 8, 3, 3, 9, 5, 17)   \n13   (3, 1, 0, 3, 11, 3, 17)  (4, 0, 2, 8, 4, 5, 17)  (2, 9, 0, 3, 9, 5, 17)   \n14   (3, 2, 1, 3, 11, 3, 17)  (4, 0, 0, 8, 4, 5, 17)  (2, 9, 3, 3, 9, 5, 17)   \n15   (3, 2, 1, 3, 11, 3, 17)  (4, 2, 1, 8, 4, 5, 17)  (2, 8, 3, 3, 9, 5, 17)   \n16   (3, 5, 1, 3, 11, 3, 17)  (1, 4, 0, 8, 4, 5, 17)  (1, 7, 3, 3, 9, 5, 17)   \n17   (3, 5, 2, 3, 11, 3, 17)  (1, 4, 0, 8, 4, 5, 17)  (1, 7, 3, 3, 9, 5, 17)   \n18   (3, 2, 0, 3, 11, 3, 17)  (8, 3, 0, 8, 4, 5, 17)  (0, 5, 3, 3, 9, 5, 17)   \n19   (3, 2, 0, 3, 11, 3, 17)  (1, 4, 0, 8, 4, 5, 17)  (2, 7, 2, 3, 9, 5, 17)   \n20   (3, 1, 1, 3, 11, 3, 17)  (1, 4, 0, 8, 4, 5, 17)  (1, 8, 2, 3, 9, 5, 17)   \n21   (3, 5, 1, 3, 11, 3, 17)  (1, 4, 0, 8, 4, 5, 17)  (1, 6, 3, 3, 9, 5, 17)   \n22  (1, 10, 0, 3, 11, 3, 17)  (0, 3, 0, 8, 4, 5, 17)  (2, 5, 0, 3, 9, 5, 17)   \n23   (2, 7, 0, 3, 11, 3, 17)  (4, 2, 0, 8, 4, 5, 17)  (2, 7, 0, 3, 9, 5, 17)   \n24   (1, 8, 1, 3, 11, 3, 17)  (2, 0, 0, 8, 4, 5, 17)  (2, 6, 1, 3, 9, 5, 17)   \n\n    ...                                  60_y  \\\n0   ...  (751, 770, 747, 913, 893, 914, 2720)   \n1   ...  (908, 880, 913, 913, 893, 914, 2720)   \n2   ...  (698, 634, 666, 913, 893, 914, 2720)   \n3   ...  (777, 783, 839, 913, 893, 914, 2720)   \n4   ...  (906, 892, 899, 913, 893, 914, 2720)   \n5   ...  (579, 540, 505, 913, 893, 914, 2720)   \n6   ...  (494, 506, 482, 913, 893, 914, 2720)   \n7   ...  (835, 821, 802, 913, 893, 914, 2720)   \n8   ...  (913, 893, 914, 913, 893, 914, 2720)   \n9   ...  (913, 893, 914, 913, 893, 914, 2720)   \n10  ...  (913, 893, 914, 913, 893, 914, 2720)   \n11  ...  (913, 893, 914, 913, 893, 914, 2720)   \n12  ...  (913, 893, 914, 913, 893, 914, 2720)   \n13  ...  (913, 893, 914, 913, 893, 914, 2720)   \n14  ...  (913, 893, 914, 913, 893, 914, 2720)   \n15  ...  (913, 893, 914, 913, 893, 914, 2720)   \n16  ...  (907, 883, 912, 913, 893, 914, 2720)   \n17  ...  (910, 888, 912, 913, 893, 914, 2720)   \n18  ...  (571, 562, 574, 913, 893, 914, 2720)   \n19  ...  (775, 769, 749, 913, 893, 914, 2720)   \n20  ...  (892, 870, 900, 913, 893, 914, 2720)   \n21  ...  (890, 867, 901, 913, 893, 914, 2720)   \n22  ...  (805, 793, 755, 913, 893, 914, 2720)   \n23  ...  (913, 893, 914, 913, 893, 914, 2720)   \n24  ...  (913, 893, 914, 913, 893, 914, 2720)   \n\n                                    61_y  \\\n0   (804, 808, 853, 912, 907, 918, 2737)   \n1   (912, 907, 918, 912, 907, 918, 2737)   \n2   (628, 695, 644, 912, 907, 918, 2737)   \n3   (763, 833, 809, 912, 907, 918, 2737)   \n4   (904, 899, 913, 912, 907, 918, 2737)   \n5   (544, 586, 466, 912, 907, 918, 2737)   \n6   (455, 560, 465, 912, 907, 918, 2737)   \n7   (818, 827, 823, 912, 907, 918, 2737)   \n8   (912, 907, 918, 912, 907, 918, 2737)   \n9   (912, 907, 918, 912, 907, 918, 2737)   \n10  (912, 907, 918, 912, 907, 918, 2737)   \n11  (912, 907, 918, 912, 907, 918, 2737)   \n12  (912, 907, 918, 912, 907, 918, 2737)   \n13  (912, 907, 918, 912, 907, 918, 2737)   \n14  (912, 907, 918, 912, 907, 918, 2737)   \n15  (912, 907, 918, 912, 907, 918, 2737)   \n16  (902, 898, 916, 912, 907, 918, 2737)   \n17  (908, 903, 917, 912, 907, 918, 2737)   \n18  (570, 603, 567, 912, 907, 918, 2737)   \n19  (777, 779, 752, 912, 907, 918, 2737)   \n20  (887, 887, 905, 912, 907, 918, 2737)   \n21  (887, 883, 904, 912, 907, 918, 2737)   \n22  (809, 821, 762, 912, 907, 918, 2737)   \n23  (912, 907, 918, 912, 907, 918, 2737)   \n24  (912, 907, 918, 912, 907, 918, 2737)   \n\n                                    62_y  \\\n0   (808, 833, 852, 918, 914, 922, 2754)   \n1   (918, 914, 922, 918, 914, 922, 2754)   \n2   (707, 594, 633, 918, 914, 922, 2754)   \n3   (772, 858, 794, 918, 914, 922, 2754)   \n4   (917, 909, 919, 918, 914, 922, 2754)   \n5   (548, 588, 467, 918, 914, 922, 2754)   \n6   (444, 585, 441, 918, 914, 922, 2754)   \n7   (831, 837, 807, 918, 914, 922, 2754)   \n8   (918, 914, 922, 918, 914, 922, 2754)   \n9   (918, 914, 922, 918, 914, 922, 2754)   \n10  (918, 914, 922, 918, 914, 922, 2754)   \n11  (918, 914, 922, 918, 914, 922, 2754)   \n12  (918, 914, 922, 918, 914, 922, 2754)   \n13  (918, 914, 922, 918, 914, 922, 2754)   \n14  (918, 914, 922, 918, 914, 922, 2754)   \n15  (918, 914, 922, 918, 914, 922, 2754)   \n16  (908, 903, 919, 918, 914, 922, 2754)   \n17  (914, 909, 921, 918, 914, 922, 2754)   \n18  (570, 594, 567, 918, 914, 922, 2754)   \n19  (779, 788, 749, 918, 914, 922, 2754)   \n20  (894, 891, 906, 918, 914, 922, 2754)   \n21  (898, 889, 906, 918, 914, 922, 2754)   \n22  (799, 825, 768, 918, 914, 922, 2754)   \n23  (918, 914, 922, 918, 914, 922, 2754)   \n24  (918, 914, 922, 918, 914, 922, 2754)   \n\n                                    63_y  \\\n0   (815, 788, 756, 936, 909, 926, 2771)   \n1   (931, 896, 925, 936, 909, 926, 2771)   \n2   (721, 683, 616, 936, 909, 926, 2771)   \n3   (839, 842, 753, 936, 909, 926, 2771)   \n4   (931, 901, 915, 936, 909, 926, 2771)   \n5   (600, 537, 495, 936, 909, 926, 2771)   \n6   (529, 468, 479, 936, 909, 926, 2771)   \n7   (846, 838, 804, 936, 909, 926, 2771)   \n8   (936, 909, 926, 936, 909, 926, 2771)   \n9   (936, 909, 926, 936, 909, 926, 2771)   \n10  (936, 909, 926, 936, 909, 926, 2771)   \n11  (936, 909, 926, 936, 909, 926, 2771)   \n12  (936, 909, 926, 936, 909, 926, 2771)   \n13  (936, 909, 926, 936, 909, 926, 2771)   \n14  (936, 909, 926, 936, 909, 926, 2771)   \n15  (936, 909, 926, 936, 909, 926, 2771)   \n16  (929, 899, 920, 936, 909, 926, 2771)   \n17  (934, 905, 923, 936, 909, 926, 2771)   \n18  (591, 552, 569, 936, 909, 926, 2771)   \n19  (800, 782, 743, 936, 909, 926, 2771)   \n20  (912, 886, 909, 936, 909, 926, 2771)   \n21  (913, 886, 910, 936, 909, 926, 2771)   \n22  (832, 825, 762, 936, 909, 926, 2771)   \n23  (936, 909, 926, 936, 909, 926, 2771)   \n24  (936, 909, 926, 936, 909, 926, 2771)   \n\n                                    64_y  \\\n0   (795, 800, 767, 938, 910, 940, 2788)   \n1   (938, 910, 940, 938, 910, 940, 2788)   \n2   (725, 794, 679, 938, 910, 940, 2788)   \n3   (806, 783, 696, 938, 910, 940, 2788)   \n4   (918, 896, 919, 938, 910, 940, 2788)   \n5   (595, 525, 510, 938, 910, 940, 2788)   \n6   (505, 495, 493, 938, 910, 940, 2788)   \n7   (857, 830, 822, 938, 910, 940, 2788)   \n8   (938, 910, 940, 938, 910, 940, 2788)   \n9   (938, 910, 940, 938, 910, 940, 2788)   \n10  (938, 910, 940, 938, 910, 940, 2788)   \n11  (938, 910, 940, 938, 910, 940, 2788)   \n12  (938, 910, 940, 938, 910, 940, 2788)   \n13  (938, 910, 940, 938, 910, 940, 2788)   \n14  (938, 910, 940, 938, 910, 940, 2788)   \n15  (938, 910, 940, 938, 910, 940, 2788)   \n16  (929, 900, 933, 938, 910, 940, 2788)   \n17  (934, 903, 937, 938, 910, 940, 2788)   \n18  (597, 544, 581, 938, 910, 940, 2788)   \n19  (803, 775, 764, 938, 910, 940, 2788)   \n20  (910, 884, 921, 938, 910, 940, 2788)   \n21  (913, 883, 917, 938, 910, 940, 2788)   \n22  (827, 823, 785, 938, 910, 940, 2788)   \n23  (938, 910, 940, 938, 910, 940, 2788)   \n24  (938, 910, 940, 938, 910, 940, 2788)   \n\n                                    65_y  \\\n0   (837, 823, 822, 939, 932, 934, 2805)   \n1   (934, 919, 933, 939, 932, 934, 2805)   \n2   (769, 762, 709, 939, 932, 934, 2805)   \n3   (782, 805, 687, 939, 932, 934, 2805)   \n4   (930, 918, 909, 939, 932, 934, 2805)   \n5   (580, 589, 467, 939, 932, 934, 2805)   \n6   (428, 589, 435, 939, 932, 934, 2805)   \n7   (848, 856, 818, 939, 932, 934, 2805)   \n8   (939, 932, 934, 939, 932, 934, 2805)   \n9   (939, 932, 934, 939, 932, 934, 2805)   \n10  (939, 932, 934, 939, 932, 934, 2805)   \n11  (939, 932, 934, 939, 932, 934, 2805)   \n12  (939, 932, 934, 939, 932, 934, 2805)   \n13  (939, 932, 934, 939, 932, 934, 2805)   \n14  (939, 932, 934, 939, 932, 934, 2805)   \n15  (939, 932, 934, 939, 932, 934, 2805)   \n16  (930, 922, 927, 939, 932, 934, 2805)   \n17  (936, 928, 932, 939, 932, 934, 2805)   \n18  (585, 595, 571, 939, 932, 934, 2805)   \n19  (805, 811, 753, 939, 932, 934, 2805)   \n20  (911, 910, 920, 939, 932, 934, 2805)   \n21  (916, 907, 913, 939, 932, 934, 2805)   \n22  (836, 850, 769, 939, 932, 934, 2805)   \n23  (939, 932, 934, 939, 932, 934, 2805)   \n24  (939, 932, 934, 939, 932, 934, 2805)   \n\n                                    66_y  \\\n0   (828, 796, 777, 951, 932, 939, 2822)   \n1   (946, 919, 938, 951, 932, 939, 2822)   \n2   (750, 725, 736, 951, 932, 939, 2822)   \n3   (863, 830, 740, 951, 932, 939, 2822)   \n4   (934, 917, 927, 951, 932, 939, 2822)   \n5   (610, 562, 477, 951, 932, 939, 2822)   \n6   (506, 517, 470, 951, 932, 939, 2822)   \n7   (861, 854, 816, 951, 932, 939, 2822)   \n8   (951, 932, 939, 951, 932, 939, 2822)   \n9   (951, 932, 939, 951, 932, 939, 2822)   \n10  (951, 932, 939, 951, 932, 939, 2822)   \n11  (951, 932, 939, 951, 932, 939, 2822)   \n12  (951, 932, 939, 951, 932, 939, 2822)   \n13  (951, 932, 939, 951, 932, 939, 2822)   \n14  (951, 932, 939, 951, 932, 939, 2822)   \n15  (951, 932, 939, 951, 932, 939, 2822)   \n16  (943, 925, 932, 951, 932, 939, 2822)   \n17  (947, 926, 938, 951, 932, 939, 2822)   \n18  (592, 576, 571, 951, 932, 939, 2822)   \n19  (815, 807, 747, 951, 932, 939, 2822)   \n20  (925, 912, 922, 951, 932, 939, 2822)   \n21  (921, 908, 923, 951, 932, 939, 2822)   \n22  (833, 835, 777, 951, 932, 939, 2822)   \n23  (951, 932, 939, 951, 932, 939, 2822)   \n24  (951, 932, 939, 951, 932, 939, 2822)   \n\n                                    67_y  \\\n0   (815, 765, 860, 958, 931, 950, 2839)   \n1   (958, 930, 950, 958, 931, 950, 2839)   \n2   (687, 730, 594, 958, 931, 950, 2839)   \n3   (808, 768, 738, 958, 931, 950, 2839)   \n4   (939, 922, 936, 958, 931, 950, 2839)   \n5   (617, 522, 525, 958, 931, 950, 2839)   \n6   (540, 432, 543, 958, 931, 950, 2839)   \n7   (870, 853, 829, 958, 931, 950, 2839)   \n8   (958, 931, 950, 958, 931, 950, 2839)   \n9   (958, 931, 950, 958, 931, 950, 2839)   \n10  (958, 931, 950, 958, 931, 950, 2839)   \n11  (958, 931, 950, 958, 931, 950, 2839)   \n12  (958, 931, 950, 958, 931, 950, 2839)   \n13  (958, 931, 950, 958, 931, 950, 2839)   \n14  (958, 931, 950, 958, 931, 950, 2839)   \n15  (958, 931, 950, 958, 931, 950, 2839)   \n16  (948, 920, 942, 958, 931, 950, 2839)   \n17  (954, 925, 946, 958, 931, 950, 2839)   \n18  (623, 527, 571, 958, 931, 950, 2839)   \n19  (829, 800, 756, 958, 931, 950, 2839)   \n20  (929, 905, 929, 958, 931, 950, 2839)   \n21  (933, 903, 927, 958, 931, 950, 2839)   \n22  (847, 841, 782, 958, 931, 950, 2839)   \n23  (958, 931, 950, 958, 931, 950, 2839)   \n24  (958, 931, 950, 958, 931, 950, 2839)   \n\n                                    68_y                                  69_y  \n0   (796, 752, 878, 952, 943, 961, 2856)  (782, 800, 840, 959, 960, 954, 2873)  \n1   (944, 899, 960, 952, 943, 961, 2856)  (956, 960, 954, 959, 960, 954, 2873)  \n2   (716, 795, 685, 952, 943, 961, 2856)  (764, 789, 687, 959, 960, 954, 2873)  \n3   (793, 798, 721, 952, 943, 961, 2856)  (768, 820, 711, 959, 960, 954, 2873)  \n4   (940, 937, 935, 952, 943, 961, 2856)  (954, 949, 945, 959, 960, 954, 2873)  \n5   (589, 545, 532, 952, 943, 961, 2856)  (583, 619, 463, 959, 960, 954, 2873)  \n6   (473, 494, 522, 952, 943, 961, 2856)  (413, 609, 420, 959, 960, 954, 2873)  \n7   (865, 865, 839, 952, 943, 961, 2856)  (868, 881, 817, 959, 960, 954, 2873)  \n8   (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n9   (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n10  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n11  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n12  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n13  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n14  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n15  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n16  (942, 930, 955, 952, 943, 961, 2856)  (949, 951, 951, 959, 960, 954, 2873)  \n17  (949, 937, 960, 952, 943, 961, 2856)  (956, 955, 953, 959, 960, 954, 2873)  \n18  (597, 561, 587, 952, 943, 961, 2856)  (589, 600, 576, 959, 960, 954, 2873)  \n19  (810, 806, 777, 952, 943, 961, 2856)  (809, 835, 768, 959, 960, 954, 2873)  \n20  (924, 914, 943, 952, 943, 961, 2856)  (930, 937, 935, 959, 960, 954, 2873)  \n21  (924, 912, 944, 952, 943, 961, 2856)  (931, 935, 937, 959, 960, 954, 2873)  \n22  (825, 848, 794, 952, 943, 961, 2856)  (844, 864, 789, 959, 960, 954, 2873)  \n23  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n24  (952, 943, 961, 952, 943, 961, 2856)  (959, 960, 954, 959, 960, 954, 2873)  \n\n[25 rows x 140 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0_x</th>\n      <th>0_y</th>\n      <th>2_x</th>\n      <th>3_x</th>\n      <th>4_x</th>\n      <th>5_x</th>\n      <th>6_x</th>\n      <th>7_x</th>\n      <th>8_x</th>\n      <th>9_x</th>\n      <th>...</th>\n      <th>60_y</th>\n      <th>61_y</th>\n      <th>62_y</th>\n      <th>63_y</th>\n      <th>64_y</th>\n      <th>65_y</th>\n      <th>66_y</th>\n      <th>67_y</th>\n      <th>68_y</th>\n      <th>69_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 1</td>\n      <td>(4, 3, 0, 5, 8, 4, 17)</td>\n      <td>(1, 3, 0, 9, 3, 5, 17)</td>\n      <td>(5, 3, 0, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(4, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 5, 3, 1, 10, 6, 17)</td>\n      <td>(2, 0, 3, 3, 11, 3, 17)</td>\n      <td>(8, 3, 0, 8, 4, 5, 17)</td>\n      <td>(2, 7, 2, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(751, 770, 747, 913, 893, 914, 2720)</td>\n      <td>(804, 808, 853, 912, 907, 918, 2737)</td>\n      <td>(808, 833, 852, 918, 914, 922, 2754)</td>\n      <td>(815, 788, 756, 936, 909, 926, 2771)</td>\n      <td>(795, 800, 767, 938, 910, 940, 2788)</td>\n      <td>(837, 823, 822, 939, 932, 934, 2805)</td>\n      <td>(828, 796, 777, 951, 932, 939, 2822)</td>\n      <td>(815, 765, 860, 958, 931, 950, 2839)</td>\n      <td>(796, 752, 878, 952, 943, 961, 2856)</td>\n      <td>(782, 800, 840, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeClassifier 2</td>\n      <td>(4, 3, 2, 5, 8, 4, 17)</td>\n      <td>(2, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(4, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 6, 0, 1, 10, 6, 17)</td>\n      <td>(2, 0, 3, 3, 11, 3, 17)</td>\n      <td>(6, 3, 1, 8, 4, 5, 17)</td>\n      <td>(1, 7, 2, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(908, 880, 913, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(931, 896, 925, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(934, 919, 933, 939, 932, 934, 2805)</td>\n      <td>(946, 919, 938, 951, 932, 939, 2822)</td>\n      <td>(958, 930, 950, 958, 931, 950, 2839)</td>\n      <td>(944, 899, 960, 952, 943, 961, 2856)</td>\n      <td>(956, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DecisionTreeClassifier 3</td>\n      <td>(5, 2, 1, 5, 8, 4, 17)</td>\n      <td>(2, 3, 0, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(3, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 5, 0, 1, 10, 6, 17)</td>\n      <td>(0, 0, 3, 3, 11, 3, 17)</td>\n      <td>(3, 4, 0, 8, 4, 5, 17)</td>\n      <td>(0, 3, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(698, 634, 666, 913, 893, 914, 2720)</td>\n      <td>(628, 695, 644, 912, 907, 918, 2737)</td>\n      <td>(707, 594, 633, 918, 914, 922, 2754)</td>\n      <td>(721, 683, 616, 936, 909, 926, 2771)</td>\n      <td>(725, 794, 679, 938, 910, 940, 2788)</td>\n      <td>(769, 762, 709, 939, 932, 934, 2805)</td>\n      <td>(750, 725, 736, 951, 932, 939, 2822)</td>\n      <td>(687, 730, 594, 958, 931, 950, 2839)</td>\n      <td>(716, 795, 685, 952, 943, 961, 2856)</td>\n      <td>(764, 789, 687, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier 4</td>\n      <td>(3, 6, 0, 5, 8, 4, 17)</td>\n      <td>(4, 0, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 4, 5, 3, 9, 17)</td>\n      <td>(4, 0, 0, 17, 0, 0, 17)</td>\n      <td>(6, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 4, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 3, 3, 11, 3, 17)</td>\n      <td>(4, 0, 1, 8, 4, 5, 17)</td>\n      <td>(2, 7, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(777, 783, 839, 913, 893, 914, 2720)</td>\n      <td>(763, 833, 809, 912, 907, 918, 2737)</td>\n      <td>(772, 858, 794, 918, 914, 922, 2754)</td>\n      <td>(839, 842, 753, 936, 909, 926, 2771)</td>\n      <td>(806, 783, 696, 938, 910, 940, 2788)</td>\n      <td>(782, 805, 687, 939, 932, 934, 2805)</td>\n      <td>(863, 830, 740, 951, 932, 939, 2822)</td>\n      <td>(808, 768, 738, 958, 931, 950, 2839)</td>\n      <td>(793, 798, 721, 952, 943, 961, 2856)</td>\n      <td>(768, 820, 711, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DecisionTreeClassifier 5</td>\n      <td>(3, 3, 0, 5, 8, 4, 17)</td>\n      <td>(4, 3, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 4, 5, 3, 9, 17)</td>\n      <td>(6, 0, 0, 17, 0, 0, 17)</td>\n      <td>(5, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 4, 0, 1, 10, 6, 17)</td>\n      <td>(2, 7, 3, 3, 11, 3, 17)</td>\n      <td>(7, 1, 0, 8, 4, 5, 17)</td>\n      <td>(1, 7, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(906, 892, 899, 913, 893, 914, 2720)</td>\n      <td>(904, 899, 913, 912, 907, 918, 2737)</td>\n      <td>(917, 909, 919, 918, 914, 922, 2754)</td>\n      <td>(931, 901, 915, 936, 909, 926, 2771)</td>\n      <td>(918, 896, 919, 938, 910, 940, 2788)</td>\n      <td>(930, 918, 909, 939, 932, 934, 2805)</td>\n      <td>(934, 917, 927, 951, 932, 939, 2822)</td>\n      <td>(939, 922, 936, 958, 931, 950, 2839)</td>\n      <td>(940, 937, 935, 952, 943, 961, 2856)</td>\n      <td>(954, 949, 945, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RandomForestClassifier 4</td>\n      <td>(5, 4, 0, 5, 8, 4, 17)</td>\n      <td>(8, 3, 2, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(5, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(2, 3, 0, 3, 11, 3, 17)</td>\n      <td>(1, 0, 0, 8, 4, 5, 17)</td>\n      <td>(1, 5, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(579, 540, 505, 913, 893, 914, 2720)</td>\n      <td>(544, 586, 466, 912, 907, 918, 2737)</td>\n      <td>(548, 588, 467, 918, 914, 922, 2754)</td>\n      <td>(600, 537, 495, 936, 909, 926, 2771)</td>\n      <td>(595, 525, 510, 938, 910, 940, 2788)</td>\n      <td>(580, 589, 467, 939, 932, 934, 2805)</td>\n      <td>(610, 562, 477, 951, 932, 939, 2822)</td>\n      <td>(617, 522, 525, 958, 931, 950, 2839)</td>\n      <td>(589, 545, 532, 952, 943, 961, 2856)</td>\n      <td>(583, 619, 463, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RandomForestClassifier 5</td>\n      <td>(5, 4, 0, 5, 8, 4, 17)</td>\n      <td>(8, 3, 2, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(2, 0, 0, 17, 0, 0, 17)</td>\n      <td>(5, 5, 0, 11, 5, 1, 17)</td>\n      <td>(0, 5, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(0, 1, 0, 8, 4, 5, 17)</td>\n      <td>(2, 5, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(494, 506, 482, 913, 893, 914, 2720)</td>\n      <td>(455, 560, 465, 912, 907, 918, 2737)</td>\n      <td>(444, 585, 441, 918, 914, 922, 2754)</td>\n      <td>(529, 468, 479, 936, 909, 926, 2771)</td>\n      <td>(505, 495, 493, 938, 910, 940, 2788)</td>\n      <td>(428, 589, 435, 939, 932, 934, 2805)</td>\n      <td>(506, 517, 470, 951, 932, 939, 2822)</td>\n      <td>(540, 432, 543, 958, 931, 950, 2839)</td>\n      <td>(473, 494, 522, 952, 943, 961, 2856)</td>\n      <td>(413, 609, 420, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GradientBoostingClassifier 1</td>\n      <td>(4, 5, 4, 5, 8, 4, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(9, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 0, 3, 11, 3, 17)</td>\n      <td>(4, 2, 0, 8, 4, 5, 17)</td>\n      <td>(2, 6, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(835, 821, 802, 913, 893, 914, 2720)</td>\n      <td>(818, 827, 823, 912, 907, 918, 2737)</td>\n      <td>(831, 837, 807, 918, 914, 922, 2754)</td>\n      <td>(846, 838, 804, 936, 909, 926, 2771)</td>\n      <td>(857, 830, 822, 938, 910, 940, 2788)</td>\n      <td>(848, 856, 818, 939, 932, 934, 2805)</td>\n      <td>(861, 854, 816, 951, 932, 939, 2822)</td>\n      <td>(870, 853, 829, 958, 931, 950, 2839)</td>\n      <td>(865, 865, 839, 952, 943, 961, 2856)</td>\n      <td>(868, 881, 817, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GradientBoostingClassifier 2</td>\n      <td>(4, 4, 3, 5, 8, 4, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(3, 3, 8, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 2, 0, 11, 5, 1, 17)</td>\n      <td>(0, 9, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 2, 3, 11, 3, 17)</td>\n      <td>(4, 3, 0, 8, 4, 5, 17)</td>\n      <td>(1, 9, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GradientBoostingClassifier 3</td>\n      <td>(5, 1, 4, 5, 8, 4, 17)</td>\n      <td>(5, 3, 2, 9, 3, 5, 17)</td>\n      <td>(5, 3, 9, 5, 3, 9, 17)</td>\n      <td>(14, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 3, 0, 11, 5, 1, 17)</td>\n      <td>(1, 6, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(5, 4, 0, 8, 4, 5, 17)</td>\n      <td>(1, 7, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBClassifier 1</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 2, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(4, 3, 0, 8, 4, 5, 17)</td>\n      <td>(1, 9, 2, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGBClassifier 2</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 9, 0, 1, 10, 6, 17)</td>\n      <td>(3, 0, 1, 3, 11, 3, 17)</td>\n      <td>(3, 3, 0, 8, 4, 5, 17)</td>\n      <td>(1, 8, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGBClassifier 3</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(3, 3, 3, 9, 3, 5, 17)</td>\n      <td>(2, 3, 9, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(1, 7, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(4, 4, 0, 8, 4, 5, 17)</td>\n      <td>(2, 8, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGBClassifier 4</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(4, 3, 3, 9, 3, 5, 17)</td>\n      <td>(3, 3, 8, 5, 3, 9, 17)</td>\n      <td>(13, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 1, 1, 10, 6, 17)</td>\n      <td>(3, 1, 0, 3, 11, 3, 17)</td>\n      <td>(4, 0, 2, 8, 4, 5, 17)</td>\n      <td>(2, 9, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGBClassifier 5</td>\n      <td>(4, 4, 4, 5, 8, 4, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 8, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 1, 3, 11, 3, 17)</td>\n      <td>(4, 0, 0, 8, 4, 5, 17)</td>\n      <td>(2, 9, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGBClassifier 6</td>\n      <td>(4, 1, 4, 5, 8, 4, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(4, 3, 9, 5, 3, 9, 17)</td>\n      <td>(15, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 7, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 1, 3, 11, 3, 17)</td>\n      <td>(4, 2, 1, 8, 4, 5, 17)</td>\n      <td>(2, 8, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>XGBRFClassifier 1</td>\n      <td>(4, 2, 3, 5, 8, 4, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 1, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>(1, 7, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(907, 883, 912, 913, 893, 914, 2720)</td>\n      <td>(902, 898, 916, 912, 907, 918, 2737)</td>\n      <td>(908, 903, 919, 918, 914, 922, 2754)</td>\n      <td>(929, 899, 920, 936, 909, 926, 2771)</td>\n      <td>(929, 900, 933, 938, 910, 940, 2788)</td>\n      <td>(930, 922, 927, 939, 932, 934, 2805)</td>\n      <td>(943, 925, 932, 951, 932, 939, 2822)</td>\n      <td>(948, 920, 942, 958, 931, 950, 2839)</td>\n      <td>(942, 930, 955, 952, 943, 961, 2856)</td>\n      <td>(949, 951, 951, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>XGBRFClassifier 2</td>\n      <td>(4, 2, 3, 5, 8, 4, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 2, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>(1, 7, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(910, 888, 912, 913, 893, 914, 2720)</td>\n      <td>(908, 903, 917, 912, 907, 918, 2737)</td>\n      <td>(914, 909, 921, 918, 914, 922, 2754)</td>\n      <td>(934, 905, 923, 936, 909, 926, 2771)</td>\n      <td>(934, 903, 937, 938, 910, 940, 2788)</td>\n      <td>(936, 928, 932, 939, 932, 934, 2805)</td>\n      <td>(947, 926, 938, 951, 932, 939, 2822)</td>\n      <td>(954, 925, 946, 958, 931, 950, 2839)</td>\n      <td>(949, 937, 960, 952, 943, 961, 2856)</td>\n      <td>(956, 955, 953, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>XGBRFClassifier 3</td>\n      <td>(4, 3, 2, 5, 8, 4, 17)</td>\n      <td>(0, 3, 2, 9, 3, 5, 17)</td>\n      <td>(0, 3, 0, 5, 3, 9, 17)</td>\n      <td>(4, 0, 0, 17, 0, 0, 17)</td>\n      <td>(10, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 0, 3, 11, 3, 17)</td>\n      <td>(8, 3, 0, 8, 4, 5, 17)</td>\n      <td>(0, 5, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(571, 562, 574, 913, 893, 914, 2720)</td>\n      <td>(570, 603, 567, 912, 907, 918, 2737)</td>\n      <td>(570, 594, 567, 918, 914, 922, 2754)</td>\n      <td>(591, 552, 569, 936, 909, 926, 2771)</td>\n      <td>(597, 544, 581, 938, 910, 940, 2788)</td>\n      <td>(585, 595, 571, 939, 932, 934, 2805)</td>\n      <td>(592, 576, 571, 951, 932, 939, 2822)</td>\n      <td>(623, 527, 571, 958, 931, 950, 2839)</td>\n      <td>(597, 561, 587, 952, 943, 961, 2856)</td>\n      <td>(589, 600, 576, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>XGBRFClassifier 4</td>\n      <td>(4, 3, 3, 5, 8, 4, 17)</td>\n      <td>(2, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(11, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 2, 0, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>(2, 7, 2, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(775, 769, 749, 913, 893, 914, 2720)</td>\n      <td>(777, 779, 752, 912, 907, 918, 2737)</td>\n      <td>(779, 788, 749, 918, 914, 922, 2754)</td>\n      <td>(800, 782, 743, 936, 909, 926, 2771)</td>\n      <td>(803, 775, 764, 938, 910, 940, 2788)</td>\n      <td>(805, 811, 753, 939, 932, 934, 2805)</td>\n      <td>(815, 807, 747, 951, 932, 939, 2822)</td>\n      <td>(829, 800, 756, 958, 931, 950, 2839)</td>\n      <td>(810, 806, 777, 952, 943, 961, 2856)</td>\n      <td>(809, 835, 768, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>XGBRFClassifier 5</td>\n      <td>(4, 1, 3, 5, 8, 4, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 10, 0, 1, 10, 6, 17)</td>\n      <td>(3, 1, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>(1, 8, 2, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(892, 870, 900, 913, 893, 914, 2720)</td>\n      <td>(887, 887, 905, 912, 907, 918, 2737)</td>\n      <td>(894, 891, 906, 918, 914, 922, 2754)</td>\n      <td>(912, 886, 909, 936, 909, 926, 2771)</td>\n      <td>(910, 884, 921, 938, 910, 940, 2788)</td>\n      <td>(911, 910, 920, 939, 932, 934, 2805)</td>\n      <td>(925, 912, 922, 951, 932, 939, 2822)</td>\n      <td>(929, 905, 929, 958, 931, 950, 2839)</td>\n      <td>(924, 914, 943, 952, 943, 961, 2856)</td>\n      <td>(930, 937, 935, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>XGBRFClassifier 6</td>\n      <td>(4, 3, 3, 5, 8, 4, 17)</td>\n      <td>(3, 3, 1, 9, 3, 5, 17)</td>\n      <td>(4, 3, 0, 5, 3, 9, 17)</td>\n      <td>(10, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 1, 0, 11, 5, 1, 17)</td>\n      <td>(0, 8, 0, 1, 10, 6, 17)</td>\n      <td>(3, 5, 1, 3, 11, 3, 17)</td>\n      <td>(1, 4, 0, 8, 4, 5, 17)</td>\n      <td>(1, 6, 3, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(890, 867, 901, 913, 893, 914, 2720)</td>\n      <td>(887, 883, 904, 912, 907, 918, 2737)</td>\n      <td>(898, 889, 906, 918, 914, 922, 2754)</td>\n      <td>(913, 886, 910, 936, 909, 926, 2771)</td>\n      <td>(913, 883, 917, 938, 910, 940, 2788)</td>\n      <td>(916, 907, 913, 939, 932, 934, 2805)</td>\n      <td>(921, 908, 923, 951, 932, 939, 2822)</td>\n      <td>(933, 903, 927, 958, 931, 950, 2839)</td>\n      <td>(924, 912, 944, 952, 943, 961, 2856)</td>\n      <td>(931, 935, 937, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>GradientBoostingClassifier 1S</td>\n      <td>(4, 2, 4, 5, 8, 4, 17)</td>\n      <td>(2, 3, 3, 9, 3, 5, 17)</td>\n      <td>(5, 3, 0, 5, 3, 9, 17)</td>\n      <td>(14, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(1, 3, 0, 1, 10, 6, 17)</td>\n      <td>(1, 10, 0, 3, 11, 3, 17)</td>\n      <td>(0, 3, 0, 8, 4, 5, 17)</td>\n      <td>(2, 5, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(805, 793, 755, 913, 893, 914, 2720)</td>\n      <td>(809, 821, 762, 912, 907, 918, 2737)</td>\n      <td>(799, 825, 768, 918, 914, 922, 2754)</td>\n      <td>(832, 825, 762, 936, 909, 926, 2771)</td>\n      <td>(827, 823, 785, 938, 910, 940, 2788)</td>\n      <td>(836, 850, 769, 939, 932, 934, 2805)</td>\n      <td>(833, 835, 777, 951, 932, 939, 2822)</td>\n      <td>(847, 841, 782, 958, 931, 950, 2839)</td>\n      <td>(825, 848, 794, 952, 943, 961, 2856)</td>\n      <td>(844, 864, 789, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>GradientBoostingClassifier 2S</td>\n      <td>(3, 1, 2, 5, 8, 4, 17)</td>\n      <td>(4, 0, 3, 9, 3, 5, 17)</td>\n      <td>(5, 3, 4, 5, 3, 9, 17)</td>\n      <td>(15, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 3, 3, 1, 10, 6, 17)</td>\n      <td>(2, 7, 0, 3, 11, 3, 17)</td>\n      <td>(4, 2, 0, 8, 4, 5, 17)</td>\n      <td>(2, 7, 0, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>GradientBoostingClassifier 3S</td>\n      <td>(3, 2, 1, 5, 8, 4, 17)</td>\n      <td>(2, 0, 3, 9, 3, 5, 17)</td>\n      <td>(5, 2, 4, 5, 3, 9, 17)</td>\n      <td>(17, 0, 0, 17, 0, 0, 17)</td>\n      <td>(11, 0, 0, 11, 5, 1, 17)</td>\n      <td>(0, 3, 0, 1, 10, 6, 17)</td>\n      <td>(1, 8, 1, 3, 11, 3, 17)</td>\n      <td>(2, 0, 0, 8, 4, 5, 17)</td>\n      <td>(2, 6, 1, 3, 9, 5, 17)</td>\n      <td>...</td>\n      <td>(913, 893, 914, 913, 893, 914, 2720)</td>\n      <td>(912, 907, 918, 912, 907, 918, 2737)</td>\n      <td>(918, 914, 922, 918, 914, 922, 2754)</td>\n      <td>(936, 909, 926, 936, 909, 926, 2771)</td>\n      <td>(938, 910, 940, 938, 910, 940, 2788)</td>\n      <td>(939, 932, 934, 939, 932, 934, 2805)</td>\n      <td>(951, 932, 939, 951, 932, 939, 2822)</td>\n      <td>(958, 931, 950, 958, 931, 950, 2839)</td>\n      <td>(952, 943, 961, 952, 943, 961, 2856)</td>\n      <td>(959, 960, 954, 959, 960, 954, 2873)</td>\n    </tr>\n  </tbody>\n</table>\n<p>25 rows Ã— 140 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_points_train = pd.DataFrame(points_train.items())\n",
    "df_temp = pd.DataFrame(df_points_train[1].tolist(), index=score_df_train.index)\n",
    "df_points_train = df_points.drop(df_points_train.columns[1], axis=1)\n",
    "df_points_train = pd.merge(df_points_train, df_temp, how='left', left_index=True, right_index=True)\n",
    "df_points_train.to_csv(filename_to_export_train)\n",
    "df_points_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-58-812f24ef7503>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test_points.csv\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"wb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moutfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m    \u001B[0mwriter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcsv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m    \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriterow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpoints\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m    \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriterows\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mpoints\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"test_points.csv\", \"wb\") as outfile:\n",
    "   writer = csv.writer(outfile)\n",
    "   writer.writerow(points.keys())\n",
    "   writer.writerows(zip(*points.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"test_points_train.csv\", \"wb\") as outfile:\n",
    "   writer = csv.writer(outfile)\n",
    "   writer.writerow(points_train.keys())\n",
    "   writer.writerows(zip(*points_train.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "magisterka_analiza",
   "language": "python",
   "display_name": "Python magisterka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}