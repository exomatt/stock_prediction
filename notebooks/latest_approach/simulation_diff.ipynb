{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from finta import TA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "from xgboost import XGBClassifier\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "WINDOW = 16  # number of rows to look ahead to see what the price did\n",
    "FETCH_INTERVAL = \"60m\"  # fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "INTERVAL = '2y'  # use \"period\" instead of start/end\n",
    "# valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "# (optional, default is '1mo')\n",
    "symbol = 'AAPL'  # Symbol of the desired stock\n",
    "\n",
    "# one day 16 rows of data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 Open        High         Low       Close  \\\nDatetime                                                                    \n2019-08-26 04:00:00-04:00   51.025000   51.470000   51.025000   51.400000   \n2019-08-26 05:00:00-04:00   51.357500   51.710000   51.357500   51.427500   \n2019-08-26 06:00:00-04:00   51.412500   51.712500   51.040000   51.500000   \n2019-08-26 07:00:00-04:00   51.495000   51.657500   51.290000   51.600000   \n2019-08-26 08:00:00-04:00   51.620000   51.700000   51.303050   51.437500   \n...                               ...         ...         ...         ...   \n2021-08-25 09:30:00-04:00  149.699997  150.320007  148.535004  148.625397   \n2021-08-25 10:30:00-04:00  148.620193  148.735001  148.119995  148.505005   \n2021-08-25 11:30:00-04:00  148.500000  148.548004  148.059998  148.239700   \n2021-08-25 12:30:00-04:00  148.237000  148.440002  148.169998  148.264999   \n2021-08-25 13:03:49-04:00  148.300003  148.300003  148.300003  148.300003   \n\n                            Adj Close    Volume  \nDatetime                                         \n2019-08-26 04:00:00-04:00   51.400000         0  \n2019-08-26 05:00:00-04:00   51.427500         0  \n2019-08-26 06:00:00-04:00   51.500000         0  \n2019-08-26 07:00:00-04:00   51.600000         0  \n2019-08-26 08:00:00-04:00   51.437500         0  \n...                               ...       ...  \n2021-08-25 09:30:00-04:00  148.625397  15348852  \n2021-08-25 10:30:00-04:00  148.505005   9910979  \n2021-08-25 11:30:00-04:00  148.239700   5739516  \n2021-08-25 12:30:00-04:00  148.264999   2533769  \n2021-08-25 13:03:49-04:00  148.300003         0  \n\n[8373 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-08-26 04:00:00-04:00</th>\n      <td>51.025000</td>\n      <td>51.470000</td>\n      <td>51.025000</td>\n      <td>51.400000</td>\n      <td>51.400000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 05:00:00-04:00</th>\n      <td>51.357500</td>\n      <td>51.710000</td>\n      <td>51.357500</td>\n      <td>51.427500</td>\n      <td>51.427500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 06:00:00-04:00</th>\n      <td>51.412500</td>\n      <td>51.712500</td>\n      <td>51.040000</td>\n      <td>51.500000</td>\n      <td>51.500000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 07:00:00-04:00</th>\n      <td>51.495000</td>\n      <td>51.657500</td>\n      <td>51.290000</td>\n      <td>51.600000</td>\n      <td>51.600000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 08:00:00-04:00</th>\n      <td>51.620000</td>\n      <td>51.700000</td>\n      <td>51.303050</td>\n      <td>51.437500</td>\n      <td>51.437500</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 09:30:00-04:00</th>\n      <td>149.699997</td>\n      <td>150.320007</td>\n      <td>148.535004</td>\n      <td>148.625397</td>\n      <td>148.625397</td>\n      <td>15348852</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 10:30:00-04:00</th>\n      <td>148.620193</td>\n      <td>148.735001</td>\n      <td>148.119995</td>\n      <td>148.505005</td>\n      <td>148.505005</td>\n      <td>9910979</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 11:30:00-04:00</th>\n      <td>148.500000</td>\n      <td>148.548004</td>\n      <td>148.059998</td>\n      <td>148.239700</td>\n      <td>148.239700</td>\n      <td>5739516</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 12:30:00-04:00</th>\n      <td>148.237000</td>\n      <td>148.440002</td>\n      <td>148.169998</td>\n      <td>148.264999</td>\n      <td>148.264999</td>\n      <td>2533769</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 13:03:49-04:00</th>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8373 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = yf.download(  # or pdr.get_data_yahoo(...\n",
    "    tickers=symbol,\n",
    "\n",
    "    period=INTERVAL,\n",
    "\n",
    "    interval=FETCH_INTERVAL,\n",
    "\n",
    "    # group by ticker (to access via data['SPY'])\n",
    "    # (optional, default is 'column')\n",
    "    group_by='ticker',\n",
    "\n",
    "    # adjust all OHLC automatically\n",
    "    # (optional, default is False)\n",
    "    # auto_adjust = True,\n",
    "\n",
    "    # download pre/post regular market hours data\n",
    "    # (optional, default is False)\n",
    "    prepost=True,\n",
    "\n",
    "    # use threads for mass downloading? (True/False/Integer)\n",
    "    # (optional, default is True)\n",
    "    threads=False,\n",
    "\n",
    "    # proxy URL scheme use use when downloading?\n",
    "    # (optional, default is None)\n",
    "    proxy=None\n",
    ")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n",
    "data.head(10)\n",
    "important_columns = ['open', 'high', 'low', 'close', 'volume']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_diffs(diff_number, col_name):\n",
    "    new_col_name = f'{col_name}_{diff_number}'\n",
    "    data[new_col_name] = data[col_name].diff(diff_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# for name in important_columns:\n",
    "#     for i in range(1, 11):\n",
    "#         calculate_diffs(i, name)\n",
    "#\n",
    "# data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Datetime'>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEECAYAAADTdnSRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3IElEQVR4nO3dd5hU5fXA8e/Zzu7SWTq4dAQRRYqIiohgIYlGE0WNXYklan4aI3ajaDSWxG7sNaLGGlFQkWIDQYo0KdL7wtKXrfP+/rh3Zqfuzuy0u7Pn8zw8e9vMnB3unnnnve89rxhjUEoplVrSkh2AUkqp2NPkrpRSKUiTu1JKpSBN7koplYI0uSulVArKSHYAAK1atTKFhYXJDkMppeqVH3/8cYcxpiDYPkck98LCQubOnZvsMJRSql4RkXWh9mm3jFJKpSBN7koplYI0uSulVArS5K6UUilIk7tSSqUgTe5KKZWCNLkrpVSCbN9Xylc/b0vIa2lyV0qpBLnslblc+spcDpZXxf21NLkrpVSC/Lx1LwCfLtqCy2VYuGF33F5Lk7tSStXBvPW7KK2IrAVeUWVNjnTjuwv57TPfcfpT3zLhk6XxCE+Tu1JKRWrV9v2c+fR3/GPycs+2DcUlFI6fxPqdJUEfU1nlonvrfM+6u9X+wjdr4hKjJnellIrQ2h0HAHjp2+rEfNYz3wFw/EPTAo4v2ldG99s+Y9X2/QAc3rGpZ1+/Dk0Djo8FTe5KKRWh8iqXz/pFL/3A9n1lIY9fsnmPz/pPG6vXVxftj21wNk3uSikVoX2lFT7rM1YUeZbT0wRjDNf8Zx5Tl1nDHkUk5HMN7dYqLjE6ouSvUkrVJ2/P2eBZ3uuX6C8aWkhZpYtJP21h0k9beHvc0ZSUVQY8x+lHtOexsUdijIlLjJrclVIqQvPW7/YsH3735z77yiqrmLt2l2f9nOdmBX2OX+zumJpa9dHQbhmllIrArgPlQbefP6QzAG/OXs8fXpwd9Jinzx/gWY73jUya3JVSKkzb9pZy1IQvgu7r36lZrY8fWNjcs3zTyb1iFVZQmtyVUiF998sOHp+6MtlhOMaQ+6fi8uoiv3J4N89yt4L8gOPHDurEx38a5lkvyM/2LJ/Qq3V8grRpn7tSKihjDOc9b3UvXDm8G1kZDbstOH/9roBtXVrlepaPOqS5z773rz6GAZ2tba9cMohmuVmICI1zMthXWklmenzfT03uSqmgPl201bNc6XKR1cC/6C/bsi9gmztBDyr0TeyDCpt7Ejv4ttI/vGYY363aQXpafC6kumlyV0oFNXvNTs9yWYWL3KwkBuMAB73qyPzh6M70bNPYk6Dn2KNjGudk0Cw3kxcuGhTyeboV5Aftwok1Te5KqaB27q8eFVJW6arhyIbhXq8CXxPO6AfAk1/5Xo9YdPfJCY2pJg37e5ZSKqSZK6vvupyaoAkm6ptyu8rj9SN7JDmSQJrclVJB7SutvqtyddGBJEbiXJ2aNwKgTZOcJEcSqNbkLiIvich2EVkcZN9fRMSISCuvbbeIyCoRWS4izvmOopSqs027DiY7hKQb3KUFAA+e1c+z7bdHduCp8wYwdlCnZIUVUjgt91eAU/w3ikgnYBSw3mtbH2As0Nd+zNMikh6TSJVSCbNlj28yn7xka4gjG47Fm/bQPDeTcwZ19mzLSE9jzOHtSIvzyJe6qDW5G2NmAsVBdv0T+CvgXfXmdGCiMabMGLMGWAUMjkWgSqnEWWDXTvnL6J7JDcQBFm7YTeH4SZSUV7GrpKL2BzhEnfrcReQ3wCZjzEK/XR2ADV7rG+1twZ5jnIjMFZG5RUVFwQ5RSiWJe9hf00aZSY4k+T5asNmzPMTumqkPIh4KKSK5wG3A6GC7g2wLWs/SGPMc8BzAwIED41PzUilVJ5X2KJB430VZH7hnTZo47miOCKN+jFPU5X+uG9AFWCgia4GOwDwRaYvVUve+stAR2BzwDEqppFhdtJ9Xv1tb63EVLmtce4Zfcv+laD+LN+0J9pCEMsZEPDl1XZVVWq/TuUUuOZn15xJixMndGLPIGNPaGFNojCnESugDjDFbgY+BsSKSLSJdgB7ADzGNWKkkmfjDep6atirZYUTlxEdmcNfHS3C5av6yXGHftJThd6Fw5CMz+NUT38QtvnC9Pmsdve+Y7KmJHk/u0rzZ9ay2TjhDId8Cvgd6ichGEbks1LHGmCXAO8BSYDJwjTEmMR+vSsXZ+PcX8dCU5bUfWA/Udsfp3f+z7sYMVf+kcPwkHpryc8zjCtfdHy8B4Oo35sX1dTbtPuh5L/Jz6tcN/eGMljnXGNPOGJNpjOlojHnRb3+hMWaH1/p9xphuxphexpjP4hG0Uio6+4NM+xZMXnZ1N8SH8zf57Htq2i+e54rXVHGhuLtH4j0E8cmvqr+pZWfUny4Z0DtUlWqQtu0tDeu4/Ozq0TJ/fntBwP4Xvl7NYXdN4Z25GwL2xdPVJ1h11I/t3jKur/PWD+trP8ihNLkr1QDdZXdr1CY3q+bW6oRJywDYvrcs6phqU1lV3ZX08OcrAKioSsw3hr+f2a/2gxymfnUiKaViwjtRBlPQOJv2TXM4pGVuwL5xx3fluZmrfbY1quVDoK4+WrCJ6ycu8NmW5/VaFbX8HnW1umg/lV4Xnc8d3LmGo51JW+5KNUALN+7h0c+XhxxtUrSvjMz0NBrnZHL5sV189p3ctw2fXX+cz7Z4lQT2T+wAB7wmlg732kEkivaVceIjMxj9z5kxf+5E0uSuVAPi3ep9/KtVXPhi6JHKc9dZE1Ckp/tetMzNyqB900aedRESNubc30cLNlM4fhJ7DsauLMCpj/km9d5tG8fsuRNJk7tSYaitG8PJVm3fz+D7vuTeT5YG9FH369A04Hj373rmkVblkMy06jSRlZHGoe2a0DQ3k5/uHs2Sv51MdkYaB8pim9xLK6q49q35Ifc3z/Uti7BmR+xKEg/2KzFQn0oOeNPkrlQY6vNMRFe8Npft+8p48Zs1lPt9SG3Zc5DC8ZPY7jV6Zts+6+LoIDupebeK598xyrPcJCeTvOwMMtPT+HFdsNqCdff7Z7/nfwtD39z+3fiRPusl5bHrnvGvXX/ioW1i9tyJpBdUlQrDtOXbkx1CnXm3anu0zufCYwr5Zft+Xp+1joUbrVIC05Zv95Sy3V1iTa/X3J409eetewF49dLB5GUHpoxuBflUumL74ed+TbeczDRKK6pfw/8C7todJRzTLbrXHPXoDFZur74GsfaBMdE9YZJpclcqDN6tV2MMIs6r3x2ONy4f4pk16BWvGjM3v7eIDcUH+fNJPTxJNCfT+mI/pl875qzdxREdmwV9znZNc1i1PbZlAPy7j+bcdhKZ6Wmc+/wsmgWpVPnEVys5WFHFZX4XfyOxMsa/Q7Jpt4xSYWicU51Q6lsXzaHtmniWa5oO7slpq+h+22eeQlnuOzIvHtaFX+4/jaa5wcv/zl5TzMrt+2Pa7+2vcU4mOZnpfHD1MF6+xJoi4sNrhvG7ozoCsGVPqc8E1pHaV1p/6rSHS5O7UmEY/95PnuW99SwRdGzeqPaDvLg/vNwtdwhdYwag+IDVjTPi4emRBxeGUK3xIzo1465f9/HZ9v68jXV6jX53f16nxzmZJnelwlDiNbZ678HYj62OJ5fL0Ld9k7D7kMsqfFvuyTRx3NHc8as+Iff7f+jc8I7//EGRy8pIY/pfToj6eZJN+9yVilBVLeVyneLHdcWc9cz3QPWEE+H4cpl18Tg7M7y2X9smOWy1R9tUuUyNrfxIdW2VV+P+WL6W24oJp8b8OZNBW+5KhfDjul1c8+a8gNrn9SW5uxM7QFoEF4D/+6PVtRHuxBQXDyv0LC/dbI1y2bqnlGk/122EkXeFydY1XCMAyPAag9/MviZQl5IELfKskUHXjIhyyI2DaHJXKoQ/vj6XSYu2sOOAb1EsV4LL29bFLrsf3K0uLdzGYdYvz/caHvn0dKtE7tAHpnLJK3PqNGvTjv1W7OHcGer9a+22J69et7PEU1bhowWb+GFNMePf+6nGpF9aUcXlx3bhppN7RxyvU2m3jFIhuFvoO/eXB91eF5VVLlzG6teNp4Ubd/usB8vtlx3bhRe/WRPyOZrkhDc59thBnSitqGLCpGV8tngrLpfB/fn3/rxNHBbkLtiauOvFXHFc11qP9R6SOqRLC2avKeaSV35gQ/FBerbJZ8W26uGNE+dsYPX9p3lqwC/csJvTn/rWs78+TaEXDm25KxXCLrsleOpjX/tsj6blfsbT39Lz9vjPYeOfqOas3RVwzLUndvcsz7jphIj65b1lpKdxuVcinrVmp9c+30+VPQcr+GB+zSNaHvjMKiM8b31gzKE0z83krAHWsMgNxQcBfBK72zKvm6O8EzvAqf3ahv169YEmd6UiFE1yX7xpb+0HxUBlGHXOM+3Jr88a0JFDWubx+mVDOKRlLrNvHcnCO0fX+bXPe342Azo3AyDLb4Ltm95dyP+9vZAV2/aFfPyUJdsAa6hjON7541Am//n4sC4Ar99Z4vPTE9fJvejbvm4fbk6lyV2pCNWHGmIVdjmAf5x1eMhj8rIz+PqvIzwTUTRtlMmMm0bQpklOyBuWwuXudnrSb0Jx941OB8trLzS2bEvoDwBvg7u0oE2TnIAPkmCuenMec9YWc/xD03y2nz2wU1ivVZ9oclcqiKJ9oWcWcvJomfs/XcbL367xtNy9704NplOL3Jj1/99yavXFyFDJ232L/8OfB59o3Htk0m1jDo3o9Wv6PZp5fVj9/tnvffatmHAqBY2zI3qt+kCTu1JB/N3u9/X29rijARI+GXS4NhSX8NzM1fztf0s9ZXv9+7zjyTtB1jZx9dcrdwTd7q5aeXLfNhGP8AmV3C8+ppDv/apIhvO4+i41fyulouRfJ+WVSwZ5ElaVw5K7MYbte0u5bmJ1/fMKuwWcmcDk7n1H6/z1uwE4sXfrkMcbYzx1bMBq7e8rtUbKDOkS+cTX3t0yL1080LM8dnAnGmWl06aJb+v8jl/1YfX9p0X8OvWFDoVUKoi+7ZuwuuiApxrkCb1a86M9M5HTumVu+u9PnhuP3FbaFywz0tL4v5N60rpJ/LsdDIHvS01jy7vc8ikAP997CjmZ6Rx652TPvsw6tKa9W+BNvSpH9mpjjZeffetJlJRX0ufOKQDsKSmv9RtGfaYtd6WCqHJZo0lG9WnDK5cMAqpvBHLSTUz7SisCEjvAE19ZFzIz0oXrT+qRkAme/W+cguoiZOt3lvDC16vpH2S45X2TlrFsi+8oIu/JQ8LVtSDfs5zp1Yr3Hgufm5XBjaN6AnCmPXQyVWnLXakgqlwuMtKE5y+s/nqfbieJCZOWcWLv6GbniVVN+Jou/ELgUMR4cnfFuLXIy+KHNdYMTe7RKe2bBpYTWL5tHzv2V/8elw7rwrjja7+ByZ93a72iyvDOH4cGvXnr2pE9uHZkj4ifv77RlrtSQVS5Am/Zd+di/2nYwn/O6hb/mMe/CahZ46+ssorXZ62L6gJu4zDvMo2FO/3K77pLAc9cUeTZtnlPYIt8RK/W/Gf2ep/nqWvcRx3SHLDuyB3cpQUDC+vn/KexUGtyF5GXRGS7iCz22vaQiPwsIj+JyAci0sxr3y0iskpElovIyXGKW6m4chlDmt9fR7QVCL37n5du2Vvr1H1D7p/KHR8uDjmyBPCZes7fyX3bBExHF0/NcrP44OpjArZf+NIPNT7uwck/89nirQC8efmQqGJ464qjee6Coziyc/OonicVhNNyfwU4xW/bF8BhxpjDgRXALQAi0gcYC/S1H/O0iKRWwQbVIFS5jKcbxi3a5F7p11K/7NW5fDh/U8jjD9g1VlrkZbGhuITC8ZPYUOx7Z6W7QNaDZ/XjyxuG++z79wUDSbSaPmzC0TI/K6rHZ2WkMbpvapURqKtak7sxZiZQ7Lftc2OMe8aCWYD7ysTpwERjTJkxZg2wChgcw3iVSogqE1iX3Lts7pLNezjp0RkRzcpUGWTkyKeLtoQ83j2P6O+e/Y7j/mH1Wbt/um3ebdVRGda9Fd1b59OjtXVR8cWLEp/YoXqse4u8LB49u3/QY0b1CX29IjdTLwPGSiz63C8F3JWQOgAbvPZttLcFEJFxIjJXROYWFRUFO0SppKmscgVJ7tXLf3n3J1Zt38+cNcWEa0uQ/uY+7YPfQeo9WqSm1rD7y0DLPCup3vfbfvTr0JRje7QKO65Y6t46nx9vP4l5d4wiN0SXkDvpjw6S5JvlJe4aQaqLKrmLyG1AJfCme1OQw4JeDTLGPGeMGWiMGVhQUBBNGErF3JQl2wKqCnq33N1D99JEKCkPb9q9r4JMXtE8N3g3xOD7p4Z8nsmLq1v77puA3GO8B3dpwf+uPTapU+S1zLc+aLq3zg/Y9+BZ/Wick8nyCafw7wuOCtgfbplhVbs6J3cRuQj4FXC+qb6cvxHwrsDTEdhc9/CUco5gd6Z+vXIHfe6cwpy1tbfgpy7bRj+/2uZ3fbwkYDRMaUXNRbWufGOeZ7m80kVmusRlurlodW8dONnGOYOs8fbZGemIiGcGpB9vP4nJfz4uofGlujoldxE5BbgZ+I0xxvsKz8fAWBHJFpEuQA+g5kvlSjmMu2987CDfSoHB7kz97hdrJMvcIPXSd+wvY7V9wXNvaQXzN+wOejv+a9+v81l3zyjkrUOzRpzQq/ob7iN24a2ySldCx7LH2td/HcHCO0fTMj+b3m1rLnKmIhPOUMi3gO+BXiKyUUQuA54EGgNfiMgCEXkWwBizBHgHWApMBq4xxtRe21MpBym176rsWuA7OXOwGunuG5GC3Xo/+p8zOfGRGYB196YxcEjLXAA6Nm/kOe6uj5f4PK68MrCP/ZubR/hciHziq1VM/GE9ny3aQraDZxDqEaRrxltedkbU5YVVcLVemjbGnBtk84s1HH8fcF80QSmVTIfdZdUe8Z9UulvrvIBj3b0hJWWBbRj3TTyvfrfWU6MmLzuDZfdYI4u9a6k8PGU5T05bxe1jDmVY98CLoSLC+UMO4cP5mzyzKo1/fxGAo8vVfnHDcArHT0p2GA1S/f0+p1QcuMeWQ2BN8mAXKZdsti6s+k9K4e2uj5fw6BcrAGicnUGjrHRy/GYNcj9+wqRlngu0zYO0aN+98hj+ekovn21OLUHs1qVV4Ieiij9N7krZHv1iBX3tVjsQ1rRtobj74v3lZVtflkUkZB3xEvtD5bkLg49Vz/S6dfap8wbw5HkD6hxnInxy7bHJDqFB0uSulO3xqSt91kceWvfiYOc9Pzvodu+hjysmnBr0mAN2F0+oceLeo2nGHN6Oo7tGXvs8kdwfaCqxNLmrBm368u38tHF30JEw3QpqvhhYF51aNPJZD9Zl4e6WycuykuKw7r7J+98zV8c8LpV69CNVNWgXvzwHsCaM8OafhGtznNcdocVB6poDtGuaE1Dm950/DmXQfV/6bDtgd8vkZqfzw60jadLIt+99f1l4N005ybtXDiU7Raezcyp9t5UC7v1kqWf567+O4NPrgt9Qc+/pfbnn9L4B2zO8biJyz9jkL1hNlYLG2Yzo5XuH9kG75Z6blUHrJjnk+A11/I9dOXFwPSpnO6iwBYd3bJbsMBoUTe5KAW961RPv1CI3ZD3xC4YWcnKQqoPTlhexfmcJ416by2vfrw362MM6BM5CBARcEHX3uTcKMX79mO6tePTs/jzzB2dfSFXJpcldKS/h1BMPNYHSHR8t5vOl2zz11/9x1uEADOnSgo7NG3HmkUFr6JGXncHfflP9baCkvJJGmek1lhQ4c0BHTw0XpYLRPnelvHRukVvnx85Y4Vvd9OxBnTj9yPZkpqXVOhGzd532kvIq8rKde9epqh80uSvlJdTYc2/Z6bUn3qtP6GYdG2Z1xt7tquuqeHcRKVVX2i2jGqxnpv8SsM0Vxt2eTXMzeeuKo2s8Jj8nsnbTWQOCd9koVVea3FWDVFnl4sHJPwdsb9skJ6zHD+1mjT0f0LlZ0P2Z/hOw1sJ/iKRS0dJuGdUgTVte3T+eniaem5giSbLLJ5xCugjdb/ssYN+BMCfwUCpeNLmrBsn7hppJ1x3Llt2lARNY1/4cofvT1+8sCblPqUTQbhnVILXMr67x0rttE0b0bl3jxM01+eiaYQHb1hdrclfJpcldNUhl9oQYT8WgomL/Ts0CtkX6LQDglUsGeZavskfbKFVX2i2jGpzyShdnPv0d4Fu/PZYmnHFYxI85oVdr1j4whhkrijimm7MrPSrn0+SuGpwvlm7zLA84pFlcXsN/ir5IDO9ZUPtBStVCu2VUg+OeBemja4bRvXXjuLxGZj2etFqlBj0DVYPjHvZYU+2WSL14ke+sSRkxfG6l6kKTu2pw3Heh+k+AHQ3vrpROLRrpTUkq6TS5qwanyhooE9OWe4ZXN8zXfz0xZs+rVF3pBVXV4FQZd7dMbJ+3Y/NGFDTWMrzKGTS5qwbn5W/XANUt+Fj55mZtsSvn0G4Z1eDMX78bAEPkNxopVV9oclcNVmWVJneVumpN7iLykohsF5HFXttaiMgXIrLS/tnca98tIrJKRJaLyMnxClypaOVla6+kSl3htNxfAU7x2zYemGqM6QFMtdcRkT7AWKCv/ZinRUTnC1OO0see9aiwZd2n1FPK6WpN7saYmUCx3+bTgVft5VeBM7y2TzTGlBlj1gCrgMGxCVWp8FVUuXCFKN7VrmkOh3VoomPRVUqra597G2PMFgD7Z2t7ewdgg9dxG+1tAURknIjMFZG5RUVFwQ5Rqs563PYZl782F4C1Ow6wbMtezz6XMQia2FVqi/UF1WB/MUGbT8aY54wxA40xAwsKtFCSir2vft7OnoMVnPDwdE597GvPdgNoo12lurom920i0g7A/rnd3r4R6OR1XEdgc93DUyo6t3+4OGCbMTpnqUp9dU3uHwMX2csXAR95bR8rItki0gXoAfwQXYhK1d2cNdWXixZs2A24u2WUSm3hDIV8C/ge6CUiG0XkMuABYJSIrARG2esYY5YA7wBLgcnANcaYqngFr1Rttu4t9Syf8dS3nmUt2qhSXa0DfY0x54bYNTLE8fcB90UTlFLRMKbmm5Ncxmi3jEp5eoeqSikfLdhEl1s+rfEYY4Jf+VcqlWhyVynl+okLQu47rkcrALbtLY1puV+lnEjvv1Yp7Ylzj2TNjgO8N28j5ZUubnh7Ab8UHeCXogPJDk2puNLkrlLGTxt3B2z7df/2AMxZW8y+0kren78pwVEplRya3FXK+M2T1miYEb0K+Mfv+rNy+z7PvtysdNbtLElWaEolnCZ3lXKmLS+ioHG2z6xIeVkZbN1TPSzy4mMKkxCZUomjF1RVvVNaUcW9nyxlb2lF0P1PnHtkwDYDlHtNvXT3b/rGKzylHEGTu6p33pu3kRe/WcO/vlgJWOPa1+yovkDq7mf3NnnxVs/ywrtGxz9IpZJMu2VUvVNll/LdsucgLpfhvBdmMWu1VWbg0bP7B31Mq8ZZbCg+yLN/GEDTRpkJi1WpZNHkruqdzHTrC+dni7dy4iPTWWtfKD1/SGfOHNAx6GPKKqwumS6t8hMTpFJJpsld1TvpXqUD1nqNgPnzST1DPuaLG4bzxqx19GrbOK6xKeUU2ueu6p3New4GbPvTiO4+o2P8NW2UyTUjusczLKUcRZO7qnd27i8P2HaRDm1Uyod2yyhH21NSwd7SCjq1yOU/s9fz2NQVuIw1k5K7+OMx3VrW2GpXqiHSlrtytF8/+Q3H/WMaALd+sIhte8so2ldGrzbVfeff/bIzWeEp5Via3JWjrS+2Lpj+UrTfZ3tuVjpHd20BwKg+bRIel1JOp90yql54e84Gn/W2TXN4+vyjkhSNUs6nLXdVLzw3c7XP+q2nHZqkSJSqHzS5q3pnaNeWdGyem+wwlHI07ZZR9cryCaeQmaZtEqVqo8ldOZbL5TvRdXqakJWeppNbKxUGbQIpx/Iu0QvQKDNdE7tSYdLkrhyrtKLKZ31/WWWSIlGq/tHkrhzr2RnWCJmMNG2tKxUpTe7Kkca9NpdnZ/wCwKn92iU5GqXqH72gmgKKD5TTOCfDU+fce3vz3Mx62U89a3V1SYErh3fllL5t2XMw+LR6SqlAmtzruYoqFwPu/YJzB3fm72f2Y9X2/WzYVUKfdk0Ycv9UAFbffxppDu/a+GFNMT3b5NMsN4tvV+1gb2kljTLTGXd8V/q2b0rf9k2THaJS9UpU3TIi8n8iskREFovIWyKSIyItROQLEVlp/2weq2BVoI8XbAbgrR/W43IZTnp0Bpe8PIf/zF7vOebeSUspHD+JjxZsSlaYNVq2ZS9n//t7/vSf+ZRWVHH+C7MBOFhRxf+NCj0Bh1IqtDondxHpAFwHDDTGHAakA2OB8cBUY0wPYKq9rmJsQ3EJheMnceO7Cz3bnpq2yrP82NSVnuWXv10LwPUTFyQqvIg8bsf6zaod9L5jcpKjUSo1RNstkwE0EpEKIBfYDNwCnGDvfxWYDtwc5es0eFUuQ0WViyWb95CVns6vn/wm4JiPFm5OQmSRWbPjAFUuQ/fW+Xy7aoenlR7M8xcOTGBkSqWWOid3Y8wmEXkYWA8cBD43xnwuIm2MMVvsY7aISOtgjxeRccA4gM6dO9c1jAZjxMPTPeVvQ1m1fX/AtutH9vBpxRtjknqBdcTD0wFY+8CYGhP7m5cPYVj3VgmKSqnUE023THPgdKAL0B7IE5E/hPt4Y8xzxpiBxpiBBQUFdQ2jwagtsQN0aNYIgCfOPRKAIzs347gevgnS/67PRNp1oHp6vHfnbqjhSDSxKxWlaLplTgLWGGOKAETkfeAYYJuItLNb7e2A7TGIs0H74+tzfda/v+VE2jVtxHe/7OC856tbv5t2WxNHuxPjWQM60r9TM5/HHiyvIjsjPb4Bh7CrpDq5L9q0x2ffDaN68ugXK5h7+0kBd6YqpSIXzWiZ9cDRIpIr1vf8kcAy4GPgIvuYi4CPoguxYfulaD9Tlmzz2da2SQ5glb4NpkVeFmsfGMMfjj6EzPQ0/n3BUeRkWv/Vs1YXxzfgEDYUl3DiIzM86699vw6Arq3yuH3MoVw3sgdrHxhDq/xsLeerVAxE0+c+W0T+C8wDKoH5wHNAPvCOiFyG9QHw+1gE2lAt2bzXs3zjqJ6M6N3a02cebt/5yX3bMum64xj5yAyKvbpGEmne+l1Bt798ySAOaZmX4GiUSn1RjZYxxtwF3OW3uQyrFa9iYNMuq6tl0d2jaZyTGbD/upE96NehKSXllTUOdezSMo+8rHSWb90b8ph4ChbbjaN6amJXKk60tozDPTj5Z4CgiR2svupRfdpwymFtAejROj/ocWlpQo82jVm+bZ9n26mPfc3Y576PccQ1e+6C6nlPxw3vmtDXVqoh0eTuQOWVLm55/ydWbd9X+8G27Ix0Xrt0MBPHHR3ymB37y5i1uphLXv4BYwzLtuxl1uriiF6nLt6Ytc6zPLpvW89yVrqefkrFi9aWcaAPF2zirR828NYP1nDBvKzwRrcc37PmIaUb7S6eacuL2LG/uu993vrddG/duI7R1u72DxcDcEhL60LpNSO6sWLb/npZ0Eyp+kKTuwMZ4zu9XKMwk3tt+rRrwtItVp/7Q1N+9mz/639/4uyBnWLyGt4Ollfx+dKtnvVPrj0WgJtO7h3z11JK+dLvxQ702eKtPuuxGhr4/tXHeJZ7toldS/3179dSOH4SB/xmShrx8HSfC6mhrhsopWJPW+4O8tzMX1hfXML05UUM6dKC2WusMelPnndkTJ4/J7P6G4D3DUXReskuTLZ1byndCqwLui6XYeve0pi9hlIqMprcHWJfaQX3f1rdVdK1II+J447GGOJSi3368iLPcou8rKieq8y+o7S8srq0gf+Hx/lDtH6QUomkyd0htuzxbeVeP7InIkK8rjl6v140Nzat2XGAzfZzeSf3NTsO+ByXl62nmlKJpH3uDrC/rJLR/5zps61t05y4vmZetu9F2otf/oHXvYYshutxr4qTZXZy/3HdLn73bPX4+cGFLThnUOwv2CqlQtPmVJIt2riHhz5f7ln/5NpjadoofhceG2dnsK+skm17y3y2T19exPTlRVxw9CERPV/rJtme5aJ9ZVRWuTjrme98jnn5kkHaclcqwfQvLkm27DnI0L9/5bNt9q0jadMkvi32qTcOZ/D9U326UKLRPLe6v/6a/8xjuN9Y+2O6tdTErlQSaLdMkjw/c43P+hGdmsU9sQO0bpLjc1PUib1951KZtXpnRM/nf0lgxooin/XHxsZmpI9SKjKa3JNgT0kFP66rLr17z+l9effKoQl7/QPl1fXSzx3sO4pl7HOzInqubXvLyEiTgBb7bacdymXHdqFVfnQjcZRSdaPfl5Og/z2f+6xfOLQwKXEc270Vx/eMbsajbftK6dwil1cvHUzh+Eme7UO7teSwDk2jDVEpVUfack+Q3SXlzFu/i72lFckOxeOFiwbWeVamskqr9b+npIJmuYEXgOM92kcpVTNtucfQtr2luIyhXdNGnm2bdh/k25U7WLZ1Ly/bd3I6RVqIQfQVVS4ya6jY+Pac9dz83iLAKgbWpZVvTfYXLxpIq/zsYA9VSiWIJvcYWbltH6PsseprHxjj00UBkOtX/GtMv3ZMWrQlYfEFkx7iztfiA+U1Xtx9d+5Gz/K6nSUc3rGZz/5Y1q1RStWNdstEyRjDFa/N9SR2sCbB8FdS7jvp8/1n9ot7bLUJVdVgx/6y4DtsB/x+l+nLrTnQ/33BUXQtyKNj80bBHqaUSiBN7lHqduunfLHUdwLrZVuqp7J776rqUTDH9bAuXl51Qre43qgULnc99Z5tfGdv8q71Hox39ce2TXK4cng3wJqr9asbT9A67Uo5gCb3Opq1eie97/gMl1fp9bf9ZkF68rwjGdC5uWd9aLeWrPn7adx8ilXP/MGz+vkk/2T55Nrj+N1RHT3rO71a7sYYjDGs2XGAw++ewobiEp9vIbNuHck1I7onNF6lVO20zz1CVS5Dt1s/Ddg+86YRdG6ZyyfXHsuaHQf4df/2AcdcNbybT6v2nEHOqJSYlZFGh2bVXSk3vLOQNk1y6FqQ57mLtk+7JuwtreT9eZtq7bZRSiWfJvcIuIIk9pMObcPtYw6lsz2F3GEdmoYc3+2U7oqhXVvyvd+dqNmZvl/izn9hNjle29wzOE1atNlTa37SdcfGP1ilVJ1oco/AFr/JJz64+hiO9Op2CWXWLSPJynBOD9jrlw2mosp3Kr+cIOPdSysC68+s2LafYd1bMqBzM/q215uUlHIqTe4RcE9K8a9zjuCMIzuE/Tin3dCTkZ6Gfy7v1CK8qfya52by7aqdYU/arZRKDuc0Jx3usS9XcuIjM4DoJrdwqiM6NQvruMuP6woEDodUSjmLttzDcO1b8/nfws2e9SFdWyQxmvgIdya/h6ZYtefPGaiTbyjlZFG13EWkmYj8V0R+FpFlIjJURFqIyBcistL+WXuntINN+GSpT2IHUrKvOVQpglCuOqFbnCJRSsVCtN0yjwGTjTG9gf7AMmA8MNUY0wOYaq/XSyXllbzwjVV3vaBxatdKqS23/+Osw33WD2kZXh+9Uio56pzcRaQJcDzwIoAxptwYsxs4HXjVPuxV4IzoQkyePndO8SzPuOkEAG4fc2iSookvCZh2A7p6FQTr79cn75RhnUqp4KLpc+8KFAEvi0h/4EfgeqCNMWYLgDFmi4i0DvZgERkHjAPo3NkZN/Os2LaP175fy/Kt+7j6hOq7Lh88qx+5WRmsfWBMEqOLMztXpwmeu25L7dFB153YnV5tG5OVkUZ5pYsbRvVMUpBKqXCJMab2o4I9UGQgMAsYZoyZLSKPAXuBa40xzbyO22WMqbHffeDAgWbu3Ll1iiOWfv3ENyzatCdge0ondS8fL9zMsG4tOWrCl0D1ZNpvXDaEY3u0orSiipkrihjdt22SI1VKAYjIj8aYgcH2RdPnvhHYaIyZba//FxgAbBORdvYLtwO2R/EaCWOMYcW2fQHbG0piB/hN//a0zM9m9q0jad80h312gbBG9pj2nMx0TexK1RN1Tu7GmK3ABhHpZW8aCSwFPgYusrddBHwUVYQJsmbHAcoqXfxldE9OOrQ1Q7q0YMWEU5MdVlK0aZJDO69aM3nZesOSUvVNtOPcrwXeFJEsYDVwCdYHxjsichmwHvh9lK8RN+t2HqBDs0ZMWrSF6ycuAGBQYQv+dGKP5AbmAD+u2+VZzsvS2yGUqm+i+qs1xiwAgvX3jIzmeWPB5TLc8dFi/nxSz6DDGLftLWX4Q9MDtvfVSZ2B6v52qO6WUUrVHylbfmDinA28OXs9g+77kiqXYcnmPbi8iq/PWF4U9HH52dpKBXjl0sGeZW25K1X/pOxfrfcw7DOe+pZFm/YgAm9cNoS/vLuQE3tXj9C8+JhCXvlubeKDdLABnZt5lnMyU7YNoFTKStnkXl5ZXa7WPbzRGKtOOcCbs9d79hcfKG9Qo2LC4X2Tkt6wpFT9k5JNMmMMd328JOzj+7RvEsdolFIq8VIuuR8oq+TPby/w2dYqP4t3/jiUI726Gj68Zhiv2f3Kp+jYbaVUikm5bpm+d03xWV/7wBiqXIb0NOGDq4dRWeXCAJnpaZ79KrhXLx3MnDXFyQ5DKVUHKdFy/3D+Jn7z5DdUuXxLKdxo10BJ9ypWnpGe5knsqmbDexbwl5N71X6gUspx6n3Lfc7aYk83jHvy6kuHdeHOX/dJYlRKKZVc9boJu2N/Gb9/9vuA7X842hlVJpVSKlnqdXJfXXTAs+w9B2jXgvwkRKOUUs5Rr7tlBndpwU0n9+KhKct59Oz+NM7JxFXHEsZKKZVK6nVyB7hmRHeuGdG99gOVUqoBqdfdMkoppYLT5K6UUilIk7tSSqUgTe5KKZWCNLkrpVQK0uSulFIpSJO7UkqlIDEOuOlHRIqAdUl46VbAjiS8bjicGpvGFRmNKzIaV2R6GWMaB9vhiJuYjDEFyXhdEZlrjAk2wXfSOTU2jSsyGldkNK7IiMjcUPu0W0YppVKQJnellEpBDT25P5fsAGrg1Ng0rshoXJHRuCITMi5HXFBVSikVWw295a6UUilJk7tSSqUgTe5KKZWCUj65i0hTr2VJZizenBSLNxE5NNkxBCMiN4rIaHvZMe+dU88vcF48oOdXXdT1HEvZ5C4iJ4rIAuAZEbkVwDjg6rGInC4irwL9kx2LPxF5AvhURAqTHYubiIwWkSnAzcCF4Jj/R0eeX+Dcc0zPr8hEe4454g7VWBORfOBW4F7gB+BVEck1xtyepHjEGGNEZIQdUwUwVETWGWN2JSMm77i8NrUAdgEnicjrxpiyZMUFZAJ3AsOBvwNZwCARyQQqk/kH6LTzy47JceeYnl91F4tzLOVa7iKSBuQDG4D5xpgNwOXAOSLSOwnxeJ/ga4CTgZuAIcDhiY4nWFwikm5vngU8A5wP9EhmXMaYcuAjY8xxxphPsZLCWGNMRZITu6POLzsmx51jen5FFWNMzrGUSO4icrWInAVgjHEBBijAeoMwxqwGPgDusY9PSJ+aiPwJeF9E/k9E2hpj1hpjthhjvgK2AcNFpEMiYgkR159FpL0xpkpEsoBTsN6nacBYETlTRBJW98fv/WpnjJljb880xswAVovIqYmKxysuR55f9ms57hzT86tOscX+HDPG1Nt/QGPgWWArsB/I8Nr3D+Alr/U0rMqTfRMU22+BOcAI4GXgSeAIr/2HA28AZ/o9TpIQ11H2vr/ZP88F9gLLgNZJfL/6u98TrK/0LwCj9fxy7jmm55dzzrF63XI3xuwDZhhj2gKfAE957b4HOEJEThORbGN9Gn6C1deWCEOAZ4wx04C7sb4uX+cV+09YJ9th9oWTm+3t8f5KGCyuq+x9Y0Tka6yLSx9ifY3eG+d4aorrerDeE2NMMdAI64/T/dU1rhx+foEzzzE9vyIQ13MskZ9SUX7CSbB1IM/+2RbrROnhdcxY4DXgb/Yb9RPQJkFxjQO+9NreG3gb+I3XttbAeqAIeCjY8yUorv8CQ4ELgAe99n0GHOmg9+tUYCqQ01DOr5piTfY55sTzK8r3Km7nVy2xxeUcq08td5+RPcb+zY0xB0QkzRizFXga66uV+5iJwP1YX7sKgFONMdtiHJfPp6g7LqwTukRETrfXtwDTgT5iyQceAxYBhxtjbvJ7fCLj+go4HnjTGHOz18N+a4yZH6N46hLXdOz3y97WCJgIVMU4ppBxOeD8AnwuSjrmHIsgpkSeX3V6r+xt8Ty/QsYWt3MsEZ9QUX66HQ28af+CPYB0e3s6kGYvp3kdvx6rldAWGOL9CRnjuIYC72L98fTxiivD/ZrAJcBkqj+hbwLudh9HHPobo4jrLv/31UlxuWNLYFxJPb+8YrsnyHb3+5PwcyyKmBJxftU5rnidX7XFFs9zzNEtdxE5DHgCq59pO9ZXK/eNBlXGGJfdOmnq9bAHgW+BmUCOfWxM+xhFpDXWBZlPgZ1YfXeX2q9VaR/WCJiC1UJ4TkTaA0dijT/GGFNpjNnuoLgq7eOqjNW356i43LElMK6knV92bBcBrwK3i8jZ9rYMv9dL6DkWZUzxPL+ijssdWyzjCie2uJ5j8fikiuEn3uXA6/ZyHla/05dAV3vbvVifxMfZ66cCPwMPA5lxjGsU8JZXXCdjfQD1trdNwDqRjsS6Cj8B6yvg08SpdaBxxSWupJxf9mudBHQERgPrvba7v1ncnej3zIkxOTmuMGO7K17nWNx+qTq+EcOxv4bY6/3t/4TuXm/Ef7GSfB7wH6Cb1/F9gE5xiOsMrLvFxtjrBcBK92vbJ8xdWJ+4uf5x2cfkalz1O654nV9+sf3KXk93/3ED3wD3eh3bOhHvmRNjcnJcsYgtludYzH+5Or4hjYH3gWLgJaCFvT0fa6znTKyhU59gXT1+GN8+qni1VArs150JXInVNfQ7e98DwL/s5TTgWOB5d+zu7RpXSsQVz1ZnsNh+a+/Lsn/2BfYQZJREPN4zJ8bk5LhiFFvsryfF65eN8I3JBv4EnIb1lemPfvsPB063lwcCnyXoP+xo4Cav9QuA7+zl/lhfp06y1w8FPqZ6WJPGpXFFFZu97v76/gLwsr18akOLyclxOTW2uP/SNbwZF2J1wzSz13OwivdciDUvYM8Qj/sr1lfneI1QuBA4AetreSb2pyzW16ujgOft9TTgYmAx0B3rw+l/7t9H49K4oojt3/a6ZzSFve7CqoEynhh/6DgxJifH5fTYjDGJrQppjydti9XP5AJ+AcaJyPXGmB32Md8DPYGzsVrx7sceBTyCNQZ1nLHfpTjGdQVwvTFmm4ikG6s+xqHYV7WNdcX/FXvExXismyKuMMbs1rg0rihja27HZgAjIocA/wS+Bq4xxixO1ZicHJfTYwsQr0+NIJ9y7q8lPYE37OUMrKGO7/kd+1usK9ndgUb2tpbA8ATH9b7fMa8BZ9vLbb2eI0vj0rjiEFuB/bMZMDjVY3JyXE6PLdi/uLfc7TGd9wDpIvIp0AT7DjBjTKWIXAdsFpHhxqrMhjHmA/uTbzKQLyInGmOWAjOSGRdWYZ81InIPcKaInGKM2Wis8qEal8YVj9hOM8asx6rpnZIxOTkup8dWo3h+cmD1qS/EquF8BdaV5FOw7sAa7HXcVcA0r/XfAwewRi3E4y7OiOPC6kcrxarK9i/sT2ONS+OqL7E5MSYnx+X02GqNPa5PDscBF3itP22/CRcDP9rb0rD6sN4Bung97jgHxXUI0M3+jxqgcWlc9TE2J8bk5LicHlutscf5jcnFGubo7oc6H/i7vbwAuNZeHoh9p2BCfunI4pqocWlcqRCbE2NyclxOj622f3GtLWOMKTHGlJnqmg2jsEqPglXE51AR+QR4C5gXz1iiiOtHSMzsOhpXasTl1NicGJOT43J6bLVJyFBIscpcGqAN1g0iAPuwbtM9DFhjjNmUiFjqGpexP541Lo2rvsfmxJicHJfTYwslUVUhXViD/HcAh9ufdHcALmPMN8lI7BqXxtWAY3NiTE6Oy+mxBZeo/h+s23NdWMVzLkvU62pcGley/zkxNifG5OS4nB5bsH/uovVxJyIdseotPGqMKUvIi4ZB44qMxhU5J8bmxJjAuXGBs2MLJmHJXSmlVOI4eiYmpZRSdaPJXSmlUpAmd6WUSkGa3JVSKgVpcldKqRSkyV2lDBGpEpEFIrJERBaKyA0iUuM5LiKFInJeGM/tc5yIDBSRx2MRt1LxoMldpZKDxpgjjDF9sWqAnIY1JWNNCoFak7v/ccaYucaY6+oYp1Jxp+PcVcoQkf3GmHyv9a7AHKAVVinW14E8e/efjDHficgsrEmx1wCvAo8DD2DNjZkNPGWM+XeQ4+YDfzHG/EpE7ga6AO2wZum5AetuxlOBTcCvjTEVYk0V+SiQj3Ub+8XGmC1xejtUA6ctd5WyjDGrsc7x1sB2YJQxZgBwDlYSB2ve1K/tFv8/gcuAPcaYQcAg4AoR6RLkOH/dgDHA6cAbWBM39AMOAmNEJBNrOrbfGWOOAl4C7ovLL64UCaoKqVQSucuvZgJPisgRWFOk9Qxx/GiswlC/s9ebAj2A2qbg+8xunS/Cmolnsr19EVaXTi+s6oFf2BVh0wFttau40eSuUpbdLVOF1Wq/C9gG9MdqzZeGehjWBAxT/J7rhFpergzAGOMSkQpT3d/pwvo7E2CJMWZo5L+JUpHTbhmVkkSkAHgWeNJOtE2BLcYYF1bxp3T70H1AY6+HTgGusrtREJGeIpIX5LhILQcKRGSo/byZItI3iudTqkbacleppJGILMDqgqnEuoD6qL3vaeA9Efk9MA1rAnaAn4BKEVkIvAI8htWNMs+eUacIOCPIcfMjCcwYU2539TwuIk2x/vb+BSyJ/NdUqnY6WkYppVKQdssopVQK0uSulFIpSJO7UkqlIE3uSimVgjS5K6VUCtLkrpRSKUiTu1JKpaD/B2rahUpRkwnhAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['close'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "                              open     high       low    close  Adj Close  \\\nDatetime                                                                    \n2019-08-26 04:00:00-04:00  51.0250  51.4700  51.02500  51.4000    51.4000   \n2019-08-26 05:00:00-04:00  51.3575  51.7100  51.35750  51.4275    51.4275   \n2019-08-26 06:00:00-04:00  51.4125  51.7125  51.04000  51.5000    51.5000   \n2019-08-26 07:00:00-04:00  51.4950  51.6575  51.29000  51.6000    51.6000   \n2019-08-26 08:00:00-04:00  51.6200  51.7000  51.30305  51.4375    51.4375   \n\n                           volume  close_pct  \nDatetime                                      \n2019-08-26 04:00:00-04:00       0        NaN  \n2019-08-26 05:00:00-04:00       0   0.000535  \n2019-08-26 06:00:00-04:00       0   0.001410  \n2019-08-26 07:00:00-04:00       0   0.001942  \n2019-08-26 08:00:00-04:00       0  -0.003149  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-08-26 04:00:00-04:00</th>\n      <td>51.0250</td>\n      <td>51.4700</td>\n      <td>51.02500</td>\n      <td>51.4000</td>\n      <td>51.4000</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 05:00:00-04:00</th>\n      <td>51.3575</td>\n      <td>51.7100</td>\n      <td>51.35750</td>\n      <td>51.4275</td>\n      <td>51.4275</td>\n      <td>0</td>\n      <td>0.000535</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 06:00:00-04:00</th>\n      <td>51.4125</td>\n      <td>51.7125</td>\n      <td>51.04000</td>\n      <td>51.5000</td>\n      <td>51.5000</td>\n      <td>0</td>\n      <td>0.001410</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 07:00:00-04:00</th>\n      <td>51.4950</td>\n      <td>51.6575</td>\n      <td>51.29000</td>\n      <td>51.6000</td>\n      <td>51.6000</td>\n      <td>0</td>\n      <td>0.001942</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 08:00:00-04:00</th>\n      <td>51.6200</td>\n      <td>51.7000</td>\n      <td>51.30305</td>\n      <td>51.4375</td>\n      <td>51.4375</td>\n      <td>0</td>\n      <td>-0.003149</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close_pct'] = data['close'].pct_change()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "              open         high          low        close    Adj Close  \\\ncount  8373.000000  8373.000000  8373.000000  8373.000000  8373.000000   \nmean    101.617589   102.035643   101.110565   101.622214   101.622214   \nstd      29.966202    30.326098    29.801068    29.962638    29.962638   \nmin      50.792500    51.025024    50.727500    50.777500    50.777500   \n25%      72.117500    72.445000    71.755000    72.125000    72.125000   \n50%     112.379898   112.815000   111.572502   112.350000   112.350000   \n75%     127.110001   127.470000   126.629997   127.120003   127.120003   \nmax     151.410000   438.440000   151.290000   151.670000   151.670000   \n\n             volume    close_pct  \ncount  8.373000e+03  8372.000000  \nmean   3.816484e+06     0.000143  \nstd    6.940174e+06     0.005817  \nmin    0.000000e+00    -0.085960  \n25%    0.000000e+00    -0.001631  \n50%    0.000000e+00     0.000097  \n75%    5.513431e+06     0.001970  \nmax    9.845401e+07     0.078661  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8373.000000</td>\n      <td>8373.000000</td>\n      <td>8373.000000</td>\n      <td>8373.000000</td>\n      <td>8373.000000</td>\n      <td>8.373000e+03</td>\n      <td>8372.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>101.617589</td>\n      <td>102.035643</td>\n      <td>101.110565</td>\n      <td>101.622214</td>\n      <td>101.622214</td>\n      <td>3.816484e+06</td>\n      <td>0.000143</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>29.966202</td>\n      <td>30.326098</td>\n      <td>29.801068</td>\n      <td>29.962638</td>\n      <td>29.962638</td>\n      <td>6.940174e+06</td>\n      <td>0.005817</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>50.792500</td>\n      <td>51.025024</td>\n      <td>50.727500</td>\n      <td>50.777500</td>\n      <td>50.777500</td>\n      <td>0.000000e+00</td>\n      <td>-0.085960</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>72.117500</td>\n      <td>72.445000</td>\n      <td>71.755000</td>\n      <td>72.125000</td>\n      <td>72.125000</td>\n      <td>0.000000e+00</td>\n      <td>-0.001631</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>112.379898</td>\n      <td>112.815000</td>\n      <td>111.572502</td>\n      <td>112.350000</td>\n      <td>112.350000</td>\n      <td>0.000000e+00</td>\n      <td>0.000097</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>127.110001</td>\n      <td>127.470000</td>\n      <td>126.629997</td>\n      <td>127.120003</td>\n      <td>127.120003</td>\n      <td>5.513431e+06</td>\n      <td>0.001970</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>151.410000</td>\n      <td>438.440000</td>\n      <td>151.290000</td>\n      <td>151.670000</td>\n      <td>151.670000</td>\n      <td>9.845401e+07</td>\n      <td>0.078661</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data = add_all_ta_features(\n",
    "        data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    # data['normVol'] = data['volume'] / data['volume'].ewm(5).mean()\n",
    "    # for i in range(1,50):\n",
    "    #     data[f'close{i}'] = data['close'].shift(i)\n",
    "    # Remove columns that won't be used as features\n",
    "    # del (data['Adj Close'])\n",
    "\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2019-08-26 04:00:00-04:00   51.025000   51.470000   51.025000   51.400000   \n2019-08-26 05:00:00-04:00   51.357500   51.710000   51.357500   51.427500   \n2019-08-26 06:00:00-04:00   51.412500   51.712500   51.040000   51.500000   \n2019-08-26 07:00:00-04:00   51.495000   51.657500   51.290000   51.600000   \n2019-08-26 08:00:00-04:00   51.620000   51.700000   51.303050   51.437500   \n...                               ...         ...         ...         ...   \n2021-08-25 09:30:00-04:00  149.699997  150.320007  148.535004  148.625397   \n2021-08-25 10:30:00-04:00  148.620193  148.735001  148.119995  148.505005   \n2021-08-25 11:30:00-04:00  148.500000  148.548004  148.059998  148.239700   \n2021-08-25 12:30:00-04:00  148.237000  148.440002  148.169998  148.264999   \n2021-08-25 13:03:49-04:00  148.300003  148.300003  148.300003  148.300003   \n\n                            Adj Close    volume  close_pct  \nDatetime                                                    \n2019-08-26 04:00:00-04:00   51.400000         0        NaN  \n2019-08-26 05:00:00-04:00   51.427500         0   0.000535  \n2019-08-26 06:00:00-04:00   51.500000         0   0.001410  \n2019-08-26 07:00:00-04:00   51.600000         0   0.001942  \n2019-08-26 08:00:00-04:00   51.437500         0  -0.003149  \n...                               ...       ...        ...  \n2021-08-25 09:30:00-04:00  148.625397  15348852  -0.009428  \n2021-08-25 10:30:00-04:00  148.505005   9910979  -0.000810  \n2021-08-25 11:30:00-04:00  148.239700   5739516  -0.001787  \n2021-08-25 12:30:00-04:00  148.264999   2533769   0.000171  \n2021-08-25 13:03:49-04:00  148.300003         0   0.000236  \n\n[8373 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-08-26 04:00:00-04:00</th>\n      <td>51.025000</td>\n      <td>51.470000</td>\n      <td>51.025000</td>\n      <td>51.400000</td>\n      <td>51.400000</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 05:00:00-04:00</th>\n      <td>51.357500</td>\n      <td>51.710000</td>\n      <td>51.357500</td>\n      <td>51.427500</td>\n      <td>51.427500</td>\n      <td>0</td>\n      <td>0.000535</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 06:00:00-04:00</th>\n      <td>51.412500</td>\n      <td>51.712500</td>\n      <td>51.040000</td>\n      <td>51.500000</td>\n      <td>51.500000</td>\n      <td>0</td>\n      <td>0.001410</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 07:00:00-04:00</th>\n      <td>51.495000</td>\n      <td>51.657500</td>\n      <td>51.290000</td>\n      <td>51.600000</td>\n      <td>51.600000</td>\n      <td>0</td>\n      <td>0.001942</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 08:00:00-04:00</th>\n      <td>51.620000</td>\n      <td>51.700000</td>\n      <td>51.303050</td>\n      <td>51.437500</td>\n      <td>51.437500</td>\n      <td>0</td>\n      <td>-0.003149</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 09:30:00-04:00</th>\n      <td>149.699997</td>\n      <td>150.320007</td>\n      <td>148.535004</td>\n      <td>148.625397</td>\n      <td>148.625397</td>\n      <td>15348852</td>\n      <td>-0.009428</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 10:30:00-04:00</th>\n      <td>148.620193</td>\n      <td>148.735001</td>\n      <td>148.119995</td>\n      <td>148.505005</td>\n      <td>148.505005</td>\n      <td>9910979</td>\n      <td>-0.000810</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 11:30:00-04:00</th>\n      <td>148.500000</td>\n      <td>148.548004</td>\n      <td>148.059998</td>\n      <td>148.239700</td>\n      <td>148.239700</td>\n      <td>5739516</td>\n      <td>-0.001787</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 12:30:00-04:00</th>\n      <td>148.237000</td>\n      <td>148.440002</td>\n      <td>148.169998</td>\n      <td>148.264999</td>\n      <td>148.264999</td>\n      <td>2533769</td>\n      <td>0.000171</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 13:03:49-04:00</th>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>0</td>\n      <td>0.000236</td>\n    </tr>\n  </tbody>\n</table>\n<p>8373 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['open', 'high', 'low', 'close', 'Adj Close', 'volume', 'close_pct'], dtype='object')"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def create_class_column(row, lowest_threshold, higher_threshold):\n",
    "    if row['close_shift'] - row['close'] > higher_threshold:\n",
    "        return 1\n",
    "    if row['close_shift'] - row['close'] < lowest_threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2019-08-26 04:00:00-04:00   51.025000   51.470000   51.025000   51.400000   \n2019-08-26 05:00:00-04:00   51.357500   51.710000   51.357500   51.427500   \n2019-08-26 06:00:00-04:00   51.412500   51.712500   51.040000   51.500000   \n2019-08-26 07:00:00-04:00   51.495000   51.657500   51.290000   51.600000   \n2019-08-26 08:00:00-04:00   51.620000   51.700000   51.303050   51.437500   \n...                               ...         ...         ...         ...   \n2021-08-25 09:30:00-04:00  149.699997  150.320007  148.535004  148.625397   \n2021-08-25 10:30:00-04:00  148.620193  148.735001  148.119995  148.505005   \n2021-08-25 11:30:00-04:00  148.500000  148.548004  148.059998  148.239700   \n2021-08-25 12:30:00-04:00  148.237000  148.440002  148.169998  148.264999   \n2021-08-25 13:03:49-04:00  148.300003  148.300003  148.300003  148.300003   \n\n                            Adj Close    volume  close_pct    open_1  \\\nDatetime                                                               \n2019-08-26 04:00:00-04:00   51.400000         0        NaN       NaN   \n2019-08-26 05:00:00-04:00   51.427500         0   0.000535  0.332500   \n2019-08-26 06:00:00-04:00   51.500000         0   0.001410  0.055000   \n2019-08-26 07:00:00-04:00   51.600000         0   0.001942  0.082500   \n2019-08-26 08:00:00-04:00   51.437500         0  -0.003149  0.125000   \n...                               ...       ...        ...       ...   \n2021-08-25 09:30:00-04:00  148.625397  15348852  -0.009428 -0.280003   \n2021-08-25 10:30:00-04:00  148.505005   9910979  -0.000810 -1.079803   \n2021-08-25 11:30:00-04:00  148.239700   5739516  -0.001787 -0.120193   \n2021-08-25 12:30:00-04:00  148.264999   2533769   0.000171 -0.263000   \n2021-08-25 13:03:49-04:00  148.300003         0   0.000236  0.063004   \n\n                             open_2    open_3  ...  close_21  close_22  \\\nDatetime                                       ...                       \n2019-08-26 04:00:00-04:00       NaN       NaN  ...       NaN       NaN   \n2019-08-26 05:00:00-04:00       NaN       NaN  ...       NaN       NaN   \n2019-08-26 06:00:00-04:00  0.387500       NaN  ...       NaN       NaN   \n2019-08-26 07:00:00-04:00  0.137500  0.470000  ...       NaN       NaN   \n2019-08-26 08:00:00-04:00  0.207500  0.262500  ...       NaN       NaN   \n...                             ...       ...  ...       ...       ...   \n2021-08-25 09:30:00-04:00 -0.110003 -0.410003  ... -1.124603 -1.374603   \n2021-08-25 10:30:00-04:00 -1.359807 -1.189807  ... -1.134995 -1.244995   \n2021-08-25 11:30:00-04:00 -1.199997 -1.480000  ... -1.230300 -1.400300   \n2021-08-25 12:30:00-04:00 -0.383194 -1.462997  ... -1.335001 -1.205001   \n2021-08-25 13:03:49-04:00 -0.199997 -0.320190  ... -2.151093 -1.299997   \n\n                           close_23  close_24  close_25  close_26  close_27  \\\nDatetime                                                                      \n2019-08-26 04:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 05:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 06:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 07:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 08:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n...                             ...       ...       ...       ...       ...   \n2021-08-25 09:30:00-04:00 -1.514603 -1.064603 -1.124603 -1.094603 -1.049603   \n2021-08-25 10:30:00-04:00 -1.494995 -1.634995 -1.184995 -1.244995 -1.214995   \n2021-08-25 11:30:00-04:00 -1.510300 -1.760300 -1.900300 -1.450300 -1.510300   \n2021-08-25 12:30:00-04:00 -1.375001 -1.485001 -1.735001 -1.875001 -1.425001   \n2021-08-25 13:03:49-04:00 -1.169997 -1.339997 -1.449997 -1.699997 -1.839997   \n\n                           close_28  close_29  close_shift  \nDatetime                                                    \n2019-08-26 04:00:00-04:00       NaN       NaN      51.5750  \n2019-08-26 05:00:00-04:00       NaN       NaN      51.5000  \n2019-08-26 06:00:00-04:00       NaN       NaN      51.5125  \n2019-08-26 07:00:00-04:00       NaN       NaN      51.7225  \n2019-08-26 08:00:00-04:00       NaN       NaN      51.7750  \n...                             ...       ...          ...  \n2021-08-25 09:30:00-04:00 -1.064606 -1.339600          NaN  \n2021-08-25 10:30:00-04:00 -1.169995 -1.184998          NaN  \n2021-08-25 11:30:00-04:00 -1.480300 -1.435300          NaN  \n2021-08-25 12:30:00-04:00 -1.485001 -1.455001          NaN  \n2021-08-25 13:03:49-04:00 -1.389997 -1.449997          NaN  \n\n[8373 rows x 124 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>open_1</th>\n      <th>open_2</th>\n      <th>open_3</th>\n      <th>...</th>\n      <th>close_21</th>\n      <th>close_22</th>\n      <th>close_23</th>\n      <th>close_24</th>\n      <th>close_25</th>\n      <th>close_26</th>\n      <th>close_27</th>\n      <th>close_28</th>\n      <th>close_29</th>\n      <th>close_shift</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-08-26 04:00:00-04:00</th>\n      <td>51.025000</td>\n      <td>51.470000</td>\n      <td>51.025000</td>\n      <td>51.400000</td>\n      <td>51.400000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5750</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 05:00:00-04:00</th>\n      <td>51.357500</td>\n      <td>51.710000</td>\n      <td>51.357500</td>\n      <td>51.427500</td>\n      <td>51.427500</td>\n      <td>0</td>\n      <td>0.000535</td>\n      <td>0.332500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5000</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 06:00:00-04:00</th>\n      <td>51.412500</td>\n      <td>51.712500</td>\n      <td>51.040000</td>\n      <td>51.500000</td>\n      <td>51.500000</td>\n      <td>0</td>\n      <td>0.001410</td>\n      <td>0.055000</td>\n      <td>0.387500</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5125</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 07:00:00-04:00</th>\n      <td>51.495000</td>\n      <td>51.657500</td>\n      <td>51.290000</td>\n      <td>51.600000</td>\n      <td>51.600000</td>\n      <td>0</td>\n      <td>0.001942</td>\n      <td>0.082500</td>\n      <td>0.137500</td>\n      <td>0.470000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.7225</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 08:00:00-04:00</th>\n      <td>51.620000</td>\n      <td>51.700000</td>\n      <td>51.303050</td>\n      <td>51.437500</td>\n      <td>51.437500</td>\n      <td>0</td>\n      <td>-0.003149</td>\n      <td>0.125000</td>\n      <td>0.207500</td>\n      <td>0.262500</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.7750</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 09:30:00-04:00</th>\n      <td>149.699997</td>\n      <td>150.320007</td>\n      <td>148.535004</td>\n      <td>148.625397</td>\n      <td>148.625397</td>\n      <td>15348852</td>\n      <td>-0.009428</td>\n      <td>-0.280003</td>\n      <td>-0.110003</td>\n      <td>-0.410003</td>\n      <td>...</td>\n      <td>-1.124603</td>\n      <td>-1.374603</td>\n      <td>-1.514603</td>\n      <td>-1.064603</td>\n      <td>-1.124603</td>\n      <td>-1.094603</td>\n      <td>-1.049603</td>\n      <td>-1.064606</td>\n      <td>-1.339600</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 10:30:00-04:00</th>\n      <td>148.620193</td>\n      <td>148.735001</td>\n      <td>148.119995</td>\n      <td>148.505005</td>\n      <td>148.505005</td>\n      <td>9910979</td>\n      <td>-0.000810</td>\n      <td>-1.079803</td>\n      <td>-1.359807</td>\n      <td>-1.189807</td>\n      <td>...</td>\n      <td>-1.134995</td>\n      <td>-1.244995</td>\n      <td>-1.494995</td>\n      <td>-1.634995</td>\n      <td>-1.184995</td>\n      <td>-1.244995</td>\n      <td>-1.214995</td>\n      <td>-1.169995</td>\n      <td>-1.184998</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 11:30:00-04:00</th>\n      <td>148.500000</td>\n      <td>148.548004</td>\n      <td>148.059998</td>\n      <td>148.239700</td>\n      <td>148.239700</td>\n      <td>5739516</td>\n      <td>-0.001787</td>\n      <td>-0.120193</td>\n      <td>-1.199997</td>\n      <td>-1.480000</td>\n      <td>...</td>\n      <td>-1.230300</td>\n      <td>-1.400300</td>\n      <td>-1.510300</td>\n      <td>-1.760300</td>\n      <td>-1.900300</td>\n      <td>-1.450300</td>\n      <td>-1.510300</td>\n      <td>-1.480300</td>\n      <td>-1.435300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 12:30:00-04:00</th>\n      <td>148.237000</td>\n      <td>148.440002</td>\n      <td>148.169998</td>\n      <td>148.264999</td>\n      <td>148.264999</td>\n      <td>2533769</td>\n      <td>0.000171</td>\n      <td>-0.263000</td>\n      <td>-0.383194</td>\n      <td>-1.462997</td>\n      <td>...</td>\n      <td>-1.335001</td>\n      <td>-1.205001</td>\n      <td>-1.375001</td>\n      <td>-1.485001</td>\n      <td>-1.735001</td>\n      <td>-1.875001</td>\n      <td>-1.425001</td>\n      <td>-1.485001</td>\n      <td>-1.455001</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 13:03:49-04:00</th>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>0</td>\n      <td>0.000236</td>\n      <td>0.063004</td>\n      <td>-0.199997</td>\n      <td>-0.320190</td>\n      <td>...</td>\n      <td>-2.151093</td>\n      <td>-1.299997</td>\n      <td>-1.169997</td>\n      <td>-1.339997</td>\n      <td>-1.449997</td>\n      <td>-1.699997</td>\n      <td>-1.839997</td>\n      <td>-1.389997</td>\n      <td>-1.449997</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>8373 rows × 124 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "important_columns = ['open', 'high', 'low','close']\n",
    "\n",
    "\n",
    "for name in important_columns:\n",
    "    for i in range(1, 30):\n",
    "        calculate_diffs(i, name)\n",
    "\n",
    "data['close_shift'] = data.shift(-WINDOW)['close']\n",
    "data\n",
    "moved_price = data['close_shift'].copy()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "Datetime\n2019-08-26 04:00:00-04:00    51.5750\n2019-08-26 05:00:00-04:00    51.5000\n2019-08-26 06:00:00-04:00    51.5125\n2019-08-26 07:00:00-04:00    51.7225\n2019-08-26 08:00:00-04:00    51.7750\n                              ...   \n2021-08-25 09:30:00-04:00        NaN\n2021-08-25 10:30:00-04:00        NaN\n2021-08-25 11:30:00-04:00        NaN\n2021-08-25 12:30:00-04:00        NaN\n2021-08-25 13:03:49-04:00        NaN\nName: close_shift, Length: 8373, dtype: float64"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moved_price\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 open        high         low       close  \\\nDatetime                                                                    \n2019-08-26 04:00:00-04:00   51.025000   51.470000   51.025000   51.400000   \n2019-08-26 05:00:00-04:00   51.357500   51.710000   51.357500   51.427500   \n2019-08-26 06:00:00-04:00   51.412500   51.712500   51.040000   51.500000   \n2019-08-26 07:00:00-04:00   51.495000   51.657500   51.290000   51.600000   \n2019-08-26 08:00:00-04:00   51.620000   51.700000   51.303050   51.437500   \n...                               ...         ...         ...         ...   \n2021-08-25 09:30:00-04:00  149.699997  150.320007  148.535004  148.625397   \n2021-08-25 10:30:00-04:00  148.620193  148.735001  148.119995  148.505005   \n2021-08-25 11:30:00-04:00  148.500000  148.548004  148.059998  148.239700   \n2021-08-25 12:30:00-04:00  148.237000  148.440002  148.169998  148.264999   \n2021-08-25 13:03:49-04:00  148.300003  148.300003  148.300003  148.300003   \n\n                            Adj Close    volume  close_pct    open_1  \\\nDatetime                                                               \n2019-08-26 04:00:00-04:00   51.400000         0        NaN       NaN   \n2019-08-26 05:00:00-04:00   51.427500         0   0.000535  0.332500   \n2019-08-26 06:00:00-04:00   51.500000         0   0.001410  0.055000   \n2019-08-26 07:00:00-04:00   51.600000         0   0.001942  0.082500   \n2019-08-26 08:00:00-04:00   51.437500         0  -0.003149  0.125000   \n...                               ...       ...        ...       ...   \n2021-08-25 09:30:00-04:00  148.625397  15348852  -0.009428 -0.280003   \n2021-08-25 10:30:00-04:00  148.505005   9910979  -0.000810 -1.079803   \n2021-08-25 11:30:00-04:00  148.239700   5739516  -0.001787 -0.120193   \n2021-08-25 12:30:00-04:00  148.264999   2533769   0.000171 -0.263000   \n2021-08-25 13:03:49-04:00  148.300003         0   0.000236  0.063004   \n\n                             open_2    open_3  ...  close_22  close_23  \\\nDatetime                                       ...                       \n2019-08-26 04:00:00-04:00       NaN       NaN  ...       NaN       NaN   \n2019-08-26 05:00:00-04:00       NaN       NaN  ...       NaN       NaN   \n2019-08-26 06:00:00-04:00  0.387500       NaN  ...       NaN       NaN   \n2019-08-26 07:00:00-04:00  0.137500  0.470000  ...       NaN       NaN   \n2019-08-26 08:00:00-04:00  0.207500  0.262500  ...       NaN       NaN   \n...                             ...       ...  ...       ...       ...   \n2021-08-25 09:30:00-04:00 -0.110003 -0.410003  ... -1.374603 -1.514603   \n2021-08-25 10:30:00-04:00 -1.359807 -1.189807  ... -1.244995 -1.494995   \n2021-08-25 11:30:00-04:00 -1.199997 -1.480000  ... -1.400300 -1.510300   \n2021-08-25 12:30:00-04:00 -0.383194 -1.462997  ... -1.205001 -1.375001   \n2021-08-25 13:03:49-04:00 -0.199997 -0.320190  ... -1.299997 -1.169997   \n\n                           close_24  close_25  close_26  close_27  close_28  \\\nDatetime                                                                      \n2019-08-26 04:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 05:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 06:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 07:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n2019-08-26 08:00:00-04:00       NaN       NaN       NaN       NaN       NaN   \n...                             ...       ...       ...       ...       ...   \n2021-08-25 09:30:00-04:00 -1.064603 -1.124603 -1.094603 -1.049603 -1.064606   \n2021-08-25 10:30:00-04:00 -1.634995 -1.184995 -1.244995 -1.214995 -1.169995   \n2021-08-25 11:30:00-04:00 -1.760300 -1.900300 -1.450300 -1.510300 -1.480300   \n2021-08-25 12:30:00-04:00 -1.485001 -1.735001 -1.875001 -1.425001 -1.485001   \n2021-08-25 13:03:49-04:00 -1.339997 -1.449997 -1.699997 -1.839997 -1.389997   \n\n                           close_29  close_shift  class_column  \nDatetime                                                        \n2019-08-26 04:00:00-04:00       NaN      51.5750             0  \n2019-08-26 05:00:00-04:00       NaN      51.5000             0  \n2019-08-26 06:00:00-04:00       NaN      51.5125             0  \n2019-08-26 07:00:00-04:00       NaN      51.7225             0  \n2019-08-26 08:00:00-04:00       NaN      51.7750             0  \n...                             ...          ...           ...  \n2021-08-25 09:30:00-04:00 -1.339600          NaN             0  \n2021-08-25 10:30:00-04:00 -1.184998          NaN             0  \n2021-08-25 11:30:00-04:00 -1.435300          NaN             0  \n2021-08-25 12:30:00-04:00 -1.455001          NaN             0  \n2021-08-25 13:03:49-04:00 -1.449997          NaN             0  \n\n[8373 rows x 125 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>Adj Close</th>\n      <th>volume</th>\n      <th>close_pct</th>\n      <th>open_1</th>\n      <th>open_2</th>\n      <th>open_3</th>\n      <th>...</th>\n      <th>close_22</th>\n      <th>close_23</th>\n      <th>close_24</th>\n      <th>close_25</th>\n      <th>close_26</th>\n      <th>close_27</th>\n      <th>close_28</th>\n      <th>close_29</th>\n      <th>close_shift</th>\n      <th>class_column</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-08-26 04:00:00-04:00</th>\n      <td>51.025000</td>\n      <td>51.470000</td>\n      <td>51.025000</td>\n      <td>51.400000</td>\n      <td>51.400000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5750</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 05:00:00-04:00</th>\n      <td>51.357500</td>\n      <td>51.710000</td>\n      <td>51.357500</td>\n      <td>51.427500</td>\n      <td>51.427500</td>\n      <td>0</td>\n      <td>0.000535</td>\n      <td>0.332500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 06:00:00-04:00</th>\n      <td>51.412500</td>\n      <td>51.712500</td>\n      <td>51.040000</td>\n      <td>51.500000</td>\n      <td>51.500000</td>\n      <td>0</td>\n      <td>0.001410</td>\n      <td>0.055000</td>\n      <td>0.387500</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.5125</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 07:00:00-04:00</th>\n      <td>51.495000</td>\n      <td>51.657500</td>\n      <td>51.290000</td>\n      <td>51.600000</td>\n      <td>51.600000</td>\n      <td>0</td>\n      <td>0.001942</td>\n      <td>0.082500</td>\n      <td>0.137500</td>\n      <td>0.470000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.7225</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-26 08:00:00-04:00</th>\n      <td>51.620000</td>\n      <td>51.700000</td>\n      <td>51.303050</td>\n      <td>51.437500</td>\n      <td>51.437500</td>\n      <td>0</td>\n      <td>-0.003149</td>\n      <td>0.125000</td>\n      <td>0.207500</td>\n      <td>0.262500</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>51.7750</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 09:30:00-04:00</th>\n      <td>149.699997</td>\n      <td>150.320007</td>\n      <td>148.535004</td>\n      <td>148.625397</td>\n      <td>148.625397</td>\n      <td>15348852</td>\n      <td>-0.009428</td>\n      <td>-0.280003</td>\n      <td>-0.110003</td>\n      <td>-0.410003</td>\n      <td>...</td>\n      <td>-1.374603</td>\n      <td>-1.514603</td>\n      <td>-1.064603</td>\n      <td>-1.124603</td>\n      <td>-1.094603</td>\n      <td>-1.049603</td>\n      <td>-1.064606</td>\n      <td>-1.339600</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 10:30:00-04:00</th>\n      <td>148.620193</td>\n      <td>148.735001</td>\n      <td>148.119995</td>\n      <td>148.505005</td>\n      <td>148.505005</td>\n      <td>9910979</td>\n      <td>-0.000810</td>\n      <td>-1.079803</td>\n      <td>-1.359807</td>\n      <td>-1.189807</td>\n      <td>...</td>\n      <td>-1.244995</td>\n      <td>-1.494995</td>\n      <td>-1.634995</td>\n      <td>-1.184995</td>\n      <td>-1.244995</td>\n      <td>-1.214995</td>\n      <td>-1.169995</td>\n      <td>-1.184998</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 11:30:00-04:00</th>\n      <td>148.500000</td>\n      <td>148.548004</td>\n      <td>148.059998</td>\n      <td>148.239700</td>\n      <td>148.239700</td>\n      <td>5739516</td>\n      <td>-0.001787</td>\n      <td>-0.120193</td>\n      <td>-1.199997</td>\n      <td>-1.480000</td>\n      <td>...</td>\n      <td>-1.400300</td>\n      <td>-1.510300</td>\n      <td>-1.760300</td>\n      <td>-1.900300</td>\n      <td>-1.450300</td>\n      <td>-1.510300</td>\n      <td>-1.480300</td>\n      <td>-1.435300</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 12:30:00-04:00</th>\n      <td>148.237000</td>\n      <td>148.440002</td>\n      <td>148.169998</td>\n      <td>148.264999</td>\n      <td>148.264999</td>\n      <td>2533769</td>\n      <td>0.000171</td>\n      <td>-0.263000</td>\n      <td>-0.383194</td>\n      <td>-1.462997</td>\n      <td>...</td>\n      <td>-1.205001</td>\n      <td>-1.375001</td>\n      <td>-1.485001</td>\n      <td>-1.735001</td>\n      <td>-1.875001</td>\n      <td>-1.425001</td>\n      <td>-1.485001</td>\n      <td>-1.455001</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-08-25 13:03:49-04:00</th>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>148.300003</td>\n      <td>0</td>\n      <td>0.000236</td>\n      <td>0.063004</td>\n      <td>-0.199997</td>\n      <td>-0.320190</td>\n      <td>...</td>\n      <td>-1.299997</td>\n      <td>-1.169997</td>\n      <td>-1.339997</td>\n      <td>-1.449997</td>\n      <td>-1.699997</td>\n      <td>-1.839997</td>\n      <td>-1.389997</td>\n      <td>-1.449997</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8373 rows × 125 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_class(df):\n",
    "    higher_threshold = 1.5\n",
    "    lowest_threshold = -1.5\n",
    "    last_values_higher = []\n",
    "    last_values_lower = []\n",
    "    df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)), axis=1)\n",
    "    while True:\n",
    "        class_counts = df['class_column'].value_counts()\n",
    "        if abs(class_counts[0] - class_counts[1]) < 15 and abs(class_counts[0] - class_counts[-1]) < 15:\n",
    "            break\n",
    "\n",
    "        if len(last_values_higher) == 3:\n",
    "            last_values_higher.pop(0)\n",
    "        if len(last_values_lower) == 3:\n",
    "            last_values_lower.pop(0)\n",
    "\n",
    "        last_values_higher.append(higher_threshold)\n",
    "        last_values_lower.append(lowest_threshold)\n",
    "        if class_counts[0] > class_counts[1]:\n",
    "            higher_threshold -= 0.01\n",
    "        if class_counts[0] > class_counts[-1]:\n",
    "            lowest_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[1]:\n",
    "            higher_threshold += 0.01\n",
    "        if class_counts[0] < class_counts[-1]:\n",
    "            lowest_threshold -= 0.01\n",
    "\n",
    "        if higher_threshold in last_values_higher and lowest_threshold in last_values_lower:\n",
    "            break\n",
    "        df['class_column'] = df.apply((lambda x: create_class_column(x, lowest_threshold, higher_threshold)),\n",
    "                                      axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "data = create_class(data)\n",
    "\n",
    "data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\Desktop\\repo\\magisterka_analiza\\data\\results\\train_test\\AAPL_2y_16_diff_25_08_2021 19_04_05_full.csv\n"
     ]
    }
   ],
   "source": [
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\train_test\\\\{symbol}_{INTERVAL}_{WINDOW}_diff_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}_full.csv'\n",
    "data.to_csv(filename_to_export, index=True)\n",
    "print(filename_to_export)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": " 0    2823\n 1    2779\n-1    2771\nName: class_column, dtype: int64"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Class divide\n",
    "data['class_column'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# del (data['close'])\n",
    "# del (data['close_shift'])\n",
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": " 0    2790\n 1    2779\n-1    2759\nName: class_column, dtype: int64"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_column'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size=17):\n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i * chunk_size:(i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def train_model(model, train_x, train_y):\n",
    "    model.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "490"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataframe = split_dataframe(data, 17)\n",
    "len(splited_dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "y = data['class_column']\n",
    "features = [x for x in data.columns if x not in ['class_column', 'close_shift']]\n",
    "x = data[features]\n",
    "scaler = MinMaxScaler()\n",
    "# x = pd.DataFrame(scaler.fit_transform(x.values), columns=x.columns, index=x.index)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "classifiers = dict()\n",
    "classifiers['DecisionTreeClassifier 1'] = DecisionTreeClassifier(max_depth=15, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 2'] = DecisionTreeClassifier(max_depth=40, random_state=0, criterion='gini',\n",
    "                                                                 splitter='best')\n",
    "classifiers['DecisionTreeClassifier 3'] = DecisionTreeClassifier(max_depth=10, random_state=0,criterion='gini',splitter='best')\n",
    "\n",
    "\n",
    "classifiers['RandomForestClassifier 1'] = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0,\n",
    "                                                                 criterion='gini', n_jobs=-1)\n",
    "\n",
    "classifiers['RandomForestClassifier 2'] = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0,\n",
    "                                                                 criterion='entropy', n_jobs=-1)\n",
    "\n",
    "classifiers['RandomForestClassifier 3'] = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=0,\n",
    "                                                                 criterion='entropy', n_jobs=-1)\n",
    "\n",
    "classifiers['GradientBoostingClassifier 1'] = GradientBoostingClassifier(n_estimators=100, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.1)\n",
    "\n",
    "classifiers['GradientBoostingClassifier 2'] = GradientBoostingClassifier(n_estimators=1000, random_state=0,\n",
    "                                                                         criterion='friedman_mse', max_depth=3,\n",
    "                                                                         learning_rate=0.5)\n",
    "\n",
    "classifiers['XGBRFClassifier 1'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=2, n_estimators=1000, eta=0.2 )\n",
    "classifiers['XGBRFClassifier 2'] = xgb.sklearn.XGBRFClassifier(n_jobs=-1, max_depth=6, n_estimators=1000, eta=0.3)\n",
    "\n",
    "classifiers['XGBClassifier 1'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.2)\n",
    "classifiers['XGBClassifier 2'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.3)\n",
    "\n",
    "classifiers_boosted = dict()\n",
    "# classifiers_boosted['SGradientBoostingClassifier 1'] = GradientBoostingClassifier(n_estimators=100,random_state=0,criterion='friedman_mse',max_depth=3, learning_rate=0.1)\n",
    "classifiers_boosted['SXGBClassifier 1'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.2)\n",
    "classifiers_boosted['SXGBClassifier 2'] = xgb.XGBClassifier(nthread=-1, max_depth=3, n_estimators=1000, eta=0.3)\n",
    "classifiers_boosted['SXGBClassifier 3'] = xgb.XGBClassifier(nthread=-1, max_depth=10, n_estimators=1000, eta=0.2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "78.39250183105469"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close'][1701]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "81.7125015258789"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['close'][1718]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "4.235098532738775"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((data['close'][1718] / data['close'][1701])*100)-100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def calculate_change(predicted, correct, index):\n",
    "    # wzrost\n",
    "    change_percentage = (((data['close'][index+17] / data['close'][index])*100)-100)\n",
    "    if (correct == 1):\n",
    "        if (predicted == correct):\n",
    "            return change_percentage\n",
    "        else:\n",
    "            return 0\n",
    "    # spadek\n",
    "    elif (correct == -1):\n",
    "        if (predicted == correct):\n",
    "            return 0\n",
    "        else:\n",
    "            return change_percentage\n",
    "\n",
    "    # stagnacja\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Datetime'>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['close'][1000:1700].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n"
     ]
    }
   ],
   "source": [
    "print(len(splited_dataframe))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "start\n",
      "Calculate:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:06:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:12:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:12:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:12:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:12:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:14:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:17:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:17:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:18:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:20:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:20:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:20:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:20:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:23:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:23:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:23:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:23:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:26:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:26:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:26:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:26:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:29:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:29:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:29:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:32:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:32:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:32:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:32:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:35:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:35:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:35:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:35:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:38:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:38:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:38:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:38:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:41:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:41:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:44:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:44:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:44:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:45:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:51:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:51:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:51:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:54:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:54:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:54:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:57:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:57:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:57:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:00:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:04:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:04:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:07:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:07:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:07:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:07:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:10:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:10:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:10:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:10:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:14:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:14:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:14:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:14:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:17:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:17:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:17:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:17:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:20:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:21:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:24:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:24:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:24:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:24:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:27:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:27:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:28:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:31:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:31:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:31:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:31:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:34:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:34:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:35:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:38:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:38:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:38:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Calculate:  130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exomat\\anaconda3\\envs\\magisterka_analiza\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "first_row = 99\n",
    "start = 100\n",
    "start_data = splited_dataframe[:first_row + 1]\n",
    "next_data = splited_dataframe[first_row + 1:first_row + 1 + 31]\n",
    "print(len(next_data))\n",
    "score = defaultdict(list)\n",
    "points = defaultdict(list)\n",
    "points_train = defaultdict(list)\n",
    "score_train = defaultdict(list)\n",
    "step_headers = []\n",
    "i = 0\n",
    "print(\"start\")\n",
    "for idx, day in enumerate(next_data):\n",
    "    first_row += 1\n",
    "    start_data.append(day)\n",
    "    data_set = pd.concat(start_data)\n",
    "    data_set = create_class(data_set)\n",
    "    y = data_set['class_column']\n",
    "    features = [x for x in data_set.columns if x not in ['class_column']]\n",
    "    x = data_set[features]\n",
    "    x_train = x.iloc[:-17]\n",
    "    y_train = y.iloc[:-17]\n",
    "    x_test = x.iloc[-17:]\n",
    "    y_test = y.iloc[-17:]\n",
    "\n",
    "    i = i + 1\n",
    "    predictions_train = dict()\n",
    "    predictions = dict()\n",
    "    print(\"Calculate: \", first_row)\n",
    "    for k, v in classifiers.items():\n",
    "        train_model(v, x_train, y_train)\n",
    "        # predictions_train[k] = v.predict(x_train)\n",
    "        # score_train[k].append(accuracy_score(y_train.values, predictions_train[k]))\n",
    "\n",
    "        predicted_class = v.predict(x_test)\n",
    "        for idx, el in enumerate(predicted_class):\n",
    "            score[k].append(calculate_change(el, y_test[idx], (first_row*17)+idx))\n",
    "    #\n",
    "    # rfe = RFE(classifiers['XGBRFClassifier 1'], 10)\n",
    "    # fited = rfe.fit(x_train, y_train)\n",
    "    # names = x.columns\n",
    "    # columns = []\n",
    "    # for i in range(len(fited.support_)):\n",
    "    #     if fited.support_[i]:\n",
    "    #         columns.append(names[i])\n",
    "    #\n",
    "    # print(\"Columns with predictive power:\", columns)\n",
    "    # columns = columns + ['high', 'low', 'volume', 'open']\n",
    "    # x_test_cropped = x_test[columns]\n",
    "    # x_train_cropped = x_train[columns]\n",
    "    # x_test_cropped = x_test_cropped.loc[:, ~x_test_cropped.columns.duplicated()]\n",
    "    # x_train_cropped = x_train_cropped.loc[:, ~x_train_cropped.columns.duplicated()]\n",
    "    # for k, v in classifiers_boosted.items():\n",
    "    #     print(\"Calculate: \", k)\n",
    "    #     train_model(v, x_train, y_train)\n",
    "    #     predicted_class = v.predict(x_test)\n",
    "    #     for idx, el in enumerate(predicted_class):\n",
    "    #         score[k].append(calculate_change(el, y_test[idx], (first_row*17)+idx))\n",
    "    #\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "527"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score['DecisionTreeClassifier 1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "527"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_headers.clear()\n",
    "for a in range(start,start + 31):\n",
    "    for b in range (1,18):\n",
    "        step_headers.append(f'<{a},{b}>')\n",
    "len(step_headers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+\n",
      "|    | Classifier type              |   <100,1> |   <100,2> |   <100,3> |   <100,4> |   <100,5> |   <100,6> |   <100,7> |   <100,8> |     <100,9> |   <100,10> |   <100,11> |   <100,12> |   <100,13> |   <100,14> |   <100,15> |   <100,16> |   <100,17> |   <101,1> |   <101,2> |   <101,3> |   <101,4> |   <101,5> |   <101,6> |   <101,7> |   <101,8> |   <101,9> |   <101,10> |   <101,11> |   <101,12> |   <101,13> |   <101,14> |   <101,15> |   <101,16> |   <101,17> |   <102,1> |   <102,2> |   <102,3> |   <102,4> |   <102,5> |   <102,6> |   <102,7> |   <102,8> |   <102,9> |   <102,10> |   <102,11> |   <102,12> |   <102,13> |   <102,14> |   <102,15> |   <102,16> |   <102,17> |   <103,1> |   <103,2> |   <103,3> |   <103,4> |   <103,5> |   <103,6> |   <103,7> |   <103,8> |   <103,9> |   <103,10> |   <103,11> |   <103,12> |   <103,13> |   <103,14> |   <103,15> |   <103,16> |   <103,17> |   <104,1> |   <104,2> |   <104,3> |   <104,4> |   <104,5> |   <104,6> |   <104,7> |   <104,8> |   <104,9> |   <104,10> |   <104,11> |   <104,12> |   <104,13> |   <104,14> |   <104,15> |   <104,16> |   <104,17> |   <105,1> |   <105,2> |   <105,3> |   <105,4> |   <105,5> |   <105,6> |   <105,7> |   <105,8> |   <105,9> |   <105,10> |   <105,11> |   <105,12> |   <105,13> |   <105,14> |   <105,15> |   <105,16> |   <105,17> |   <106,1> |   <106,2> |   <106,3> |   <106,4> |   <106,5> |   <106,6> |   <106,7> |   <106,8> |   <106,9> |   <106,10> |   <106,11> |   <106,12> |   <106,13> |   <106,14> |   <106,15> |   <106,16> |   <106,17> |   <107,1> |   <107,2> |   <107,3> |   <107,4> |   <107,5> |   <107,6> |   <107,7> |   <107,8> |   <107,9> |   <107,10> |   <107,11> |   <107,12> |   <107,13> |   <107,14> |   <107,15> |   <107,16> |   <107,17> |   <108,1> |   <108,2> |   <108,3> |   <108,4> |   <108,5> |   <108,6> |   <108,7> |   <108,8> |   <108,9> |   <108,10> |   <108,11> |   <108,12> |   <108,13> |   <108,14> |   <108,15> |   <108,16> |   <108,17> |   <109,1> |   <109,2> |   <109,3> |   <109,4> |   <109,5> |   <109,6> |   <109,7> |   <109,8> |   <109,9> |   <109,10> |   <109,11> |   <109,12> |   <109,13> |   <109,14> |   <109,15> |   <109,16> |   <109,17> |   <110,1> |   <110,2> |   <110,3> |   <110,4> |   <110,5> |   <110,6> |   <110,7> |   <110,8> |   <110,9> |   <110,10> |   <110,11> |   <110,12> |   <110,13> |   <110,14> |   <110,15> |   <110,16> |   <110,17> |   <111,1> |   <111,2> |   <111,3> |   <111,4> |   <111,5> |   <111,6> |   <111,7> |   <111,8> |   <111,9> |   <111,10> |   <111,11> |   <111,12> |   <111,13> |   <111,14> |   <111,15> |   <111,16> |   <111,17> |   <112,1> |   <112,2> |   <112,3> |   <112,4> |   <112,5> |   <112,6> |   <112,7> |   <112,8> |   <112,9> |   <112,10> |   <112,11> |   <112,12> |   <112,13> |   <112,14> |   <112,15> |   <112,16> |   <112,17> |   <113,1> |   <113,2> |   <113,3> |   <113,4> |   <113,5> |   <113,6> |   <113,7> |   <113,8> |   <113,9> |   <113,10> |   <113,11> |   <113,12> |   <113,13> |   <113,14> |   <113,15> |   <113,16> |   <113,17> |   <114,1> |   <114,2> |   <114,3> |   <114,4> |   <114,5> |   <114,6> |   <114,7> |   <114,8> |   <114,9> |   <114,10> |   <114,11> |   <114,12> |   <114,13> |   <114,14> |   <114,15> |   <114,16> |   <114,17> |   <115,1> |   <115,2> |   <115,3> |   <115,4> |   <115,5> |   <115,6> |   <115,7> |   <115,8> |   <115,9> |   <115,10> |   <115,11> |   <115,12> |   <115,13> |   <115,14> |   <115,15> |   <115,16> |   <115,17> |   <116,1> |   <116,2> |   <116,3> |   <116,4> |   <116,5> |   <116,6> |   <116,7> |   <116,8> |   <116,9> |   <116,10> |   <116,11> |   <116,12> |   <116,13> |   <116,14> |   <116,15> |   <116,16> |   <116,17> |   <117,1> |   <117,2> |   <117,3> |   <117,4> |   <117,5> |   <117,6> |   <117,7> |   <117,8> |   <117,9> |   <117,10> |   <117,11> |   <117,12> |   <117,13> |   <117,14> |   <117,15> |   <117,16> |   <117,17> |   <118,1> |   <118,2> |   <118,3> |   <118,4> |   <118,5> |   <118,6> |   <118,7> |   <118,8> |   <118,9> |   <118,10> |   <118,11> |   <118,12> |   <118,13> |   <118,14> |   <118,15> |   <118,16> |   <118,17> |   <119,1> |   <119,2> |   <119,3> |   <119,4> |   <119,5> |   <119,6> |   <119,7> |   <119,8> |   <119,9> |   <119,10> |   <119,11> |   <119,12> |   <119,13> |   <119,14> |   <119,15> |   <119,16> |   <119,17> |   <120,1> |   <120,2> |   <120,3> |   <120,4> |   <120,5> |   <120,6> |   <120,7> |   <120,8> |   <120,9> |   <120,10> |   <120,11> |   <120,12> |   <120,13> |   <120,14> |   <120,15> |   <120,16> |   <120,17> |   <121,1> |   <121,2> |   <121,3> |   <121,4> |   <121,5> |   <121,6> |   <121,7> |   <121,8> |   <121,9> |   <121,10> |   <121,11> |   <121,12> |   <121,13> |   <121,14> |   <121,15> |   <121,16> |   <121,17> |   <122,1> |   <122,2> |   <122,3> |   <122,4> |   <122,5> |   <122,6> |   <122,7> |   <122,8> |   <122,9> |   <122,10> |   <122,11> |   <122,12> |   <122,13> |   <122,14> |   <122,15> |   <122,16> |   <122,17> |   <123,1> |   <123,2> |   <123,3> |   <123,4> |   <123,5> |   <123,6> |   <123,7> |   <123,8> |   <123,9> |   <123,10> |   <123,11> |   <123,12> |   <123,13> |   <123,14> |   <123,15> |   <123,16> |   <123,17> |   <124,1> |   <124,2> |   <124,3> |   <124,4> |   <124,5> |   <124,6> |   <124,7> |   <124,8> |   <124,9> |   <124,10> |   <124,11> |   <124,12> |   <124,13> |   <124,14> |   <124,15> |   <124,16> |   <124,17> |   <125,1> |   <125,2> |   <125,3> |   <125,4> |   <125,5> |   <125,6> |   <125,7> |   <125,8> |   <125,9> |   <125,10> |   <125,11> |   <125,12> |   <125,13> |   <125,14> |   <125,15> |   <125,16> |   <125,17> |   <126,1> |   <126,2> |   <126,3> |   <126,4> |   <126,5> |   <126,6> |   <126,7> |   <126,8> |   <126,9> |   <126,10> |   <126,11> |   <126,12> |   <126,13> |   <126,14> |   <126,15> |   <126,16> |   <126,17> |   <127,1> |   <127,2> |   <127,3> |   <127,4> |   <127,5> |   <127,6> |   <127,7> |   <127,8> |   <127,9> |   <127,10> |   <127,11> |   <127,12> |   <127,13> |   <127,14> |   <127,15> |   <127,16> |   <127,17> |   <128,1> |   <128,2> |   <128,3> |   <128,4> |   <128,5> |   <128,6> |   <128,7> |   <128,8> |   <128,9> |   <128,10> |   <128,11> |   <128,12> |   <128,13> |   <128,14> |   <128,15> |   <128,16> |   <128,17> |   <129,1> |   <129,2> |   <129,3> |   <129,4> |   <129,5> |   <129,6> |   <129,7> |   <129,8> |   <129,9> |   <129,10> |   <129,11> |   <129,12> |   <129,13> |   <129,14> |   <129,15> |   <129,16> |   <129,17> |   <130,1> |   <130,2> |   <130,3> |   <130,4> |   <130,5> |   <130,6> |   <130,7> |   <130,8> |   <130,9> |   <130,10> |   <130,11> |   <130,12> |   <130,13> |   <130,14> |   <130,15> |   <130,16> |   <130,17> |       sum |\n",
      "|----+------------------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------|\n",
      "|  0 | DecisionTreeClassifier 1     |   3.77278 |    4.2351 |   0       |    0      |   0       |   0       |         2 |   1.87877 | -0.00929056 |          0 | -0.0186567 |   -0.87963 |   -1.04327 |   -1.20589 |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |   -4.6432 |         0 |  -3.97648 |          0 |    -4.5843 |   -4.62419 |          0 |   -5.44901 |   -2.68497 |  -0.902958 |    0       |   -2.9313 |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    0       |    1.55221 |     0      |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   1.61605 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   1.23457 |   1.22145 |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |   -1.94461 |          0 |   -1.61434 |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |   -1.10753 |  -0.737095 |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0.953951 |   0        |    0       |    0       |    2.37477 |    0       |    0       |    0       |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    1.1516 |    1.3354 |         0 |  0        |         0 |         0 | -0.239778 | -0.415857 | -0.344349 |  -0.537634 | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0         |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |    0       |   -5.37108 |   0       |   0       |   0       |  -3.53643 |  -4.22702 |   0       |    -4.789 |   0       |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |    0       |    0       |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0          |         0 |   1.08933 |   1.40092 |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |   0       |  -8.64505 |   0       |   -6.89837 |    0       |    0       |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    7.56362 |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   0       |   0       |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     0      |    1.44843 |   0        |   0        |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 |  0        |  -1.21992 |  -1.80141 | -0.568182 |    0      |  -1.52018 |  -2.24782 |  -0.837535 |    1.62312 |    2.04377 |          0 |    0       |          0 |    4.24804 |          0 |   4.88153 |   3.35075 |         0 |  0        |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |    0       |    0       |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |   0       |   0       |   0       |   0       |  -5.23745 |   0       |  -3.89305 |   0       |   -3.18777 |    0       |   -3.55285 |    0       |    0       |     0      |      0     |    0       |   0       |   0       |  -5.31289 |  -5.49879 |  -6.12456 |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |   -8.22664 |    0       |    0       |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0        |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |   -2.64148 |   -2.89148 |    0       |    -1.3381 |          0 |   0       |  -5.85389 |   0       |  -7.49774 |  -7.89142 |  -5.72372 |   0       |  -9.35872 |  -5.27969 |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |   0       |   0       |   0       |   0       |   0       |   0       |   0.56852 |         0 |  -3.04919 |     0      |   -4.14027 |    -13.159 |   -10.5878 |   -10.7377 |   -10.1989 |   -8.65448 |          0 |   10.3981 |\n",
      "|  1 | DecisionTreeClassifier 2     |   3.77278 |    4.2351 |   0       |    0      |   0       |   0       |         0 |   1.87877 | -0.00929056 |          0 |  0         |    0       |   -1.04327 |   -1.20589 |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |   -4.6432 |         0 |  -3.97648 |          0 |    -4.5843 |   -4.62419 |          0 |   -5.44901 |   -2.68497 |  -0.902958 |    0       |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    0       |    0       |     0      |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   1.61605 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   1.23457 |   1.22145 |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |   -1.94461 |          0 |   -1.61434 |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |    0       |  -0.737095 |          0 |    1.01712 |          0 |          0 |         0 |         0 |   1.28688 |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0.953951 |   0.787757 |    0       |    0       |    2.37477 |    0       |    2.44208 |    1.32028 |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    1.1516 |    1.3354 |         0 |  0        |         0 |         0 |  0        |  0        |  0        |   0        |  0         |          0 |   0        |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |    -5.5622 |    0       |    0       |   -5.19166 |   -6.71796 |    0       |    0       |   0       |   0       |   0       |  -3.53643 |  -4.22702 |   0       |    -4.789 |   0       |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |    0       |    0       |  -1.61219 |   0       |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0          |         0 |   0       |   0       |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |   0       |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |   0       |  -8.64505 |   0       |   -6.89837 |    0       |    0       |   -2.72436 |   -3.02339 |   -4.41286 |    0       |    0       |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    7.56362 |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   0       |   0       |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    1.44843 |   0        |   0        |   0.946811 |          0 |    -3.4065 |   0       |  -3.51201 | -0.700101 |   0       |  -1.80141 | -0.568182 |   -1.9861 |  -1.52018 |   0       |  -0.837535 |    1.62312 |    2.04377 |          0 |    0       |          0 |    4.24804 |          0 |   4.88153 |   3.35075 |         0 | -0.388042 |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |    0       |    0       |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |   0       |   0       |   0       |   0       |  -5.23745 |  -2.28591 |   0       |   0       |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |    0       |     0      |      0     |    0       |   0       |   0       |  -5.31289 |  -5.49879 |  -6.12456 |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |   -8.22664 |   -6.14543 |   -4.46771 |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |   -3.63331 |   -2.64148 |   -2.89148 |    0       |     0      |          0 |   0       |  -5.85389 |  -6.42367 |  -7.49774 |  -7.89142 |  -5.72372 |   0       |  -9.35872 |   0       |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |         0 |   0       |    -2.7895 |    0       |      0     |   -10.5878 |   -10.7377 |   -10.1989 |   -8.65448 |          0 |   55.8234 |\n",
      "|  2 | DecisionTreeClassifier 3     |   3.77278 |    4.2351 |   3.40683 |    3.1417 |   3.23135 |   3.19122 |         2 |   0       | -0.00929056 |          0 | -0.0186567 |   -0.87963 |   -1.04327 |   -1.20589 |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 | -0.314613 |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |   0        |   -1.45157 |    0      |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0.823269 |          0 |         0 |         0 |         0 |         0 |         0 |   1.61605 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   1.23457 |   1.22145 |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |   -1.6848 |         0 |         0 |         0 |         0 |          0 |   -1.16092 |   -1.10753 |  -0.737095 |          0 |    0       |          0 |          0 |         0 |         0 |   1.28688 |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    2.37477 |    0       |    0       |    0       |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    1.1516 |    1.3354 |         0 |  0        |         0 |         0 | -0.239778 | -0.415857 | -0.344349 |  -0.537634 | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0         |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |    0       |    0       |  -5.59105 |  -5.13477 |  -4.40971 |  -3.53643 |  -4.22702 |  -4.67687 |    -4.789 |   0       |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |    0       |    0       |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0.00340225 |         0 |   1.08933 |   1.40092 |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |  -8.64505 |  -6.94907 |   -6.89837 |   -9.63855 |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    7.56362 |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   0       |   0       |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    1.44843 |   0        |   0        |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 | -0.700101 |   0       |  -1.80141 | -0.568182 |   -1.9861 |  -1.52018 |  -2.24782 |  -0.837535 |    1.62312 |    2.04377 |          0 |    0       |          0 |    4.24804 |          0 |   4.88153 |   3.35075 |         0 | -0.388042 |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |    0       |    0       |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |   0       |   0       |   0       |  -5.43497 |  -5.23745 |  -2.28591 |  -3.89305 |   0       |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |     0      |      0     |    0       |  -5.60134 |   0       |  -5.31289 |  -5.49879 |  -6.12456 |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |   -8.22664 |   -6.14543 |   -4.46771 |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0        |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |   -2.64148 |   -2.89148 |   -1.05579 |    -1.3381 |          0 |  -6.83485 |  -5.85389 |   0       |   0       |  -7.89142 |   0       |   0       |   0       |  -5.27969 |    0       |   -8.12968 |   -9.91973 |   -11.3114 |   -13.2499 |   -12.9776 |   -13.3304 |    0       |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |   0       |   0       |   0       |   0       |   0       |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |    -13.159 |   -10.5878 |   -10.7377 |   -10.1989 |   -8.65448 |          0 |  -80.1474 |\n",
      "|  3 | RandomForestClassifier 1     |   3.77278 |    4.2351 |   0       |    0      |   0       |   0       |         0 |   0       |  0          |          0 |  0         |    0       |    0       |    0       |    0       |    0       |   0        |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |  -2.59445 |  -3.04355 |  -3.85195 |         0 |    0      |         0 |   0       |          0 |     0      |   -4.62419 |          0 |    0       |    0       |  -0.902958 |   -1.45157 |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    0       |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |  0        |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |    0       |    0       |   0        |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    0       |    0       |    0       |    0       |  0        |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    0      |    0      |         0 |  0        |         0 |         0 |  0        |  0        |  0        |   0        |  0         |          0 |   0        |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0.780031 |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |  -2.17969 |   -2.2474 |    -5.8805 |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |   -5.78021 |   -5.37108 |  -5.59105 |  -5.13477 |  -4.40971 |  -3.53643 |  -4.22702 |  -4.67687 |    -4.789 |  -4.31586 |  -3.61832 |          0 |          0 |          0 |  -0.231788 |          0 |   0.228949 |   -2.65257 |   -2.07363 |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0.00340225 |         0 |   0       |   0       |  0.978825 |         0 |  0.955691 |         0 |    0      |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |  -8.64505 |  -6.94907 |   -6.89837 |   -9.63855 |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    0       |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    0       |   0        |   0        |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 | -0.700101 |  -1.21992 |  -1.80141 | -0.568182 |   -1.9861 |  -1.52018 |  -2.24782 |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   0       |   0       |         0 | -0.388042 |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |   -1.89944 |   -2.44186 |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |  -4.27676 |  -4.10751 |  -3.89892 |  -5.43497 |  -5.23745 |  -2.28591 |  -3.89305 |  -4.75845 |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |    -2.0319 |     -2.165 |   -8.43702 |  -5.60134 |  -4.27772 |  -5.31289 |  -5.49879 |  -6.12456 |  -2.49866 |  -5.24452 |  -4.10492 |  -3.99398 |   -6.30455 |   -4.44648 |   -6.20065 |   -8.12859 |   -8.22664 |   -6.14543 |   -4.46771 |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |  -6.83485 |  -5.85389 |  -6.42367 |  -7.49774 |  -7.89142 |  -5.72372 |   0       |  -9.35872 |  -5.27969 |   -7.54655 |   -8.12968 |   -9.91973 |   -11.3114 |   -13.2499 |   -12.9776 |   -13.3304 |   -1.97719 |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |  -4.99614 |  -5.84851 |  -9.37382 |  -9.22921 |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |      0     |     0      |     0      |     0      |    0       |          0 | -251.135  |\n",
      "|  4 | RandomForestClassifier 2     |   3.77278 |    4.2351 |   0       |    0      |   0       |   0       |         0 |   0       |  0          |          0 |  0         |    0       |    0       |    0       |    0       |    0       |   0        |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |   -4.62419 |          0 |    0       |    0       |  -0.902958 |   -1.45157 |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |  0        |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |    0       |    0       |   0        |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0.680947 |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    0       |    0       |    0       |    0       |  0        |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    0      |    0      |         0 |  0        |         0 |         0 |  0        |  0        |  0        |   0        |  0         |          0 |   0        |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    2.04017 |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0.780031 |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |  -2.17969 |   -2.2474 |    -5.8805 |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |   -5.78021 |   -5.37108 |  -5.59105 |  -5.13477 |  -4.40971 |  -3.53643 |  -4.22702 |  -4.67687 |    -4.789 |  -4.31586 |  -3.61832 |          0 |          0 |          0 |  -0.231788 |          0 |   0.228949 |   -2.65257 |   -2.07363 |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0.00340225 |         0 |   0       |   0       |  0.978825 |         0 |  0.955691 |         0 |    0      |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |  -8.64505 |  -6.94907 |   -6.89837 |   -9.63855 |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    0       |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    0       |   0        |   0        |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 | -0.700101 |  -1.21992 |  -1.80141 | -0.568182 |   -1.9861 |  -1.52018 |  -2.24782 |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   0       |   0       |         0 | -0.388042 |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |   -1.89944 |   -2.44186 |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |  -4.27676 |  -4.10751 |  -3.89892 |  -5.43497 |  -5.23745 |  -2.28591 |  -3.89305 |  -4.75845 |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |    -2.0319 |     -2.165 |   -8.43702 |  -5.60134 |  -4.27772 |  -5.31289 |  -5.49879 |  -6.12456 |  -2.49866 |  -5.24452 |  -4.10492 |  -3.99398 |   -6.30455 |   -4.44648 |   -6.20065 |   -8.12859 |   -8.22664 |   -6.14543 |   -4.46771 |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |  -6.83485 |  -5.85389 |  -6.42367 |  -7.49774 |  -7.89142 |  -5.72372 |  -7.68896 |  -9.35872 |  -5.27969 |   -7.54655 |   -8.12968 |   -9.91973 |   -11.3114 |   -13.2499 |   -12.9776 |   -13.3304 |   -1.97719 |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |  -4.99614 |  -5.84851 |  -9.37382 |  -9.22921 |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |      0     |     0      |     0      |     0      |    0       |          0 | -245.114  |\n",
      "|  5 | RandomForestClassifier 3     |   3.77278 |    4.2351 |   0       |    0      |   0       |   0       |         0 |   0       |  0          |          0 |  0         |    0       |    0       |    0       |    0       |    0       |   0        |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |  -1.66815 |  -2.39031 |  -2.59445 |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |   -4.62419 |          0 |    0       |    0       |  -0.902958 |   -1.45157 |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |  0        |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |    0       |    0       |   0        |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   0       |   0       |   0       |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    0       |    0       |    0       |    0       |  0.361371 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |   0        |          0 |    0      |    0      |         0 |  0        |         0 |         0 |  0        |  0        | -0.344349 |   0        | -0.0814797 |          0 |  -0.135239 |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |   -1.63934 |          0 |          0 |   -1.57895 |          0 |          0 |          0 |          0 |         0 |         0 |  -1.44254 |  -2.72549 |    0      |   0       |   -2.3446 |  -2.17969 |   -2.2474 |    -5.8805 |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |   -5.78021 |   -5.37108 |  -5.59105 |  -5.13477 |  -4.40971 |  -3.53643 |  -4.22702 |  -4.67687 |    -4.789 |  -4.31586 |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |   -2.65257 |   -2.07363 |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0          |         0 |   0       |   0       |  0        |         0 |  0        |         0 |    0      |  0        |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |   0       |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |  -8.64505 |   0       |    0       |    0       |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |   0       |         0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    10.9887 |    0       |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   0       |   0       |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    1.44843 |   0.486258 |  -0.176812 |   0.946811 |          0 |    -3.4065 |   0       |  -3.51201 | -0.700101 |   0       |  -1.80141 | -0.568182 |    0      |  -1.52018 |   0       |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   4.88153 |   3.35075 |         0 | -0.388042 |  0.388514 |         0 |         0 |  0        |         0 |          0 |          0 |    0       |   -2.44186 |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |  -4.45168 |  -4.27676 |  -4.10751 |   0       |  -5.43497 |  -5.23745 |  -2.28591 |  -3.89305 |   0       |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |    -2.0319 |     -2.165 |   -8.43702 |  -5.60134 |  -4.27772 |  -5.31289 |  -5.49879 |  -6.12456 |  -2.49866 |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |   -3.63331 |   -2.64148 |   -2.89148 |   -1.05579 |     0      |          0 |  -6.83485 |  -5.85389 |  -6.42367 |  -7.49774 |  -7.89142 |  -5.72372 |  -7.68896 |   0       |   0       |    0       |    0       |   -9.91973 |   -11.3114 |   -13.2499 |   -12.9776 |   -13.3304 |   -1.97719 |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |  -4.99614 |  -5.84851 |  -9.37382 |  -9.22921 |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |      0     |     0      |     0      |     0      |    0       |          0 | -144.044  |\n",
      "|  6 | GradientBoostingClassifier 1 |   3.77278 |    4.2351 |   3.40683 |    3.1417 |   3.23135 |   3.19122 |         2 |   1.87877 | -0.00929056 |          0 |  0         |   -0.87963 |   -1.04327 |   -1.20589 |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |   0       |  -1.89066 |   0       |         0 |         0 |         0 | -0.314613 |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |   0        |    0       |    0      |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    0       |    0       |   0.823269 |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   1.22145 |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |   -1.10753 |  -0.737095 |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   0       |   1.26089 |   0       |   2.21099 |   2.57676 |   1.32827 |   0        |    1.00532 |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0.953951 |   0.787757 |    1.47187 |    2.51269 |    2.37477 |    2.30044 |    2.44208 |    1.32028 |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |  -0.150581 |  -0.565407 |  -0.522616 |  -0.642497 |  -0.382029 |          0 |    1.1516 |    1.3354 |         0 |  0.361591 |         0 |         0 |  0        |  0        |  0        |   0        | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    2.09939 |    2.04017 |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0.780031 |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |     0      |    0       |    0       |    0       |    0       |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |     0     |  -4.31586 |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0.228949 |   -2.65257 |   -2.07363 |  -1.61219 |   0       |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |    0       |    0       |     0      | -0.0470645 |   0.180425 | 0.00340225 |         0 |   0       |   0       |  0        |         0 |  0.955691 |         0 |    0      |  0        |          0 |   -1.14931 |   -1.78603 |    0       |    0       |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |   0       |   0       |  -9.71429 |   0       |   0       |   0       |    0       |    0       |    0       |   -2.72436 |    0       |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   6.99634 |   0       |   0       |   0       |    0       |    9.34608 |    10.9887 |    7.56362 |    0       |    7.38746 |    9.22434 |    12.0577 |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   0       |   0       |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    0       |   0        |   0        |   0        |          0 |     0      |   0       |   0       |  0        |   0       |   0       | -0.568182 |    0      |   0       |   0       |  -0.837535 |    0       |    0       |          0 |    4.35537 |          0 |    0       |          0 |   4.88153 |   3.35075 |         0 |  0        |  0        |         0 |         0 |  0        |         0 |          0 |          0 |    0       |    0       |   -2.50948 |    -3.3373 |    0       |   -2.75476 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |  -4.75845 |    0       |    0       |    0       |    0       |    0       |    -2.0319 |     -2.165 |   -8.43702 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |   0       | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |   0       |   0       |   0       |   0       |   0       |   0       |         0 |   0       |     0      |    0       |      0     |     0      |     0      |     0      |    0       |          0 |  363.393  |\n",
      "|  7 | GradientBoostingClassifier 2 |   3.77278 |    0      |   0       |    3.1417 |   3.23135 |   3.19122 |         2 |   0       |  0          |          0 |  0         |   -0.87963 |    0       |   -1.20589 |    0       |   -0.94879 |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 | -0.314613 |   -0.44691 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |   0        |    0       |    0      |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    0       |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    0       |    1.02985 |   0.823269 |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   0       |   1.22145 |  0.705188 |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |   -1.26123 |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |    0       |  -0.737095 |          0 |    1.01712 |          0 |          0 |         0 |         0 |   0       |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 | -0.139672 |         0 |         0 |         0 |         0 |   0        |   0.787757 |    1.47187 |    2.51269 |    0       |    2.30044 |    2.44208 |    1.32028 |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |  -0.382029 |          0 |    0      |    1.3354 |         0 |  0        |         0 |         0 |  0        |  0        | -0.344349 |  -0.537634 | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   0       |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   0       |   1.38471 |    1.34478 |    0       |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |  -1.22318 |  -1.23962 |  -1.11893 |  -1.20494 |  -1.27928 |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |     0      |    0       |    0       |    0       |    0       |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |     0     |   0       |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |    0       |    0       |   0       |  -3.39023 |   0       |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |    0      |  -4.78405 |    0       |    0       |    0       |    0       |     0      | -0.0470645 |   0        | 0          |         0 |   1.08933 |   0       |  0        |         0 |  0        |         0 |    0      |  0        |          0 |   -1.14931 |   -1.78603 |    0       |    0       |    0       |   -3.95929 |    0       |  -4.90613 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |   -4.41286 |    0       |    0       |   0       |         0 |   1.99926 |   2.06914 |   4.04412 |   6.99634 |   0       |   4.78261 |   2.99765 |    6.50075 |    9.34608 |    10.9887 |    7.56362 |    0       |    7.38746 |    9.22434 |    12.0577 |   8.37823 |   8.07424 |   7.84029 |   0       |   0       |   0       |   0       |   8.70648 |   9.99095 |    0       |     3.8171 |    0       |   0        |   0        |   0        |          0 |     0      |   0       |   0       |  0        |   0       |   0       |  0        |    0      |   0       |   0       |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   4.88153 |   0       |         0 | -0.388042 |  0.388514 |         0 |         0 |  0        |         0 |          0 |          0 |    0       |    0       |    0       |     0      |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |  -4.75845 |    0       |    0       |    0       |    0       |    0       |    -2.0319 |     -2.165 |   -8.43702 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |   0       |  0        |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |   0       |  0        |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |         0 |   0       |     0      |    0       |      0     |     0      |     0      |     0      |    0       |          0 |  418.747  |\n",
      "|  8 | XGBRFClassifier 1            |   3.77278 |    4.2351 |   3.40683 |    0      |   3.23135 |   3.19122 |         2 |   0       |  0          |          0 |  0         |    0       |    0       |    0       |    0       |    0       |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |  -0.902958 |   -1.45157 |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |  0        |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |    0       |    0       |   0        |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   0       |   0       |   0       |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    2.37477 |    0       |    0       |    0       |  0        |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |  -0.150581 |  -0.565407 |  -0.522616 |  -0.642497 |  -0.382029 |          0 |    0      |    0      |         0 |  0        |         0 |         0 |  0        |  0        |  0        |   0        |  0         |          0 |   0        |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    2.46334 |    2.09939 |    2.04017 |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0.780031 |          0 |   0.689569 |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |  -1.44254 |  -2.72549 |   -2.7033 |  -2.24228 |   -2.3446 |  -2.17969 |   -2.2474 |    -5.8805 |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |   -5.78021 |   -5.37108 |  -5.59105 |  -5.13477 |  -4.40971 |  -3.53643 |  -4.22702 |  -4.67687 |    -4.789 |  -4.31586 |  -3.61832 |          0 |          0 |          0 |  -0.231788 |          0 |   0.228949 |   -2.65257 |   -2.07363 |  -1.61219 |  -3.39023 |  -4.00376 |  -4.14597 |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |   -4.87732 |   -4.90209 |    -2.1047 | -0.0470645 |   0.180425 | 0.00340225 |         0 |   1.08933 |   1.40092 |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |  -8.64505 |  -6.94907 |   -6.89837 |   -9.63855 |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   1.99926 |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |     0      |    0       |    0       |    0       |    9.22434 |    12.0577 |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   0       |   0       |   0       |   0       |   0       |    0       |     0      |    0       |   0        |   0        |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 | -0.700101 |  -1.21992 |  -1.80141 | -0.568182 |   -1.9861 |  -1.52018 |  -2.24782 |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   0       |   0       |         0 |  0        |  0.388514 |         0 |         0 |  0        |         0 |          0 |          0 |    0       |   -2.44186 |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |   0       |  -4.27676 |  -4.10751 |  -3.89892 |  -5.43497 |   0       |   0       |  -3.89305 |  -4.75845 |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |    -2.0319 |     -2.165 |   -8.43702 |  -5.60134 |  -4.27772 |  -5.31289 |  -5.49879 |  -6.12456 |  -2.49866 |  -5.24452 |  -4.10492 |  -3.99398 |   -6.30455 |   -4.44648 |   -6.20065 |   -8.12859 |   -8.22664 |   -6.14543 |   -4.46771 |    0       |    0      |   0       |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |   -3.63331 |   -2.64148 |   -2.89148 |   -1.05579 |    -1.3381 |          0 |  -6.83485 |  -5.85389 |  -6.42367 |  -7.49774 |  -7.89142 |  -5.72372 |  -7.68896 |  -9.35872 |  -5.27969 |   -7.54655 |   -8.12968 |   -9.91973 |   -11.3114 |   -13.2499 |   -12.9776 |   -13.3304 |   -1.97719 |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |  -4.99614 |  -5.84851 |  -9.37382 |  -9.22921 |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |    -13.159 |   -10.5878 |   -10.7377 |   -10.1989 |   -8.65448 |          0 | -324.407  |\n",
      "|  9 | XGBRFClassifier 2            |   3.77278 |    4.2351 |   3.40683 |    3.1417 |   3.23135 |   3.19122 |         0 |   0       |  0          |          0 |  0         |    0       |    0       |    0       |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |   0       |   0       |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |  -2.39031 |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |  -0.902958 |   -1.45157 |   -2.9313 |  -1.87722 | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    1.55221 |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    0       |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |  0        |  0        |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |    0      |    0      |         0 |         0 |         0 |         0 |          0 |    0       |    0       |   0        |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   0       |   0       |   0       |   2.21099 |   2.57676 |   1.32827 |   0        |    0       |   0        |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |  0        |         0 |         0 |         0 |         0 |   0.953951 |   0.787757 |    1.47187 |    2.51269 |    2.37477 |    2.30044 |    2.44208 |    1.32028 |  0        |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |  -0.150581 |  -0.565407 |  -0.522616 |  -0.642497 |  -0.382029 |          0 |    1.1516 |    1.3354 |         0 |  0        |         0 |         0 |  0        |  0        |  0        |   0        |  0         |          0 |   0        |   0        |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    0       |    0       |    0       |   1.93682 |  0.700355 |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0.780031 |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |   0       |   0       |   0       |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |    -5.5622 |   -6.05803 |   -5.00157 |   -5.19166 |   -6.71796 |    0       |    0       |   0       |  -5.13477 |   0       |   0       |   0       |   0       |     0     |  -4.31586 |  -3.61832 |          0 |          0 |          0 |   0        |          0 |   0        |   -2.65257 |    0       |  -1.61219 |  -3.39023 |   0       |   0       |   -3.1358 |   -3.5658 |   -2.6929 |   -5.1988 |  -4.78405 |   -4.25957 |   -3.95198 |    0       |    0       |    -2.1047 | -0.0470645 |   0.180425 | 0          |         0 |   0       |   0       |  0        |         0 |  0.955691 |         0 |    0      |  0        |          0 |    0       |    0       |    0       |   -4.74937 |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |  -6.74778 |  -7.71806 |  -7.13598 |  -6.40055 |  -9.71429 |  -8.24422 |   0       |   0       |    0       |   -9.63855 |   -7.13523 |   -2.72436 |   -3.02339 |   -4.41286 |   -5.05948 |   -5.02876 |  -1.11971 |         0 |   0       |   0       |   0       |   6.99634 |   0       |   0       |   0       |    0       |    9.34608 |    10.9887 |    0       |    0       |    0       |    0       |     0      |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   0       |   6.61725 |   0       |   8.70648 |   0       |    6.10749 |     3.8171 |    1.44843 |   0        |  -0.176812 |   0.946811 |          0 |    -3.4065 |  -3.57803 |  -3.51201 | -0.700101 |  -1.21992 |  -1.80141 | -0.568182 |    0      |  -1.52018 |   0       |  -0.837535 |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   4.88153 |   0       |         0 | -0.388042 |  0.388514 |         0 |         0 |  0.340408 |         0 |          0 |          0 |   -1.89944 |   -2.44186 |   -2.50948 |    -3.3373 |   -2.90451 |   -2.75476 |   0       |  -4.27676 |  -4.10751 |  -3.89892 |   0       |   0       |  -2.28591 |  -3.89305 |  -4.75845 |   -3.18777 |   -3.68296 |   -3.55285 |   -1.18653 |   -1.60795 |    -2.0319 |     -2.165 |   -8.43702 |  -5.60134 |  -4.27772 |  -5.31289 |  -5.49879 |  -6.12456 |  -2.49866 |  -5.24452 |   0       |  -3.99398 |   -6.30455 |   -4.44648 |   -6.20065 |   -8.12859 |    0       |    0       |   -4.46771 |    2.38273 |    0      |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0        |  0.240333 |  -1.27986 | -0.734372 |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |   -1.05579 |    -1.3381 |          0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |     0      |     0      |   -12.9776 |   -13.3304 |   -1.97719 |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0.0336692 |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |  -4.99614 |  -5.84851 |  -9.37382 |   0       |  -4.99828 |   0.56852 |         0 |  -3.04919 |    -2.7895 |   -4.14027 |    -13.159 |     0      |     0      |     0      |    0       |          0 |   18.0449 |\n",
      "| 10 | XGBClassifier 1              |   3.77278 |    4.2351 |   0       |    3.1417 |   3.23135 |   3.19122 |         2 |   1.87877 | -0.00929056 |          0 |  0         |    0       |    0       |    0       |   -1.02104 |    0       |  -0.468917 |         0 |         0 |  -2.09776 |  -1.89066 |   0       |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |   0        |    0       |    0      |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    0       |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    1.02985 |   0        |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   1.23457 |   1.22145 |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |   -2.1316 |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |    0       |  -0.737095 |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   1.67686 |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0.680947 |    1.00532 |   0.319086 |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 | -0.139672 |         0 |         0 |         0 |         0 |   0        |   0        |    0       |    0       |    0       |    2.30044 |    0       |    0       |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |   0        |  -0.382029 |          0 |    1.1516 |    1.3354 |         0 |  0        |         0 |         0 |  0        | -0.415857 | -0.344349 |  -0.537634 | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    2.46334 |    2.09939 |    2.04017 |   1.93682 |  0.700355 |   0       |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    0       |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |  -1.11893 |  -1.20494 |  -1.27928 |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |     0      |    0       |    0       |    0       |    0       |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |     0     |   0       |   0       |          0 |          0 |          0 |   0        |          0 |   0.228949 |    0       |    0       |   0       |   0       |   0       |   0       |   -3.1358 |   -3.5658 |   -2.6929 |    0      |   0       |    0       |    0       |    0       |    0       |     0      | -0.0470645 |   0.180425 | 0          |         0 |   1.08933 |   1.40092 |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |    0       |    0       |   -4.95376 |   -3.95929 |   -4.55952 |  -4.90613 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |   -4.41286 |   -5.05948 |    0       |   0       |         0 |   1.99926 |   0       |   0       |   6.99634 |   3.88346 |   4.78261 |   2.99765 |    6.50075 |    9.34608 |    10.9887 |    7.56362 |    5.91583 |    7.38746 |    9.22434 |    12.0577 |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    1.44843 |   0.486258 |  -0.176812 |   0        |          0 |     0      |   0       |   0       |  0        |   0       |   0       |  0        |    0      |   0       |   0       |   0        |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   4.88153 |   3.35075 |         0 | -0.388042 |  0.388514 |         0 |         0 |  0        |         0 |          0 |          0 |    0       |    0       |    0       |     0      |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    -2.0319 |      0     |   -8.43702 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |   0       | -0.734372 |         0 |         0 |  -1.45923 |   0       |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |  -1.74765 | -0.575374 |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0         |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |   0       |   0       |   0       |   0       |   0       |   0       |         0 |   0       |     0      |    0       |      0     |     0      |     0      |     0      |    0       |          0 |  482.748  |\n",
      "| 11 | XGBClassifier 2              |   3.77278 |    4.2351 |   0       |    0      |   3.23135 |   3.19122 |         0 |   1.87877 | -0.00929056 |          0 |  0         |   -0.87963 |    0       |   -1.20589 |   -1.02104 |   -0.94879 |  -0.468917 |         0 |         0 |  -2.09776 |  -1.89066 |  -1.53457 |         0 |         0 |         0 |  0        |    0       |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0       |   0       |   0       |   0       |   0       |         0 |    0      |         0 |   0       |          0 |     0      |    0       |          0 |    0       |    0       |   0        |    0       |    0      |   0       | -0.860657 | -0.690105 | -0.355543 | -0.339696 | -0.414132 |   1.44134 |   1.21495 |    2.51813 |    2.32573 |    2.82035 |    2.43157 |    3.96147 |    1.80978 |    0       |     2.3638 |     3.917 |   3.65074 |   3.22588 |    3.4437 |   3.58753 |   3.24947 |   4.55491 |   2.74594 |   2.62043 |    2.58052 |    2.75355 |    2.58443 |    1.77722 |    1.49884 |    1.02985 |   0.823269 |          0 |         0 |         0 |         0 |         0 |         0 |   0       |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |          0 |   0.98082 |   1.23457 |   0       |  0.705188 |  0.500202 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |    0       |    0       |          0 |    0       |         0 |         0 |         0 |   -2.1316 |    0      |         0 |         0 |         0 |         0 |          0 |   -1.16092 |    0       |  -0.737095 |          0 |    0       |          0 |          0 |         0 |         0 |   0       |   0       |   1.26089 |   1.85489 |   2.21099 |   2.57676 |   1.32827 |   0.680947 |    1.00532 |   0.319086 |          0 |          0 |          0 |          0 |          0 |         0 |         0 |         0 |         0 | -0.139672 |         0 |         0 |         0 |         0 |   0        |   0.787757 |    0       |    2.51269 |    0       |    2.30044 |    0       |    0       |  0.361371 |         0 |         0 |         0 |         0 |  0.765089 |         0 |         0 |         0 |          0 |          0 |   0        |   0        |   0        |  -0.642497 |  -0.382029 |          0 |    0      |    1.3354 |         0 |  0        |         0 |         0 | -0.239778 | -0.415857 | -0.344349 |  -0.537634 | -0.0814797 |          0 |  -0.135239 |  -0.138253 |          0 |          0 |          0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |         0 |          0 |          0 |          0 |          0 |          0 |    2.46334 |    2.09939 |    2.04017 |   1.93682 |  0        |   2.50579 |   2.38254 |   2.19648 |   2.51982 |   1.95952 |   1.41309 |   1.38471 |    1.34478 |    1.41379 |    1.48561 |    1.06383 |   0        |          0 |   0        |          0 |         0 | 0.0218287 |         0 |         0 |   0       |   0       |  -1.11893 |  -1.20494 |  -1.27928 |    0       |          0 |          0 |    0       |          0 |          0 |          0 |          0 |         0 |         0 |   0       |   0       |    0      |   0       |    0      |   0       |    0      |     0      |     0      |    0       |    0       |    0       |    0       |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |     0     |   0       |   0       |          0 |          0 |          0 |   0        |          0 |   0.228949 |    0       |    0       |   0       |   0       |   0       |   0       |   -3.1358 |   -3.5658 |   -2.6929 |    0      |   0       |    0       |    0       |    0       |    0       |     0      | -0.0470645 |   0.180425 | 0          |         0 |   1.08933 |   0       |  0.978825 |         0 |  0.955691 |         0 |    2.0406 |  0.572226 |          0 |   -1.14931 |   -1.78603 |   -3.28063 |    0       |   -4.95376 |   -3.95929 |    0       |  -4.90613 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |   -4.41286 |    0       |    0       |   0       |         0 |   1.99926 |   2.06914 |   0       |   6.99634 |   3.88346 |   4.78261 |   2.99765 |    6.50075 |    9.34608 |    10.9887 |    7.56362 |    5.91583 |    7.38746 |    9.22434 |    12.0577 |   8.37823 |   8.07424 |   7.84029 |   8.80145 |   6.51237 |   6.61725 |   9.32581 |   8.70648 |   9.99095 |    6.10749 |     3.8171 |    1.44843 |   0        |   0        |   0        |          0 |     0      |   0       |   0       |  0        |   0       |   0       |  0        |    0      |   0       |   0       |   0        |    0       |    0       |          0 |    0       |          0 |    0       |          0 |   4.88153 |   3.35075 |         0 | -0.388042 |  0.388514 |         0 |         0 |  0        |         0 |          0 |          0 |    0       |    0       |    0       |     0      |    0       |    0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |  -4.75845 |    0       |    0       |    0       |    0       |    0       |    -2.0319 |     -2.165 |   -8.43702 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |    0       |    0       |    0       |    0       |    2.38273 |    2.6814 |   3.17052 |   3.93551 |   2.65203 |   5.06463 |   1.92476 | -0.285714 |  0.191342 |         0 |      2.202 |    2.39375 |    7.36593 |    7.08878 |    6.95652 |    4.12628 |    2.11911 |   0.290909 |  0.240333 |   0       |  0        |         0 |         0 |  -1.45923 |   1.42899 |         0 |         0 |          0 |   -1.54131 |    0       |    0       |    0       |    0       |     0      |          0 |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |   0       |    0       |    0       |    0       |     0      |     0      |     0      |     0      |    0       |  -1.74765 |  0        |         0 |   3.55469 |   4.19569 |   1.05486 |  0.831331 |   1.77368 | 0         |     1.5558 |    3.63207 |     12.121 |     11.919 |    15.3575 |    13.4838 |    14.4351 |   -3.02948 |  -3.22127 |   0       |   0       |   0       |   0       |   0       |   0       |         0 |   0       |     0      |    0       |      0     |     0      |     0      |     0      |    0       |          0 |  477.961  |\n",
      "+----+------------------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-------------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Classifier type\", \"pct_change\"]\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "# print(tabulate(score_df, headers, tablefmt=\"psql\"))\n",
    "headers2 = [\"Classifier type\", ] + step_headers\n",
    "score_df = pd.DataFrame(score.items(), columns=headers)\n",
    "accuracy_df = pd.DataFrame(score_df['pct_change'].tolist(), index=score_df.index, columns=step_headers)\n",
    "score_df = score_df.drop('pct_change', 1)\n",
    "f_out: pd.DataFrame = pd.merge(score_df, accuracy_df, how='left', left_index=True, right_index=True)\n",
    "f_out['sum'] = f_out.sum(axis=1)\n",
    "f_out['sum'] = f_out['sum'].apply(lambda x: x+100)\n",
    "headers2 = headers2 + ['sum']\n",
    "print(tabulate(f_out, headers2, tablefmt=\"psql\"))\n",
    "\n",
    "filename_to_export = f'C:\\\\Users\\\\exomat\\\\Desktop\\\\repo\\\\magisterka_analiza\\\\data\\\\results\\\\simulation_diff\\\\result_test_all_{symbol}_{WINDOW}_{datetime.now().strftime(\"%d_%m_%Y %H_%M_%S\")}.csv'\n",
    "f_out.to_csv(filename_to_export, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "magisterka_analiza",
   "language": "python",
   "display_name": "Python magisterka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}